<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Paper Draft | LLM Synthetic Students</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav class="sidebar">
        <div class="logo">
            <h2>Synthetic Students</h2>
            <span class="subtitle">Research Literature</span>
        </div>
        <ul class="nav-links">
            <li><a href="index.html">Overview</a></li>
            <li><a href="literature-review.html">Literature Review</a></li>
            <li><a href="replication-studies.html">Replication Studies</a></li>
            <li><a href="paper-draft.html" class="active">Paper Draft</a></li>
            <li><a href="results.html">Results Summary</a></li>
            <li><a href="prompts.html">Prompts</a></li>
        </ul>
        <div class="nav-footer">
            <p>AIED 2026 Research</p>
        </div>
    </nav>

    <main class="content">
        <header>
            <h1>Paper Draft</h1>
            <p class="lead">Beyond Difficulty: Can LLM Synthetic Students Exhibit Human-Like Misconceptions?</p>
        </header>

        <div class="article-content">
            <section>
                <h2>Abstract</h2>
                <p>Large language models are increasingly proposed as "synthetic students" for educational applications--pretesting items, simulating learner populations, and validating instructional interventions. Prior work evaluated these systems on whether LLMs select the same wrong answers as students. We argue this criterion is insufficient: authentic simulation requires matching the underlying <em>reasoning</em>, not just the answer.</p>

                <p>We introduce the <strong>Reasoning Authenticity Gap</strong>--the difference between an LLM's rate of selecting misconception-targeted distractors versus its rate of exhibiting that misconception in chain-of-thought reasoning. In pilot data (N=208 errors across 7 models), we find a gap of <strong>41.3 percentage points</strong>: LLMs select target distractors 50% of the time but show matching reasoning only 8.7% of the time.</p>

                <p>We test whether theoretically-grounded prompt specifications can close this gap. Drawing on Knowledge Component theory (Koedinger), misconception-as-theory (Chi), and buggy procedures (Brown & Burton), we design four specification levels: (S1) persona only, (S2) knowledge state, (S3) mental model, and (S4) production rules.</p>
            </section>

            <section>
                <h2>1. Introduction</h2>

                <h3>1.1 The Promise of Synthetic Students</h3>
                <p>Simulating student behavior has been a goal of educational technology for nearly five decades. From BUGGY (Brown & Burton, 1978) to modern cognitive tutors, researchers have sought computational models that predict, explain, and replicate how students engage with content--especially when they struggle.</p>

                <p>Large language models have renewed this ambition. If LLMs could authentically simulate struggling students, researchers could:</p>
                <ul>
                    <li>Pretest assessment items without human participants</li>
                    <li>Validate whether instructional interventions address specific misconceptions</li>
                    <li>Generate diverse learner populations for training adaptive systems</li>
                    <li>Conduct controlled experiments impossible with human subjects</li>
                </ul>

                <h3>1.2 The Validity Problem</h3>
                <p>But can LLMs simulate not just student <em>performance levels</em>, but authentic student <em>reasoning</em>? When an LLM selects the wrong answer, does its underlying reasoning reflect the same misconception that leads real students astray?</p>

                <p>This distinction matters. Consider a multiple-choice item designed to diagnose whether students believe "larger denominator means larger fraction." If an LLM selects the target distractor but its reasoning says "I'll pick B because it seems like a common mistake," this is surface mimicry, not authentic simulation.</p>

                <h3>1.3 Pilot Finding: The Gap is Large</h3>

                <div class="key-finding">
                    <h3>Reasoning Authenticity Gap</h3>
                    <p>In pilot data across 7 LLMs and 208 errors:</p>
                    <ul>
                        <li><strong>Target distractor rate</strong>: 50.0% (significantly above 33.3% chance)</li>
                        <li><strong>Misconception alignment rate</strong>: 8.7% (reasoning matches target misconception)</li>
                        <li><strong>Gap</strong>: 41.3 percentage points</li>
                    </ul>
                    <p class="implication">LLMs select the "right wrong answer" but their reasoning rarely reflects the misconception.</p>
                </div>
            </section>

            <section>
                <h2>2. Theoretical Framework</h2>

                <h3>2.1 What Makes an Error "Authentic"?</h3>
                <p>We propose four criteria for authentic misconception simulation:</p>
                <ol>
                    <li><strong>Reasoning Fidelity</strong>: Chain-of-thought reflects the target misconception's logic</li>
                    <li><strong>Consistency</strong>: Same misconception produces same error across items</li>
                    <li><strong>Generalization</strong>: Error pattern transfers to novel items</li>
                    <li><strong>Discriminant Validity</strong>: Model does NOT produce off-target errors</li>
                </ol>

                <h3>2.2 Specification Levels: A Cognitive Hierarchy</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Level</th>
                            <th>Name</th>
                            <th>Theoretical Basis</th>
                            <th>What's Specified</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>S1</td>
                            <td>Persona</td>
                            <td>None (baseline)</td>
                            <td>"Struggling student"</td>
                        </tr>
                        <tr>
                            <td>S2</td>
                            <td>Knowledge State</td>
                            <td>KC Theory (Koedinger)</td>
                            <td>What's known/unknown</td>
                        </tr>
                        <tr>
                            <td>S3</td>
                            <td>Mental Model</td>
                            <td>Misconception-as-Theory (Chi)</td>
                            <td>The belief system held</td>
                        </tr>
                        <tr>
                            <td>S4</td>
                            <td>Production Rules</td>
                            <td>Buggy Procedures (Brown & Burton)</td>
                            <td>Step-by-step algorithm</td>
                        </tr>
                    </tbody>
                </table>

                <h3>2.3 Why Different Levels Might Work Differently</h3>
                <p><strong>S2 (Knowledge State)</strong> specifies <em>what</em> the student knows without specifying <em>how</em> they reason. This may be insufficient--many reasoning paths are consistent with a given knowledge state.</p>

                <p><strong>S3 (Mental Model)</strong> specifies <em>why</em> the student thinks a certain way. This should constrain reasoning but requires the model to derive procedural behavior from beliefs.</p>

                <p><strong>S4 (Production Rules)</strong> specifies <em>how</em> the student reasons step-by-step. This is most constrained but may feel artificial.</p>

                <p><strong>Hypothesis</strong>: S4 works best for procedural misconceptions (algorithms), S3 for conceptual (belief systems).</p>
            </section>

            <section>
                <h2>3. Method</h2>

                <h3>3.1 Design</h3>
                <p><strong>4 x 3 x 2 Factorial</strong>:</p>
                <ul>
                    <li>Specification Level: S1, S2, S3, S4</li>
                    <li>Model Capability: Frontier, Mid, Weak</li>
                    <li>Misconception Type: Procedural, Conceptual</li>
                </ul>

                <h3>3.2 Models</h3>
                <table>
                    <thead>
                        <tr><th>Tier</th><th>Models</th><th>GSM8K Accuracy</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>Frontier</td><td>GPT-4o, Claude 3.5 Sonnet</td><td>~95%</td></tr>
                        <tr><td>Mid</td><td>GPT-3.5-turbo, Claude 3 Haiku</td><td>~75%</td></tr>
                        <tr><td>Weak</td><td>Llama-3.1-8B, Mistral-7B</td><td>~50%</td></tr>
                    </tbody>
                </table>

                <h3>3.3 Misconceptions</h3>
                <p><strong>Procedural</strong> (algorithm misapplication):</p>
                <ul>
                    <li>PROC_ORDER_OPS: Left-to-right instead of PEMDAS</li>
                    <li>PROC_SUBTRACT_REVERSE: Smaller-from-larger in each column</li>
                </ul>

                <p><strong>Conceptual</strong> (flawed mental model):</p>
                <ul>
                    <li>CONC_FRAC_DENOM: Larger denominator = larger fraction</li>
                    <li>CONC_MULT_INCREASE: Multiplication always increases</li>
                </ul>

                <h3>3.4 Prompt Specifications</h3>
                <p>Example for PROC_ORDER_OPS:</p>

                <blockquote>
                    <strong>S1 (Persona)</strong>: You are a 6th grade student who sometimes struggles with math. Show your thinking.
                </blockquote>

                <blockquote>
                    <strong>S2 (Knowledge State)</strong>: You know single operations well. You're still learning problems with multiple operations. You've heard of "order of operations" but aren't sure how it works.
                </blockquote>

                <blockquote>
                    <strong>S3 (Mental Model)</strong>: You believe math expressions should be solved left-to-right, like reading. When teachers mention "order of operations," you think it means the order you see them.
                </blockquote>

                <blockquote>
                    <strong>S4 (Production Rules)</strong>: You solve expressions using this procedure: STEP 1: Find the leftmost operation. STEP 2: Apply it. STEP 3: Replace with result. STEP 4: Repeat until one number remains.
                </blockquote>

                <h3>3.5 Dependent Variables</h3>
                <table>
                    <thead>
                        <tr><th>Metric</th><th>Definition</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>Error Rate</td><td>1 - Accuracy</td></tr>
                        <tr><td>Target Distractor Rate</td><td>P(target distractor | error)</td></tr>
                        <tr><td>Misconception Alignment</td><td>P(reasoning matches target | error)</td></tr>
                        <tr><td><strong>Reasoning Authenticity Gap</strong></td><td>Target Rate - Alignment Rate</td></tr>
                        <tr><td>Consistency</td><td>Variance in error pattern within misconception</td></tr>
                    </tbody>
                </table>
            </section>

            <section>
                <h2>4. Expected Outcomes</h2>

                <h3>Outcome A: Specification Works</h3>
                <p><strong>Pattern</strong>: Misconception Alignment increases monotonically S1 to S4. Gap shrinks significantly at S4.</p>
                <p><strong>Implication</strong>: LLMs <em>can</em> represent stable misconceptions when given sufficient cognitive specification. Use detailed KC profiles (S4-level) for synthetic students.</p>

                <h3>Outcome B: Type-Specific Effects</h3>
                <p><strong>Pattern</strong>: S4 works for procedural misconceptions; S3 works for conceptual. Significant Type x Specification interaction.</p>
                <p><strong>Implication</strong>: Match specification type to misconception type. Develop separate prompting templates for procedural vs. conceptual errors.</p>

                <h3>Outcome C: Nothing Works</h3>
                <p><strong>Pattern</strong>: Gap persists across all specification levels. Even S4 shows low alignment.</p>
                <p><strong>Implication</strong>: LLMs fundamentally cannot represent stable misconceptions through prompting. Alternative approaches needed: fine-tuning on misconception-labeled data, neuro-symbolic architectures, or hybrid systems.</p>

                <h3>Outcome D: Only Weak Models Work</h3>
                <p><strong>Pattern</strong>: Weak models show improved alignment at S3/S4; frontier models resist error induction regardless of specification.</p>
                <p><strong>Implication</strong>: The capability paradox is fundamental. Use capability-matched models. For simulating struggling students, use weaker models with S3/S4 prompts.</p>
            </section>

            <section>
                <h2>5. Research Questions</h2>

                <p><strong>RQ1</strong>: Does the Reasoning Authenticity Gap persist across prompting conditions, or can structured specifications close it?</p>

                <p><strong>RQ2</strong>: Do different specification types work better for different misconception types (procedural vs. conceptual)?</p>

                <p><strong>RQ3</strong>: Does model capability interact with specification level--can structured prompts overcome the "capability paradox" where frontier models resist error induction?</p>

                <p><strong>RQ4</strong>: Do induced misconceptions show consistency across items, or are LLM errors fundamentally unstable?</p>
            </section>

            <section>
                <h2>References</h2>
                <ul>
                    <li>Brown, J. S., & Burton, R. R. (1978). Diagnostic models for procedural bugs in basic mathematical skills. <em>Cognitive Science</em>, 2(2), 155-192.</li>
                    <li>Chi, M. T. H. (2008). Three types of conceptual change: Belief revision, mental model transformation, and categorical shift.</li>
                    <li>Koedinger, K. R., Corbett, A. T., & Perfetti, C. (2012). The Knowledge-Learning-Instruction framework. <em>Cognitive Science</em>, 36(5), 757-798.</li>
                    <li>Lu, Y., & Wang, S. (2024). Generative Students: Using LLM-Simulated Student Profiles for Question Item Evaluation. L@S '24.</li>
                </ul>
            </section>
        </div>

        <footer>
            <p>AIED 2026 Submission | Last updated: January 2026</p>
        </footer>
    </main>
</body>
</html>
