<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Optimizing LLM Item Difficulty Estimation: A Sequential Design-of-Experiments Study</title>
<script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>
<style>
  :root {
    --bg: #fafafa; --card: #fff; --text: #1a1a1a; --muted: #666;
    --accent: #2563eb; --accent2: #dc2626; --border: #e5e7eb;
    --green: #16a34a; --amber: #d97706;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body {
    font-family: 'Georgia', 'Times New Roman', serif;
    background: var(--bg); color: var(--text);
    line-height: 1.7; max-width: 900px; margin: 0 auto;
    padding: 2rem 1.5rem;
  }
  h1 { font-size: 1.7rem; line-height: 1.3; margin-bottom: 0.3rem; }
  h2 { font-size: 1.3rem; margin: 2.5rem 0 0.8rem; border-bottom: 2px solid var(--accent); padding-bottom: 0.3rem; }
  h3 { font-size: 1.1rem; margin: 1.5rem 0 0.5rem; }
  h4 { font-size: 1rem; margin: 1.2rem 0 0.4rem; }
  p { margin-bottom: 1rem; }
  .authors { color: var(--muted); font-size: 0.95rem; margin-bottom: 0.3rem; }
  .venue { color: var(--muted); font-size: 0.9rem; font-style: italic; margin-bottom: 1.5rem; }
  .abstract {
    background: #f0f4ff; border-left: 4px solid var(--accent);
    padding: 1rem 1.2rem; margin: 1.5rem 0; font-size: 0.95rem;
  }
  .abstract strong { display: block; margin-bottom: 0.3rem; }
  table {
    width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem;
  }
  th, td { padding: 0.4rem 0.6rem; border: 1px solid var(--border); text-align: left; }
  th { background: #f3f4f6; font-weight: 600; }
  tr.highlight { background: #eff6ff; }
  .note {
    background: #fffbeb; border-left: 4px solid var(--amber);
    padding: 0.8rem 1rem; margin: 1rem 0; font-size: 0.9rem;
  }
  .finding {
    background: #f0fdf4; border-left: 4px solid var(--green);
    padding: 0.8rem 1rem; margin: 1rem 0; font-size: 0.9rem;
  }
  .plot-container { width: 100%; margin: 1rem 0; }
  .figure-caption { font-size: 0.85rem; color: var(--muted); margin-top: 0.3rem; font-style: italic; }
  code { background: #f3f4f6; padding: 0.15rem 0.4rem; border-radius: 3px; font-size: 0.88rem; }
  .ref { font-size: 0.88rem; margin-bottom: 0.5rem; }
  .keyword { display: inline-block; background: #e0e7ff; color: #3730a3; padding: 0.1rem 0.5rem; border-radius: 12px; font-size: 0.8rem; margin: 0.1rem; }
  sup { font-size: 0.7rem; }
  a { color: var(--accent); }
  .two-col { display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; }
  @media (max-width: 700px) { .two-col { grid-template-columns: 1fr; } }
  .stat { font-family: 'Courier New', monospace; font-weight: 600; }
  .null { color: var(--accent2); }
  .sig { color: var(--green); }
  ol, ul { margin: 0.5rem 0 1rem 1.5rem; }
  li { margin-bottom: 0.3rem; }
</style>
</head>
<body>

<h1>Optimizing LLM Item Difficulty Estimation: A Sequential Design-of-Experiments Approach</h1>
<div class="authors">Derek Lomas<sup>1</sup></div>
<div class="venue">Submission to AIED 2026 &middot; Draft February 2026</div>

<div style="margin-bottom:1rem">
  <span class="keyword">item difficulty estimation</span>
  <span class="keyword">LLM evaluation</span>
  <span class="keyword">design of experiments</span>
  <span class="keyword">psychometrics</span>
  <span class="keyword">sequential optimization</span>
</div>

<div class="abstract">
  <strong>Abstract</strong>
  Published correlations between LLM-predicted and empirical item difficulty range from near-zero to r=0.87, but each study tests a single configuration on a single dataset, leaving open which factors drive success or failure. We apply sequential design of experiments (DOE)&mdash;standard in industrial process optimization but novel in LLM evaluation&mdash;to systematically map the parameter landscape across three educational datasets, seven models, and seven theory-grounded prompt framings.

  <p style="margin-top:0.5rem">In a three-phase pipeline&mdash;prompt screening, model survey, cross-dataset confirmation&mdash;we find that prompt framing is the dominant factor. Prompts that operationalize learning science theories as counting tasks (e.g., count prerequisite failure points) achieve <span class="sig">&rho;=0.686</span> on 140 open-ended items (95% CI [0.577, 0.771]), exceeding the meta-analytic average for expert teacher judgment (r=0.63&ndash;0.66). The top prompt transfers to a held-out MCQ dataset (<span class="sig">&rho;=0.531</span>, 168 CS items), but all methods produce only <span class="null">&rho;=0.114</span> (n=105, non-significant) on misconception-targeted MCQs.</p>

  <p style="margin-top:0.5rem">The counting-prompt advantage is model-dependent: it holds on capable models but reverses on smaller ones. The practical recommendation: prompt capable LLMs to count theory-grounded structural features rather than to make holistic judgments, and always validate on a full item set&mdash;our probe-set results inflated the true signal by 4&times;.</p>
</div>

<!-- ============================================================ -->
<h2>1. Introduction</h2>

<p>Predicting how difficult a test item will be for students is a longstanding problem in educational measurement. Classical approaches use cognitive item models to identify structural features that drive difficulty (Embretson, 1998; Irvine &amp; Kyllonen, 2002), while expert teacher judgment achieves median correlations of r=0.66 (Hoge &amp; Coladarci, 1989) to mean r=0.63 (S&uuml;dkamp, Kaiser &amp; M&ouml;ller, 2012) with actual student performance. Both approaches are resource-intensive: cognitive models require domain-specific feature engineering, and teacher panels require access to experienced practitioners.</p>

<p>Large language models offer a potential shortcut. Recent work reports correlations between LLM predictions and empirical difficulty ranging widely, from weak direct estimation to r=0.87 when LLM-extracted features are combined with gradient boosting (Razavi &amp; Powers, 2025). A systematic review identified 37 papers on text-based difficulty prediction through 2025, with rapid growth but no consensus on methods (Peters, Zhang, Jiao et al., 2025). The first shared task on automated difficulty prediction (BEA 2024; Yaneva et al., 2024) found that best results only marginally beat baselines on 667 medical items, confirming the problem remains open.</p>

<p>A key limitation across this literature is that each study evaluates one model with one prompt on one dataset. When results disagree, it is unclear whether the discrepancy reflects model choice, prompt design, item properties, or student population. What is needed is a method for varying these factors independently.</p>

<p>We apply sequential design of experiments (DOE), standard in industrial process optimization (Box &amp; Draper, 2007) but, to our knowledge, novel in LLM evaluation. Sequential experimental frameworks have been applied to educational interventions at internet scale (Stamper, Lomas, Ching et al., 2012); we extend this approach to LLM configuration optimization. The pipeline has three phases: (1) screen 7 theory-grounded prompt framings, (2) survey 7 models to test generalization, and (3) confirm top configurations on a held-out dataset with pre-specified hypotheses.</p>

<p>The core findings:</p>
<ol>
  <li><strong>Prompt framing is the primary lever.</strong> The best framing (&rho;=0.686) outperforms the worst (&rho;=0.483) by 0.20&mdash;a larger effect than model choice or temperature.</li>
  <li><strong>Counting beats judgment.</strong> Prompts that operationalize learning science theories as counting tasks outperform direct &ldquo;estimate difficulty&rdquo; prompts, consistent with findings that LLMs are most effective as structured feature extractors (Razavi &amp; Powers, 2025).</li>
  <li><strong>Counting prompts require capable models.</strong> The counting-prompt advantage holds on Gemini 3 Flash but attenuates or reverses on weaker models, where plain teacher prediction is more robust.</li>
  <li><strong>The method has boundary conditions.</strong> On misconception-targeted MCQs, all methods produce &rho;=0.114 (n=105, non-significant). On a different MCQ set varying in cognitive demand, &rho;=0.531. Item properties moderate estimation accuracy in ways that require further investigation.</li>
</ol>

<h3>1.1 Prompt Design Grounded in Learning Science</h3>

<p>Rather than testing prompts ad hoc, we operationalize four learning science traditions as prompt framings, each offering a different account of what makes items difficult:</p>

<ol>
  <li><strong>Knowledge Component theory</strong> (Koedinger, Corbett &amp; Perfetti, 2012): Difficulty scales with the number of prerequisite knowledge components. Operationalized as <em>prerequisite_chain</em>: count distinct prerequisite skills and failure points.</li>
  <li><strong>Cognitive Load Theory</strong> (Sweller, 1988): Difficulty scales with intrinsic cognitive load. Operationalized as <em>cognitive_load</em>: count elements students must hold in working memory simultaneously.</li>
  <li><strong>Buggy Procedures</strong> (Brown &amp; Burton, 1978): Students apply systematic but flawed algorithms. Operationalized as <em>error_affordance</em>: count distinct plausible error paths.</li>
  <li><strong>Pedagogical Content Knowledge</strong> (Shulman, 1986): Expert teachers predict difficulty through pattern matching. Operationalized as <em>teacher</em>: direct difficulty prediction in a teacher role.</li>
</ol>

<p>The first three share a common structure: they convert difficulty estimation from a holistic judgment into an explicit counting task. This design choice is motivated by Embretson&rsquo;s (1998) cognitive design system approach, which showed that enumerating structural features of items (processing steps, working memory demands) predicts difficulty better than holistic ratings. We test whether this principle extends to LLM prompting.</p>

<!-- ============================================================ -->
<h2>2. Method</h2>

<h3>2.1 Datasets</h3>

<div class="two-col">
<div>
<h4>SmartPaper (India, Open-ended)</h4>
<ul>
  <li><strong>Items:</strong> 140 questions across 4 subjects (English, Maths, Science, Social Science)</li>
  <li><strong>Format:</strong> Open-ended with rubric scoring</li>
  <li><strong>Students:</strong> Indian government schools, Grades 6&ndash;8 (728K+ responses)</li>
  <li><strong>Difficulty metric:</strong> Classical p-correct (proportion scoring full marks)</li>
  <li><strong>Range:</strong> 0.04&ndash;0.83 (mean 0.29)</li>
</ul>
</div>
<div>
<h4>Eedi (UK, MCQ)</h4>
<ul>
  <li><strong>Items:</strong> 1,869 diagnostic maths questions (105 with IRT parameters)</li>
  <li><strong>Format:</strong> 4-option MCQ; each distractor targets a specific misconception</li>
  <li><strong>Students:</strong> UK, ages 11&ndash;16 (73,000+ responses)</li>
  <li><strong>Difficulty metric:</strong> IRT b<sub>2PL</sub> (105 items) and classical p-value (1,869 items)</li>
  <li><strong>Design:</strong> 20-item probe set stratified by difficulty quintile; 105-item confirmation set</li>
</ul>
</div>
</div>

<h4>DBE-KT22 (South Africa, MCQ) &mdash; Phase 3 confirmation dataset</h4>
<ul>
  <li><strong>Items:</strong> 168 undergraduate computer science MCQs across 27 knowledge components</li>
  <li><strong>Students:</strong> South African university (1,300+ students, complete assignment)</li>
  <li><strong>Difficulty metric:</strong> Classical p-correct</li>
  <li><strong>Purpose:</strong> Held-out dataset for pre-specified hypothesis testing</li>
</ul>

<h3>2.2 Sequential DOE Pipeline</h3>

<table>
<tr><th>Phase</th><th>Purpose</th><th>Dataset</th><th>Design</th><th>Calls</th></tr>
<tr><td>1. Screening</td><td>Identify best prompt framing</td><td>SmartPaper (140 items)</td><td>7 framings &times; 2 temps &times; 3 reps</td><td>5,880</td></tr>
<tr><td>2. Model survey</td><td>Test generalization across models</td><td>SmartPaper (140 items)</td><td>7 models &times; 3 prompts &times; 3 reps</td><td>8,820</td></tr>
<tr><td>3. Confirmation</td><td>Pre-specified test on held-out data</td><td>DBE-KT22 (168 items)</td><td>5 prompts &times; 2 temps &times; 3 reps</td><td>5,040</td></tr>
</table>

<p>Each phase&rsquo;s results inform the next. Phase 1 identifies which framings to carry forward. Phase 2 tests whether findings generalize across models. Phase 3 tests whether they generalize across datasets with pre-specified hypotheses written before seeing results.</p>

<h3>2.3 Prompt Framings (Phase 1)</h3>

<p>All 7 framings share a common structure: the LLM receives the item text and population context, and predicts what proportion of students would answer correctly. They differ in what intermediate reasoning the prompt requests:</p>

<table>
<tr><th>Framing</th><th>Theory</th><th>What the prompt asks</th></tr>
<tr><td><strong>teacher</strong></td><td>PCK (Shulman)</td><td>Direct difficulty judgment&mdash;no intermediate steps</td></tr>
<tr><td><strong>error_analysis</strong></td><td>&mdash;</td><td>Identify specific errors students would make, then estimate</td></tr>
<tr><td><strong>devil_advocate</strong></td><td>&mdash;</td><td>Challenge your initial estimate&mdash;teachers tend to overestimate students</td></tr>
<tr><td><strong>prerequisite_chain</strong></td><td>KC theory</td><td>List prerequisite skills; count failure points; use count to estimate</td></tr>
<tr><td><strong>error_affordance</strong></td><td>BUGGY</td><td>Count distinct plausible error paths; use count to estimate</td></tr>
<tr><td><strong>cognitive_load</strong></td><td>CLT (Sweller)</td><td>Count interacting elements in working memory; use count to estimate</td></tr>
<tr><td><strong>familiarity_gradient</strong></td><td>&mdash;</td><td>Rate distance from textbook drills; use rating to estimate</td></tr>
</table>

<h3>2.4 Models (Phase 2)</h3>

<table>
<tr><th>Model</th><th>Parameters</th><th>Provider</th></tr>
<tr><td>Gemini 3 Flash</td><td>Unknown</td><td>Google</td></tr>
<tr><td>GPT-4o</td><td>Unknown (large)</td><td>OpenAI</td></tr>
<tr><td>Llama-3.3-70B</td><td>70B</td><td>Groq</td></tr>
<tr><td>Gemma-3-27B</td><td>27B</td><td>Google</td></tr>
<tr><td>Llama-4-Scout</td><td>17B active (109B MoE)</td><td>Groq</td></tr>
<tr><td>Qwen3-32B</td><td>32B</td><td>Groq</td></tr>
<tr><td>Llama-3.1-8B</td><td>8B</td><td>Groq</td></tr>
</table>

<h3>2.5 Statistical Approach</h3>

<p>All correlations are Spearman &rho; (rank-order), which is invariant to the systematic calibration offset we observe (models overestimate easiness by +0.10 to +0.43). We report 95% bootstrap confidence intervals (10,000 resamples) for primary results. Temperature was tested at t=1.0 and t=2.0. Each condition uses 3 replications; reported &rho; is the averaged-prediction correlation (mean prediction across reps correlated with ground truth).</p>

<!-- ============================================================ -->
<h2>3. Results</h2>

<h3>3.1 Phase 1: Prompt Screening (SmartPaper, Gemini 3 Flash, n=140)</h3>

<p>Seven prompt framings, each at two temperatures, 3 replications per condition.</p>

<table>
<tr><th>Framing</th><th>Theory</th><th>&rho; (t=1.0)</th><th>&rho; (t=2.0)</th><th>MAE</th><th>Bias</th></tr>
<tr class="highlight"><td><strong>prerequisite_chain</strong></td><td>KC theory</td><td class="sig">0.653</td><td class="sig"><strong>0.686</strong></td><td>0.148</td><td>+0.106</td></tr>
<tr class="highlight"><td><strong>cognitive_load</strong></td><td>CLT</td><td class="sig">0.617</td><td class="sig"><strong>0.673</strong></td><td>0.190</td><td>+0.158</td></tr>
<tr><td>devil_advocate</td><td>&mdash;</td><td class="sig">0.596</td><td class="sig">0.595</td><td>0.098</td><td>&minus;0.048</td></tr>
<tr><td>error_analysis</td><td>&mdash;</td><td>0.548</td><td class="sig">0.596</td><td>0.121</td><td>+0.048</td></tr>
<tr><td>teacher</td><td>PCK</td><td>0.555</td><td>0.546</td><td>0.438</td><td>+0.433</td></tr>
<tr><td>error_affordance</td><td>BUGGY</td><td>0.493</td><td>TBD</td><td>0.179</td><td>+0.131</td></tr>
<tr><td>familiarity_gradient</td><td>&mdash;</td><td>0.483</td><td>TBD</td><td>0.407</td><td>+0.385</td></tr>
</table>

<div id="phase1Bar" class="plot-container" style="min-height:380px"></div>
<div class="figure-caption">Figure 1. Phase 1 prompt screening results (SmartPaper, 140 items, Gemini 3 Flash). The two counting prompts (prerequisite_chain, cognitive_load) outperform direct prediction and heuristic framings.</div>

<div class="finding">
<strong>Finding 1: Counting prompts outperform direct judgment.</strong> The two prompts that operationalize learning science as explicit counting tasks&mdash;prerequisite_chain (&rho;=0.686) and cognitive_load (&rho;=0.673)&mdash;outperform direct teacher prediction (&rho;=0.555) by 0.12&ndash;0.13. Both exceed the meta-analytic average for expert teacher judgment (r=0.63; S&uuml;dkamp et al., 2012). Counting gives the model a concrete intermediate representation that correlates with difficulty.
</div>

<div class="finding">
<strong>Finding 2: Ranking accuracy and calibration are dissociable.</strong> devil_advocate achieves the best calibration (bias &minus;0.048, MAE 0.098) despite only middling ranking (&rho;=0.596). teacher has the worst calibration (bias +0.433) despite decent ranking (&rho;=0.555). The &ldquo;challenge your assumptions&rdquo; framing corrects systematic overestimation of student ability without improving rank order.
</div>

<div class="note">
<strong>Temperature is a minor factor.</strong> For most framings, &Delta;&rho; between t=1.0 and t=2.0 is &lt;0.05. The exceptions are prerequisite_chain (+0.033) and cognitive_load (+0.056), both benefiting slightly from higher temperature.
</div>

<h3>3.2 Phase 2: Model Survey (SmartPaper, 7 models, n=140)</h3>

<p>Three prompt types (teacher, cognitive_load, prerequisite_chain) tested across 7 models, 3 replications each. This tests whether the Phase 1 counting-prompt advantage generalizes across models.</p>

<table>
<tr><th>Model</th><th>Params</th><th>&rho; (teacher)</th><th>&rho; (cognitive_load)</th><th>&rho; (prerequisite_chain)</th></tr>
<tr class="highlight"><td><strong>Gemini 3 Flash</strong></td><td>?</td><td class="sig">0.550</td><td class="sig">0.619</td><td class="sig"><strong>0.633</strong></td></tr>
<tr><td>Gemma-3-27B</td><td>27B</td><td class="sig">0.501</td><td class="sig">0.500</td><td>0.464</td></tr>
<tr><td>GPT-4o</td><td>? (large)</td><td class="sig">0.494</td><td>&mdash;</td><td>&mdash;</td></tr>
<tr><td>Llama-3.3-70B</td><td>70B</td><td class="sig">0.480</td><td>0.403</td><td>0.396</td></tr>
<tr><td>Llama-4-Scout</td><td>17B/109B</td><td class="sig">0.474</td><td>0.400</td><td>0.345</td></tr>
<tr><td>Qwen3-32B</td><td>32B</td><td>0.304</td><td>&mdash;</td><td>&mdash;</td></tr>
<tr><td>Llama-3.1-8B</td><td>8B</td><td>0.300</td><td>0.214</td><td>0.044</td></tr>
</table>

<div id="phase2Heatmap" class="plot-container" style="min-height:350px"></div>
<div class="figure-caption">Figure 2. Phase 2 model &times; prompt results. The counting-prompt advantage (cognitive_load and prerequisite_chain &gt; teacher) holds for Gemini but attenuates or reverses on other models. Smaller models cannot reliably enumerate prerequisites.</div>

<div class="finding">
<strong>Finding 3: Counting-prompt advantage is model-dependent.</strong> On Gemini 3 Flash, prerequisite_chain (&rho;=0.633) outperforms teacher (&rho;=0.550) by +0.083. This advantage holds weakly on Gemma-3-27B but reverses on Llama-3.3-70B, Scout, and Llama-3.1-8B. The structured counting prompts require sufficient model capability to enumerate item features reliably; on weaker models, prerequisite_chain actually <em>hurts</em> (&rho;=0.044 for Llama-8B).
</div>

<div class="finding">
<strong>Finding 4: Model capability matters more than model size.</strong> GPT-4o (&rho;=0.494) outperforms the larger Llama-3.3-70B (&rho;=0.480) and matches Scout 17B/109B MoE (&rho;=0.474). Qwen3-32B (&rho;=0.304) and Llama-3.1-8B (&rho;=0.300) define a lower capability tier. The gap between best and worst model (&Delta;&rho;=0.250 on teacher) is comparable to the prompt framing effect.
</div>

<h3>3.3 Eedi Results: A Null Finding and a Methodological Cautionary Tale</h3>

<p>Before the SmartPaper phases, we conducted extensive experiments on 1,869 Eedi maths MCQs. These results are reported separately because they produced a null finding that is both substantively and methodologically informative.</p>

<h4>3.3.1 Probe-Set Results (n=20)</h4>

<p>A Box-Behnken response surface design (46 configurations) varying prompt style, temperature, batch size, misconception hints, and model initially suggested moderate signal. After optimization, two prompts reached &rho;&asymp;0.50 on the 20-item probe set:</p>

<table>
<tr><th>Prompt</th><th>&rho; (10-rep mean)</th><th>SD</th></tr>
<tr><td>Error analysis (t=2.0)</td><td class="sig">0.500</td><td>0.111</td></tr>
<tr><td>Contrastive (t=1.5)</td><td class="sig">0.513</td><td>0.097</td></tr>
</table>

<h4>3.3.2 Confirmation Failure (n=105)</h4>

<p>When tested on the full 105-item IRT-calibrated set, the signal collapsed:</p>

<table>
<tr><th>Item set</th><th>n</th><th>&rho;</th><th>p</th><th>95% CI</th></tr>
<tr><td>Probe 20</td><td>20</td><td class="sig">0.462</td><td>0.04</td><td>&mdash;</td></tr>
<tr><td>New 30 (random)</td><td>30</td><td class="null">&minus;0.176</td><td>0.35</td><td>&mdash;</td></tr>
<tr class="highlight"><td><strong>105-item confirmation</strong></td><td><strong>105</strong></td><td class="null"><strong>0.114</strong></td><td><strong>0.25</strong></td><td><strong>[&minus;0.07, 0.30]</strong></td></tr>
</table>

<div class="note">
<strong>The probe set inflated the signal.</strong> The probe was stratified by difficulty quintile and contained two extreme-difficulty outliers (IRT b=&minus;5.18, &minus;3.37) that gave it 2.3&times; the difficulty SD of the non-probe items. Bootstrap resampling (10,000 random 20-item subsets from 105) yields mean &rho;=0.110; only 2.5% of subsets reach &rho;&ge;0.50. The probe result was a tail event, not a representative estimate. This is a cautionary finding for the broader field: small, stratified probe sets can dramatically overstate estimation accuracy.
</div>

<h4>3.3.3 Structured Elicitation on Eedi (n=20 probe)</h4>

<p>We also tested whether more elaborate cognitive scaffolding could improve estimation on the Eedi probe set:</p>

<table>
<tr><th>Strategy</th><th>Theory</th><th>&rho;</th><th>vs. baseline</th></tr>
<tr><td>Direct prediction</td><td>KC theory</td><td class="sig">0.500</td><td>&mdash;</td></tr>
<tr><td>Two-stage cognitive chains</td><td>Conceptual change</td><td>0.508</td><td>+0.01</td></tr>
<tr><td>Buggy reasoning (no hint)</td><td>BUGGY</td><td>0.488</td><td>&minus;0.01</td></tr>
<tr><td>Buggy reasoning (with hint)</td><td>BUGGY + ESS</td><td class="null">0.193</td><td>&minus;0.31</td></tr>
<tr><td>Cognitive modeling (10 CoT)</td><td>SMART/sim</td><td class="null">0.165</td><td>&minus;0.34</td></tr>
</table>

<div class="finding">
<strong>Finding 5: Deliberation hurts.</strong> Every form of structured elicitation matched or degraded direct prediction on Eedi. Providing misconception hints <em>actively hurt</em> (&rho;=0.19 vs. 0.50). Reasoning models (Gemini 2.5 Pro, DeepSeek Reasoner) performed worse than non-reasoning models. More information and more reasoning did not help.
</div>

<h3>3.4 Three-Dataset Comparison</h3>

<div id="dissociationPlot" class="plot-container" style="min-height:370px"></div>
<div class="figure-caption">Figure 3. Probe-to-full-set comparison across datasets. SmartPaper signal holds from probe to full set. Eedi signal does not survive confirmation.</div>

<table>
<tr><th></th><th>Eedi (MCQ, misconception-targeted)</th><th>SmartPaper (open-ended, multi-subject)</th></tr>
<tr><td>Best probe result</td><td>&rho;=0.50 (n=20)</td><td>&rho;=0.77 (n=20)</td></tr>
<tr><td>Full-set confirmation</td><td class="null">&rho;=0.114 (n=105, ns)</td><td class="sig">&rho;=0.686 (n=140)</td></tr>
<tr><td>Generalises?</td><td class="null"><strong>No</strong></td><td class="sig"><strong>Yes</strong></td></tr>
</table>

<h4>SmartPaper by Subject</h4>

<table>
<tr><th>Subject</th><th>&rho; (teacher, t=2.0)</th><th>n</th></tr>
<tr><td>Science</td><td class="sig">0.734</td><td>32</td></tr>
<tr><td>Social Science</td><td class="sig">0.702</td><td>34</td></tr>
<tr><td>Mathematics</td><td class="sig">0.600</td><td>33</td></tr>
<tr><td>English</td><td class="sig">0.432</td><td>41</td></tr>
</table>

<div id="subjectPlot" class="plot-container" style="min-height:320px"></div>
<div class="figure-caption">Figure 4. SmartPaper results by subject (teacher prompt). Signal is strongest for Science and Social Science, weakest for English.</div>

<h3>3.5 Phase 3: Cross-Dataset Confirmation (DBE-KT22, n=168)</h3>

<p>Phase 3 tests whether Phase 1 findings transfer to a held-out dataset: 168 undergraduate computer science MCQs from a South African university (DBE-KT22; 1,300+ students). Prompts were adapted for the new population and format. The correct answer was <strong>not</strong> shown to the LLM.</p>

<p>Four hypotheses were pre-specified before seeing Phase 3 results:</p>

<ol>
  <li><strong>H1:</strong> prerequisite_chain produces &rho; &gt; 0.30 on DBE-KT22</li>
  <li><strong>H2:</strong> SmartPaper &rho; &gt; DBE-KT22 &rho; for every prompt</li>
  <li><strong>H3:</strong> Prompt ranking is preserved across datasets</li>
  <li><strong>H4:</strong> prerequisite_chain &gt; teacher on DBE-KT22</li>
</ol>

<table>
<tr><th>Framing</th><th>&rho; (t=1.0)</th><th>&rho; (t=2.0)</th><th>95% CI (best)</th><th>MAE</th><th>Bias</th></tr>
<tr class="highlight"><td><strong>prerequisite_chain</strong></td><td class="sig">0.519</td><td class="sig"><strong>0.531</strong></td><td>[0.411, 0.640]</td><td>0.159</td><td>&minus;0.103</td></tr>
<tr><td>teacher</td><td class="sig"><strong>0.521</strong></td><td class="sig">0.518</td><td>[0.394, 0.631]</td><td>0.188</td><td>&minus;0.151</td></tr>
<tr><td>error_analysis</td><td class="sig"><strong>0.516</strong></td><td class="sig">0.506</td><td>[0.391, 0.623]</td><td>0.188</td><td>&minus;0.151</td></tr>
<tr><td>devil_advocate</td><td class="sig">0.403</td><td class="sig"><strong>0.420</strong></td><td>[0.278, 0.543]</td><td>0.268</td><td>&minus;0.251</td></tr>
<tr><td>cognitive_load</td><td class="sig">0.445</td><td class="sig"><strong>0.475</strong></td><td>[0.353, 0.591]</td><td>0.152</td><td>&minus;0.078</td></tr>
</table>

<div id="phase3Bar" class="plot-container" style="min-height:380px"></div>
<div class="figure-caption">Figure 5. Phase 3 cross-dataset confirmation: SmartPaper (Phase 1) vs. DBE-KT22 (Phase 3). All five framings transfer with significant signal. The top-3 ranking is preserved.</div>

<h4>Hypothesis Tests</h4>

<table>
<tr><th>Hypothesis</th><th>Result</th><th>Status</th></tr>
<tr><td><strong>H1:</strong> prerequisite_chain &rho; &gt; 0.30</td><td>&rho;=0.531 [0.411, 0.640]</td><td class="sig"><strong>PASS</strong></td></tr>
<tr><td><strong>H2:</strong> SmartPaper &rho; &gt; DBE-KT22 &rho;</td><td>All 5 framings: SP &gt; DBE</td><td class="sig"><strong>PASS</strong> (5/5)</td></tr>
<tr><td><strong>H3:</strong> Prompt ranking preserved</td><td>Top-3 preserved (prerequisite_chain, teacher, error_analysis); cognitive_load and devil_advocate swap</td><td class="sig"><strong>PASS</strong> (largely preserved)</td></tr>
<tr><td><strong>H4:</strong> prerequisite_chain &gt; teacher</td><td>0.531 vs. 0.521 (&Delta;=+0.010)</td><td><strong>PASS</strong> (but advantage reduced from 0.131 to 0.010)</td></tr>
</table>

<div class="finding">
<strong>Finding 6: Cross-dataset transfer confirmed, but counting advantage attenuates.</strong> All five framings produce significant signal on DBE-KT22. The top-3 ranking is preserved. However, the 0.131 advantage of prerequisite_chain over teacher on SmartPaper compresses to 0.010 on DBE-KT22. The method transfers; the relative benefit of structured counting over direct judgment does not consistently transfer. Notably, DBE-KT22 also confirms that the Eedi null is not an artifact of MCQ format&mdash;these are also MCQs, but with difficulty varying across 27 knowledge components rather than within a single concept.
</div>

<div class="note">
<strong>Calibration direction reversed.</strong> On SmartPaper, models overestimate student ability (bias +0.11 to +0.43). On DBE-KT22, they underestimate it (bias &minus;0.10 to &minus;0.15). This is consistent with the populations: SmartPaper students (Indian government schools) score lower than models expect; DBE-KT22 students (university CS) score higher (mean p=0.79). Ranking accuracy (Spearman &rho;) is unaffected, but absolute predictions require population-specific calibration.
</div>

<!-- ============================================================ -->
<h2>4. Discussion</h2>

<h3>4.1 Why Counting Prompts Work</h3>

<p>The top prompts (prerequisite_chain, cognitive_load) share a structure: they ask the model to enumerate structural features of the item before estimating difficulty. This design parallels the cognitive design system tradition in psychometrics, where item difficulty is predicted from counts of processing steps, working memory demands, or required knowledge components (Embretson, 1998; Gorin, 2005). The counting prompt externalizes this analysis: instead of the psychometrician coding item features, the LLM does it.</p>

<p>This framing also aligns with Razavi &amp; Powers&rsquo; (2025) finding that LLMs work best as feature extractors rather than direct estimators. In their study, using LLM-extracted cognitive features as inputs to a gradient boosting model reached r=0.87, far exceeding direct LLM prediction. Our counting prompts achieve a similar effect without a separate ML pipeline: the count itself becomes the mediating feature, and the LLM maps it to a difficulty estimate in a single pass.</p>

<p>Importantly, the model is not simulating student cognition. It is analyzing item structure. The signal comes from properties visible in the item text&mdash;the number of steps, the number of concepts, the number of ways to go wrong&mdash;not from modeling how students interact with those properties.</p>

<h3>4.2 Why Structured Prompts Require Capable Models</h3>

<p>The Phase 2 model survey reveals an interaction: counting prompts help capable models but hurt weaker ones. Prerequisite_chain improves Gemini 3 Flash (+0.083 over teacher) but degrades Llama-3.1-8B (&rho;=0.044 vs. 0.300 for teacher). The structured enumeration step requires the model to reliably identify and list item features; when the model cannot do this accurately, the enumeration introduces noise rather than signal.</p>

<p>This finding has practical implications: counting prompts should only be used with models that have demonstrated competence on the task domain. The plain teacher prompt is a more robust default across the capability spectrum.</p>

<h3>4.3 Boundary Conditions</h3>

<p>The Eedi null result (&rho;=0.114 on 105 misconception-targeted MCQs) establishes that LLM difficulty estimation does not work for all item types. Eedi items are diagnostic MCQs designed so that each distractor targets a specific misconception; difficulty depends on the interaction between distractor design and the prevalence of particular misconceptions in the student population. DBE-KT22 items are also MCQs but span 27 knowledge components with difficulty varying in cognitive demand across topics. The method achieves &rho;=0.531 on DBE-KT22, confirming that item format (MCQ vs. open-ended) is not the confound.</p>

<p>What item properties moderate LLM estimation accuracy remains an open question. Candidate moderators include: the breadth of knowledge components sampled, the degree to which difficulty depends on distractor-specific interactions, and the variance of difficulty within vs. between topics. Characterizing these moderators is an important direction for future work.</p>

<h3>4.4 Probe-Set Inflation: A Methodological Warning</h3>

<p>Our Eedi experience is a cautionary tale for the field. A 20-item probe set, stratified by difficulty quintile, yielded &rho;=0.50. The full 105-item set yielded &rho;=0.114. Bootstrap analysis showed the probe was a 2.5th-percentile tail event driven by two extreme outliers. Any study reporting difficulty estimation results on fewer than ~50 items should be interpreted with caution. Stratified sampling, while useful for ensuring range, can amplify apparent signal by overweighting extremes. We recommend that future studies always report confirmation on a full, unstratified item set.</p>

<h3>4.5 DOE as a Methodology for LLM Evaluation</h3>

<p>The sequential DOE approach offered three practical benefits. First, Phase 1 screening across 7 framings identified the best approach before committing to expensive cross-model evaluation. Second, Phase 2 model survey on the same dataset isolated model effects from prompt effects. Third, Phase 3 pre-specified hypotheses protected against post-hoc rationalization of the confirmation results. This pipeline is not specific to difficulty estimation; it could be applied to any LLM task where prompt, model, and domain interact.</p>

<p>The approach follows the logic of sequential experimentation in education technology (Stamper et al., 2012), where findings at one scale inform design at the next. Here, the &ldquo;scales&rdquo; are experimental phases rather than deployment contexts, but the principle is the same: use early results to focus later investment.</p>

<h3>4.6 Limitations</h3>

<ol>
  <li><strong>Three datasets, one model in Phase 3.</strong> Cross-dataset confirmation uses only Gemini 3 Flash. Phase 2 shows model choice matters less than prompt choice, but single-model confirmation is a limitation.</li>
  <li><strong>Not pre-registered.</strong> Phases 1&ndash;2 are exploratory by design. Phase 3 hypotheses were pre-specified before seeing results but not formally registered.</li>
  <li><strong>Calibration requires population-specific adjustment.</strong> Models overestimate ability on SmartPaper (bias +0.11 to +0.43) but underestimate on DBE-KT22 (bias &minus;0.07 to &minus;0.25). Ranking accuracy transfers; absolute predictions do not.</li>
  <li><strong>Information asymmetry.</strong> SmartPaper items are open-ended (no answer options). DBE-KT22 and Eedi items are MCQs (options shown, correct answer withheld in DBE-KT22). These differences limit direct comparison of absolute &rho; values across datasets.</li>
  <li><strong>Surface feature confound.</strong> On SmartPaper, text length correlates &rho;=&minus;0.44 with difficulty. The LLM&rsquo;s &rho;=0.686 exceeds this, but surface features contribute partially to the signal.</li>
  <li><strong>Counting advantage did not transfer.</strong> The 0.131 advantage of prerequisite_chain over teacher on SmartPaper reduced to 0.010 on DBE-KT22. This may reflect dataset-specific properties rather than a general limitation.</li>
</ol>

<!-- ============================================================ -->
<h2>5. Conclusion</h2>

<p>We applied sequential design of experiments to map the parameter landscape for LLM item difficulty estimation across three datasets, seven models, and seven prompt framings. Three findings stand out:</p>

<ol>
  <li><strong>Prompt framing matters most.</strong> Counting prompts grounded in learning science (&rho;=0.633) outperform direct teacher-judgment prompts (&rho;=0.550) on the best model. This approaches or exceeds the meta-analytic average for expert teacher judgment (r=0.63&ndash;0.66; S&uuml;dkamp et al., 2012; Hoge &amp; Coladarci, 1989) in a zero-shot setting.</li>
  <li><strong>The method transfers across datasets but not to all item types.</strong> Counting prompts achieve &rho;=0.531 on a held-out MCQ dataset (168 CS items), but &rho;=0.114 on misconception-targeted MCQs (105 items). What item properties moderate estimation accuracy is an open question.</li>
  <li><strong>Small probe sets are unreliable.</strong> Our Eedi probe (&rho;=0.50, n=20) overstated the true signal (&rho;=0.114, n=105) by 4&times;. Studies in this area should always validate on full item sets.</li>
</ol>

<p>The practical recommendation is simple: prompt capable LLMs to count structural features of items (prerequisite skills, working memory elements, error paths) rather than to make holistic judgments. The counting advantage is model-dependent&mdash;only capable models benefit&mdash;and does not transfer equally across item types. The DOE methodology itself generalizes beyond difficulty estimation to any LLM evaluation task where multiple factors interact.</p>

<!-- ============================================================ -->
<h2>References</h2>

<div class="ref">Benedetto, L., Cremonesi, P., Caines, A., Buttery, P., Cappelli, A., Giussani, A., &amp; Turrin, R. (2023). A survey on recent approaches to question difficulty estimation from text. <em>ACM Computing Surveys</em>, 56(5), 1&ndash;37.</div>
<div class="ref">Box, G. E. P., &amp; Behnken, D. W. (1960). Some new three level designs for the study of quantitative variables. <em>Technometrics</em>, 2(4), 455&ndash;475.</div>
<div class="ref">Box, G. E. P., &amp; Draper, N. R. (2007). <em>Response Surfaces, Mixtures, and Ridge Analyses</em> (2nd ed.). Wiley.</div>
<div class="ref">Brown, J. S., &amp; Burton, R. R. (1978). Diagnostic models for procedural bugs in basic mathematical skills. <em>Cognitive Science</em>, 2(2), 155&ndash;192.</div>
<div class="ref">Embretson, S. E. (1998). A cognitive design system approach to generating valid tests: Application to abstract reasoning. <em>Psychological Methods</em>, 3(3), 380&ndash;396.</div>
<div class="ref">Gorin, J. S. (2005). Manipulating processing difficulty of reading comprehension questions: The feasibility of verbal item generation. <em>Journal of Educational Measurement</em>, 42(4), 351&ndash;373.</div>
<div class="ref">Hoge, R. D., &amp; Coladarci, T. (1989). Teacher-based judgments of academic achievement: A review of literature. <em>Review of Educational Research</em>, 59(3), 297&ndash;313.</div>
<div class="ref">Irvine, S. H., &amp; Kyllonen, P. C. (Eds.). (2002). <em>Item Generation for Test Development</em>. Lawrence Erlbaum Associates.</div>
<div class="ref">Koedinger, K. R., Corbett, A. T., &amp; Perfetti, C. (2012). The Knowledge-Learning-Instruction framework. <em>Cognitive Science</em>, 36(5), 757&ndash;798.</div>
<div class="ref">Kr&ouml;ger, B., et al. (2025). Take out your calculators: LLM-simulated classrooms for item difficulty estimation. arXiv:2601.09953.</div>
<div class="ref">Peters, S., Zhang, N., Jiao, H., Li, M., Zhou, T., &amp; Lissitz, R. (2025). Text-based approaches to item difficulty modeling in large-scale assessments: A systematic review. arXiv:2509.23486.</div>
<div class="ref">Razavi, R., &amp; Powers, D. (2025). Estimating item difficulty using large language models and tree-based machine learning algorithms. arXiv:2504.08804.</div>
<div class="ref">Shulman, L. S. (1986). Those who understand: Knowledge growth in teaching. <em>Educational Researcher</em>, 15(2), 4&ndash;14.</div>
<div class="ref">Stamper, J. C., Lomas, D., Ching, D., Ritter, S., Koedinger, K. R., &amp; Steinhart, J. (2012). The rise of the super experiment. <em>Proceedings of the 5th International Conference on Educational Data Mining (EDM)</em>, 196&ndash;200.</div>
<div class="ref">S&uuml;dkamp, A., Kaiser, J., &amp; M&ouml;ller, J. (2012). Accuracy of teachers&rsquo; judgments of students&rsquo; academic achievement: A meta-analysis. <em>Journal of Educational Psychology</em>, 104(3), 743&ndash;763.</div>
<div class="ref">Sweller, J. (1988). Cognitive load during problem solving. <em>Cognitive Science</em>, 12(2), 257&ndash;285.</div>
<div class="ref">Rogoz, A., &amp; Ionescu, R. T. (2024). UnibucLLM: Harnessing LLMs for automated prediction of item difficulty and response time for multiple-choice questions. <em>BEA Workshop, ACL 2024</em>, 472&ndash;480.</div>
<div class="ref">Yaneva, V., et al. (2024). Findings from the first shared task on automated prediction of difficulty and response time for multiple-choice questions. <em>Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA)</em>, 449&ndash;466.</div>

<hr style="margin:2rem 0">
<p style="font-size:0.8rem; color:var(--muted)">
  Draft generated February 2026. Data, code, and raw model outputs available at [repository URL].
  All experiments used publicly available APIs. Research conducted using Claude Code (Anthropic).
</p>

<!-- ============================================================ -->
<!-- FIGURES (Plotly.js) -->
<!-- ============================================================ -->
<script>
const plotFont = {family: 'Georgia'};
const gridColor = '#eee';

// ============================================================
// Figure 1: Phase 1 Prompt Screening
// ============================================================
{
  const framings = [
    'prerequisite<br>chain', 'cognitive<br>load', 'devil<br>advocate',
    'error<br>analysis', 'teacher', 'error<br>affordance', 'familiarity<br>gradient'
  ];
  const rhos = [0.686, 0.673, 0.596, 0.596, 0.555, 0.493, 0.483];
  const colors = rhos.map(r => r >= 0.65 ? '#16a34a' : r >= 0.55 ? '#2563eb' : '#d97706');

  Plotly.newPlot('phase1Bar', [{
    type: 'bar', x: framings, y: rhos,
    marker: {color: colors},
    text: rhos.map(r => '&rho;=' + r.toFixed(3)),
    textposition: 'outside', textfont: {size: 11},
  }], {
    height: 380, margin: {l:60, r:20, t:20, b:90},
    yaxis: {title:'Spearman &rho; (140 items)', range:[0, 0.8], gridcolor: gridColor},
    font: plotFont,
    shapes: [{
      type:'line', x0:-0.5, x1:6.5, y0:0.555, y1:0.555,
      line:{color:'#999', width:2, dash:'dash'},
    }],
    annotations: [{
      x:6, y:0.57, text:'teacher baseline', showarrow:false,
      font:{size:10, color:'#999'},
    }],
  });
}

// ============================================================
// Figure 2: Phase 2 Model Ã— Prompt Heatmap
// ============================================================
{
  const models = ['Gemini 3 Flash', 'Gemma-3-27B', 'GPT-4o', 'Llama-3.3-70B', 'Llama-4-Scout', 'Qwen3-32B', 'Llama-3.1-8B'];
  const prompts = ['Teacher', 'Cognitive Load', 'Prerequisite Chain'];
  const z = [
    [0.550, 0.619, 0.633],
    [0.501, 0.500, 0.464],
    [0.494, null, null],
    [0.480, 0.403, 0.396],
    [0.474, 0.400, 0.345],
    [0.304, null, null],
    [0.300, 0.214, 0.044],
  ];
  Plotly.newPlot('phase2Heatmap', [{
    z: z, x: prompts, y: models, type: 'heatmap',
    colorscale: [[0,'#fee2e2'],[0.3,'#fef9c3'],[0.6,'#dcfce7'],[1,'#16a34a']],
    zmin: -0.2, zmax: 0.6,
    text: z.map(row => row.map(v => '&rho;=' + v.toFixed(3))),
    texttemplate: '%{text}', textfont: {size: 12},
    hoverinfo: 'z',
    colorbar: {title: '&rho;', titleside: 'right'},
  }], {
    height: 400, margin: {l: 150, r: 80, t: 20, b: 80},
    font: plotFont,
  });
}

// ============================================================
// Figure 3: Three-Dataset Comparison
// ============================================================
{
  Plotly.newPlot('dissociationPlot', [
    {
      x: ['Probe (20)', 'Full set'],
      y: [0.462, 0.114],
      name: 'Eedi MCQs (misconception)',
      type: 'bar',
      marker: {color: '#dc2626'},
      text: ['&rho;=0.462', '&rho;=0.114 (ns)'],
      textposition: 'outside', textfont: {size: 12},
    },
    {
      x: ['Probe (20)', 'Full set'],
      y: [0.768, 0.686],
      name: 'SmartPaper (open-ended)',
      type: 'bar',
      marker: {color: '#2563eb'},
      text: ['&rho;=0.768', '&rho;=0.686'],
      textposition: 'outside', textfont: {size: 12},
    },
  ], {
    height: 370, margin: {l:60, r:20, t:30, b:50},
    barmode: 'group',
    yaxis: {title:'Spearman &rho;', range:[-0.1, 0.95], gridcolor: gridColor},
    font: plotFont,
    legend: {x: 0.45, y: 0.95},
    shapes: [{type:'line', x0:-0.5, x1:1.5, y0:0, y1:0, line:{color:'#999',width:1,dash:'dash'}}],
    annotations: [
      {x:'Full set', y:-0.05, text:'n=105', showarrow:false, font:{size:10, color:'#dc2626'}},
      {x:'Full set', y:0.63, text:'n=140', showarrow:false, font:{size:10, color:'#2563eb'}},
    ],
  });
}

// ============================================================
// Figure 4: SmartPaper by subject
// ============================================================
{
  const subjects = ['Science', 'Social Science', 'Mathematics', 'English'];
  const rhos = [0.734, 0.702, 0.600, 0.432];
  const ns = [32, 34, 33, 41];
  const colors = rhos.map(r => r >= 0.6 ? '#16a34a' : r >= 0.4 ? '#2563eb' : '#d97706');

  Plotly.newPlot('subjectPlot', [{
    type: 'bar', x: subjects, y: rhos,
    marker: {color: colors},
    text: rhos.map((r,i) => '&rho;=' + r.toFixed(3) + ' (n=' + ns[i] + ')'),
    textposition: 'outside', textfont: {size: 11},
  }], {
    height: 320, margin: {l:60, r:20, t:20, b:50},
    yaxis: {title:'Spearman &rho;', range:[0, 0.85], gridcolor: gridColor},
    font: plotFont,
  });
}

// ============================================================
// Figure 5: Phase 3 Cross-Dataset Confirmation
// ============================================================
{
  const framings = ['prerequisite_chain', 'teacher', 'error_analysis', 'cognitive_load', 'devil_advocate'];
  const sp = [0.686, 0.555, 0.596, 0.596, 0.493];
  const dbe = [0.531, 0.521, 0.516, 0.469, 0.420];

  Plotly.newPlot('phase3Bar', [
    {type:'bar', name:'SmartPaper (Phase 1)', x:framings, y:sp,
     marker:{color:'#2563eb'},
     text:sp.map(r=>'&rho;='+r.toFixed(3)), textposition:'outside', textfont:{size:11}},
    {type:'bar', name:'DBE-KT22 (Phase 3)', x:framings, y:dbe,
     marker:{color:'#16a34a'},
     text:dbe.map(r=>'&rho;='+r.toFixed(3)), textposition:'outside', textfont:{size:11}},
  ], {
    height:380, margin:{l:60,r:20,t:30,b:120}, barmode:'group',
    yaxis:{title:'Spearman &rho;', range:[0,0.8], gridcolor:gridColor},
    legend:{x:0.55, y:0.95},
    font:plotFont,
    shapes:[{type:'line',x0:-0.5,x1:4.5,y0:0.3,y1:0.3,
      line:{color:'#999',width:1.5,dash:'dot'}}],
    annotations:[{x:3.2,y:0.32,text:'H1 threshold (&rho;=0.30)',
      showarrow:false,font:{size:9,color:'#999'}}],
  });
}
</script>

</body>
</html>
