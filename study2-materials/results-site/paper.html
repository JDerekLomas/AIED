<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Mapping the Parameter Landscape for LLM Item Difficulty Estimation</title>
<script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>
<style>
  :root {
    --bg: #fafafa; --card: #fff; --text: #1a1a1a; --muted: #666;
    --accent: #2563eb; --accent2: #dc2626; --border: #e5e7eb;
    --green: #16a34a; --amber: #d97706;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body {
    font-family: 'Georgia', 'Times New Roman', serif;
    background: var(--bg); color: var(--text);
    line-height: 1.7; max-width: 900px; margin: 0 auto;
    padding: 2rem 1.5rem;
  }
  h1 { font-size: 1.7rem; line-height: 1.3; margin-bottom: 0.3rem; }
  h2 { font-size: 1.3rem; margin: 2.5rem 0 0.8rem; border-bottom: 2px solid var(--accent); padding-bottom: 0.3rem; }
  h3 { font-size: 1.1rem; margin: 1.5rem 0 0.5rem; }
  h4 { font-size: 1rem; margin: 1.2rem 0 0.4rem; }
  p { margin-bottom: 1rem; }
  .authors { color: var(--muted); font-size: 0.95rem; margin-bottom: 0.3rem; }
  .venue { color: var(--muted); font-size: 0.9rem; font-style: italic; margin-bottom: 1.5rem; }
  .abstract {
    background: #f0f4ff; border-left: 4px solid var(--accent);
    padding: 1rem 1.2rem; margin: 1.5rem 0; font-size: 0.95rem;
  }
  .abstract strong { display: block; margin-bottom: 0.3rem; }
  table {
    width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem;
  }
  th, td { padding: 0.4rem 0.6rem; border: 1px solid var(--border); text-align: left; }
  th { background: #f3f4f6; font-weight: 600; }
  tr.highlight { background: #eff6ff; }
  .note {
    background: #fffbeb; border-left: 4px solid var(--amber);
    padding: 0.8rem 1rem; margin: 1rem 0; font-size: 0.9rem;
  }
  .finding {
    background: #f0fdf4; border-left: 4px solid var(--green);
    padding: 0.8rem 1rem; margin: 1rem 0; font-size: 0.9rem;
  }
  .plot-container { width: 100%; margin: 1rem 0; }
  .figure-caption { font-size: 0.85rem; color: var(--muted); margin-top: 0.3rem; font-style: italic; }
  code { background: #f3f4f6; padding: 0.15rem 0.4rem; border-radius: 3px; font-size: 0.88rem; }
  .ref { font-size: 0.88rem; margin-bottom: 0.5rem; }
  .keyword { display: inline-block; background: #e0e7ff; color: #3730a3; padding: 0.1rem 0.5rem; border-radius: 12px; font-size: 0.8rem; margin: 0.1rem; }
  sup { font-size: 0.7rem; }
  a { color: var(--accent); }
  .two-col { display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; }
  @media (max-width: 700px) { .two-col { grid-template-columns: 1fr; } }
  .stat { font-family: 'Courier New', monospace; font-weight: 600; }
  .null { color: var(--accent2); }
  .sig { color: var(--green); }
  ol, ul { margin: 0.5rem 0 1rem 1.5rem; }
  li { margin-bottom: 0.3rem; }
</style>
</head>
<body>

<h1>Mapping the Parameter Landscape for LLM Item Difficulty Estimation: A Sequential Design-of-Experiments Study</h1>
<div class="authors">Derek Lomas<sup>1</sup></div>
<div class="venue">Submission to AIED 2026 &middot; Draft February 2026</div>

<div style="margin-bottom:1rem">
  <span class="keyword">item difficulty estimation</span>
  <span class="keyword">LLM evaluation</span>
  <span class="keyword">design of experiments</span>
  <span class="keyword">mathematics education</span>
  <span class="keyword">psychometrics</span>
</div>

<div class="abstract">
  <strong>Abstract</strong>
  Can LLMs estimate how difficult a test item is for students? Published correlations range from r&asymp;0 to r=0.83, but each study tests one method on one dataset. We map the full parameter landscape using sequential design of experiments across two datasets&mdash;1,869 misconception-targeted maths MCQs (Eedi, UK) and 140 open-ended items across four subjects (SmartPaper, India)&mdash;testing 15+ models, 20+ prompt configurations, and multiple structured elicitation strategies grounded in four learning science traditions.

  <p style="margin-top:0.5rem">The central finding is a dissociation by item type. On open-ended items, even simple direct estimation achieves <span class="sig">&rho;=0.55</span> (n=140, p&lt;0.0001), generalising to 120 held-out items (&rho;=0.52). On misconception-targeted MCQs, the best optimised configuration achieves &rho;=0.50 on 20 probe items but collapses to <span class="null">&rho;=0.11</span> (n=105, non-significant, 95% CI [&minus;0.07, 0.30]) on confirmation items. No combination of model, prompt, temperature, or elicitation strategy overcomes this boundary.</p>

  <p style="margin-top:0.5rem">We report a registry of 18 tested hypotheses, most producing null or negative results: cognitive scaffolding hurts (misconception hints: &rho;=0.19 vs. 0.50 baseline), deliberation hurts (reasoning models: &rho;=0.05 vs. 0.50), two-stage diversity injection destroys signal at scale (&rho;=0.06 vs. 0.55), and model size does not predict success (GPT-4o &rho;=0.17 vs. Llama-4-Scout 17B &rho;=0.36). These negative results, together with a framework distinguishing complexity-driven from selectivity-driven difficulty, provide actionable guidance for researchers and practitioners.</p>
</div>

<!-- ============================================================ -->
<h2>1. Introduction</h2>

<p>Can LLMs estimate how difficult a test item is for students? The answer in the literature ranges from &ldquo;not at all&rdquo; to &ldquo;very well.&rdquo; Direct estimation studies report correlations from r&asymp;0 (Kr&ouml;ger et al., 2025) to r=0.83 (Razavi &amp; Powers, 2025). Classroom simulation approaches range from r=0.01 (our replication) to r=0.82 (Kr&ouml;ger et al., 2025). Feature extraction ranges from r=0.06 (our replication at scale) to r=0.87 (Razavi &amp; Powers, 2025). These are not minor discrepancies&mdash;they span the entire range from chance to near-perfect prediction.</p>

<p>Why the disagreement? Each study tests one method on one dataset and reports a single correlation. But difficulty estimation involves a large parameter space: the choice of model, prompt strategy, sampling temperature, aggregation method, and&mdash;critically&mdash;the type of items being estimated. A single-configuration study cannot distinguish between &ldquo;this method works&rdquo; and &ldquo;this method works on these items, with this model, at this temperature.&rdquo; The result is a literature of point estimates that cannot be compared or reconciled.</p>

<p>We take a different approach. Rather than testing one pipeline and reporting a correlation, we conduct a systematic search across the parameter landscape using sequential design of experiments (DOE)&mdash;beginning with a Box-Behnken response surface design (Box &amp; Behnken, 1960) for screening, then progressing through optimisation, boundary testing, and confirmation phases. DOE was developed precisely for situations like this: the outcome depends on multiple continuous and categorical factors whose interactions are unknown, individual experiments are cheap, and the goal is to map the response surface rather than test a single hypothesis.</p>

<p>We apply this methodology to two datasets: 1,869 misconception-targeted maths MCQs from Eedi (UK) and 140 open-ended items across four subjects from SmartPaper (India). Across these two datasets, we test 15+ models, 20+ prompt configurations, 6 temperature settings, and multiple structured elicitation strategies grounded in four learning science traditions. The result is not a single correlation but a map&mdash;one that reveals a dissociation by item type that reconciles the conflicting findings in the literature, and a set of boundary conditions that narrow the space of viable methods.</p>

<h3>Theoretical Traditions Under Test</h3>

<p>The prompt strategies we test are grounded in four theoretical traditions, each offering a different account of why students make errors:</p>

<ol>
  <li><strong>Knowledge Component theory</strong> (Koedinger, Corbett &amp; Perfetti, 2012): Difficulty reflects the knowledge components an item requires. Motivates teacher-prediction framing.</li>
  <li><strong>Buggy Procedures</strong> (Brown &amp; Burton, 1978): Students apply internally-consistent but flawed algorithms. Motivates production-rule prompts specifying step-by-step erroneous procedures.</li>
  <li><strong>Conceptual Change theory</strong> (Chi, Slotta &amp; de Leeuw, 1994): Students hold coherent-but-wrong mental models. Motivates prompts describing flawed beliefs and why they feel right.</li>
  <li><strong>Epistemic State Specification</strong> (Tack &amp; Piech, 2024): A taxonomy of student simulation prompts from unspecified (E0) to data-calibrated (E4). Motivates increasingly detailed cognitive specifications.</li>
</ol>

<p>All four predict that more scaffolding should help. Our experiments test this prediction&mdash;and find it wrong.</p>

<h3>Methodological Note</h3>
<p>This research was conducted through iterative optimisation with Claude Code (Anthropic), an AI-assisted programming environment. The experimental sequence was not pre-registered; each experiment informed the next in a sequential DOE pipeline. We address the resulting multiple-comparisons concern in three ways: (1) 10-rep stability testing showed our initial 3-rep estimates were inflated by ~0.08&ndash;0.10, and we report the corrected numbers; (2) the definitive test is a held-out 105-item confirmation on Eedi, which returned &rho;=0.11 (null); (3) all prompts, scripts, raw model outputs, and analysis code are publicly available. We report both successes and failures transparently.</p>

<!-- ============================================================ -->
<h2>2. Method</h2>

<h3>2.1 Design Framework: Sequential Experimentation</h3>

<p>We adopt a sequential design-of-experiments (DOE) pipeline, standard in industrial process optimisation but novel in LLM evaluation. The pipeline has five phases, each answering a question that determines the next:</p>

<p><strong>Phase 1&mdash;Screening (Box-Behnken RSM).</strong> A response surface design (Box &amp; Behnken, 1960) varying five factors across 46 configurations: prompt style (individual roleplay / classroom batch / teacher prediction), temperature (0.3 / 0.9 / 1.5), batch size (1 / 5 / 30 simulated students per call), misconception hint level (hidden / partial / full), and model (Gemini 2.5 Flash / Gemini 3 Flash). Each configuration was evaluated on 20 probe items with 20 simulated responses per item (18,400 API calls in this phase). The objective function was Spearman &rho; between simulated difficulty and IRT b<sub>2PL</sub>. This phase identified one viable region: teacher-prediction framing at high temperature.</p>

<p><strong>Phase 2&mdash;Optimisation (prompt sweep).</strong> Within the viable region, we tested 7 prompt variants &times; 3 temperatures (1.5, 1.8, 2.0) &times; 3 replications = 63 configurations. Each configuration predicted difficulty for the same 20 probe items. Prompts ranged from sparse (&ldquo;estimate difficulty&rdquo;) to structured (&ldquo;identify specific calculation errors students would make&rdquo;). This phase identified two top configurations: error analysis (&rho;=0.50 per-rep, 10-rep validated) and contrastive prompting (&rho;=0.51 per-rep).</p>

<p><strong>Phase 3&mdash;Stability and boundary testing.</strong> Three parallel experiments:</p>
<ul>
  <li><em>10-rep stability:</em> Top 2 configs run for 10 replications each, revealing that 3-rep estimates were optimistically biased by ~0.08&ndash;0.10.</li>
  <li><em>Cross-model survey:</em> 15 models across 5 providers (Google, OpenAI, Anthropic, Groq, DeepSeek), revealing a three-tier structure with no correlation between model size/benchmark ranking and task performance.</li>
  <li><em>Structured elicitation:</em> Cognitive modeling, buggy reasoning, student personas, two-stage diversity injection, and calibration anchors&mdash;testing predictions from four theoretical traditions. All matched or underperformed direct prediction.</li>
</ul>

<p><strong>Phase 4&mdash;Generalisation.</strong> The critical test: does the signal hold beyond the 20-item probe set?</p>
<ul>
  <li><em>Eedi expansion:</em> 20 &rarr; 50 items (adding 30 random), then 105 items with IRT b<sub>2PL</sub> parameters. Signal collapsed: &rho;=0.04 (50 items), &rho;=0.11 (105 items, non-significant).</li>
  <li><em>SmartPaper expansion:</em> 20 &rarr; 140 items. Signal held: &rho;=0.55 (140 items, p&lt;0.0001), &rho;=0.52 on 120 held-out items.</li>
</ul>

<p><strong>Phase 5&mdash;Mechanistic investigation.</strong> Controlled temperature sweeps (6 temperatures &times; 3 reps, prompt held constant) to disentangle the prompt &times; temperature confound from Phase 1. Two-stage experiments on SmartPaper&rsquo;s full 134 items to test whether context-based elicitation generalises. Temperature sweeps on SmartPaper with 2 models &times; 3 strategies &times; 4 temperatures to characterise the three-way interaction.</p>

<p>The total experiment involved approximately 25,000 API calls across all phases, at a cost of approximately $50.</p>

<h3>2.2 Datasets</h3>

<div class="two-col">
<div>
<h4>Eedi (UK, MCQ)</h4>
<ul>
  <li><strong>Items:</strong> 1,869 diagnostic maths questions (105 with IRT parameters)</li>
  <li><strong>Format:</strong> Multiple-choice, 4 options; each distractor targets a specific misconception</li>
  <li><strong>Students:</strong> UK, ages 11&ndash;16 (73,000+ responses)</li>
  <li><strong>Difficulty metric:</strong> IRT b<sub>2PL</sub> (105 items) and classical p-value (1,869 items)</li>
  <li><strong>Difficulty range:</strong> b<sub>2PL</sub> = &minus;3.5 to +3.5</li>
  <li><strong>Probe set:</strong> 20 items stratified by difficulty quintile, selected <em>before seeing any LLM results</em> to test five misconception areas (inverse operations, order of operations, fraction arithmetic, negative number operations, place value)</li>
  <li><strong>Confirmation set:</strong> 105 items with sufficient response volume for stable IRT parameter estimation</li>
</ul>
</div>
<div>
<h4>SmartPaper (India, Open-ended)</h4>
<ul>
  <li><strong>Items:</strong> 140 questions across 4 subjects</li>
  <li><strong>Format:</strong> Open-ended with rubric-based scoring</li>
  <li><strong>Students:</strong> Indian government schools, Grades 6&ndash;8</li>
  <li><strong>Difficulty metric:</strong> Classical difficulty (proportion correct)</li>
  <li><strong>Subjects:</strong> English, Maths, Science, Social Science</li>
  <li><strong>Difficulty range:</strong> 0.04&ndash;0.83 (mean 0.29)</li>
  <li><strong>Probe set:</strong> 20 items; full set: 140 items (134 non-visual)</li>
</ul>
</div>
</div>

<h3>2.3 Ground Truth and Data Quality</h3>

<p><strong>Eedi answer ordering.</strong> During this research, we discovered that the Eedi dataset contains two different answer orderings: the Kaggle ordering (how answer texts map to A/B/C/D labels in the data) and the NeurIPS competition ordering (how answers were presented to students on screen). These match only 24.5% of the time. All analyses in this paper use the NeurIPS ordering (student-facing), which is the correct basis for computing difficulty.</p>

<p><strong>IRT parameter estimation.</strong> For the 105-item confirmation set, we fitted 2PL IRT models using maximum likelihood estimation on the full response matrix (15.8M responses). Parameters were validated against the original Eedi-provided parameters at &rho;=1.000 for the overlapping items.</p>

<p><strong>SmartPaper calibration.</strong> Models systematically overestimate item easiness by approximately +0.40 (predicted mean p=0.73 vs. actual mean p=0.29), likely reflecting higher-performing populations in training data. All correlations are rank-order (Spearman &rho;), which is invariant to this monotone miscalibration.</p>

<h3>2.4 Prompt Architecture</h3>

<p>The RSM screening phase (Phase 1) tested three prompt framings: <em>individual roleplay</em> (simulate a single student answering), <em>classroom batch</em> (simulate N students answering together), and <em>teacher prediction</em> (predict what proportion of students at each ability level would choose each option). Only teacher prediction produced significant signal.</p>

<p>All prompts in the viable region share the teacher-prediction structure: the model is asked to predict what percentage of students at each of four ability levels (below basic, basic, proficient, advanced) would choose each answer option. The predicted difficulty is computed as a weighted average of predicted proportion incorrect, with weights reflecting a typical school population (below basic: 0.25, basic: 0.35, proficient: 0.25, advanced: 0.15).</p>

<p>In Phase 2, we tested seven variants of this teacher-prediction prompt, differing in what additional reasoning they request. The two best-performing were:</p>

<p><strong>Contrastive.</strong> Asks the model to consider what makes <em>this specific item</em> harder or easier than similar items, then predict response distributions. Frames the task as comparative judgment.</p>

<p><strong>Error analysis.</strong> Asks the model whether students would <em>actually make errors</em> on this item&mdash;specifically, whether the wrong answers represent errors that real students would plausibly commit. Frames the task as error plausibility assessment.</p>

<p>Both prompts redirect the model from generic difficulty judgment (&ldquo;how hard is this?&rdquo;) toward specific error analysis (&ldquo;would students actually get this wrong, and why?&rdquo;). This framing accesses the model&rsquo;s implicit pedagogical content knowledge rather than asking it to reason about difficulty in the abstract.</p>

<h3>2.5 Models</h3>

<p>We tested 15+ models across 5 providers. The primary models:</p>

<table>
<tr><th>Model</th><th>Provider</th><th>Key characteristic</th></tr>
<tr><td>Gemini 3 Flash</td><td>Google</td><td>Best on Eedi probe (&rho;=0.50 per-rep); strong on SmartPaper (&rho;=0.55)</td></tr>
<tr><td>Llama-4-Scout 17B</td><td>Groq</td><td>Free; high variance (SD=0.17) but &rho;=0.67 with 10-rep averaging</td></tr>
<tr><td>DeepSeek V3</td><td>DeepSeek</td><td>Best intrinsic calibration on SmartPaper (&rho;=0.80 at low temp)</td></tr>
<tr><td>GPT-4o</td><td>OpenAI</td><td>Weak on Eedi probe (&rho;=0.17) despite strong general benchmarks</td></tr>
</table>

<h3>2.6 Statistical Approach</h3>

<p>All correlations are Spearman rank correlations (&rho;). Significance is assessed at &alpha;=0.05 (two-tailed). For the 105-item confirmation test, we report bootstrap 95% confidence intervals (10,000 resamples).</p>

<p>We do not correct for multiple comparisons across the screening and optimisation phases, because these phases are exploratory by design. The confirmatory test is the 105-item expansion (Phase 4), which is a single pre-planned comparison. This is standard DOE practice: screening experiments identify promising regions; confirmation experiments provide the inferential test.</p>

<p>For multi-rep experiments, we report both per-rep &rho; (mean &plusmn; SD across replications) and averaged-prediction &rho;. The latter is consistently higher because averaging reduces noise, but we treat the per-rep numbers as the honest estimate of single-pass performance.</p>

<h3>2.7 Hypothesis Registry</h3>

<p>We organize the investigation as a hypothesis table. Each row is drawn from a specific claim in the prior literature or from an intermediate finding in our own pipeline.</p>

<table>
<tr>
  <th>ID</th><th>Hypothesis</th><th>Source</th><th>Eedi Result</th><th>SmartPaper Result</th>
</tr>
<tr>
  <td>H1</td>
  <td>Direct estimation yields moderate-to-strong correlations</td>
  <td>Attali (2024), Yaneva et al., Benedetto et al.</td>
  <td class="null">r&asymp;0 (5 models &times; 4 prompts, n=1,869)</td>
  <td class="sig">&rho;=0.55&ndash;0.65 (n=140)</td>
</tr>
<tr>
  <td>H2</td>
  <td>Student simulation recovers difficulty ordering</td>
  <td>Lu &amp; Wang (2024), SMART (Lan et al., 2025)</td>
  <td class="null">&rho;&asymp;0.1&ndash;0.3 (roleplay, classroom batch)</td>
  <td>&mdash;</td>
</tr>
<tr>
  <td>H3</td>
  <td>Teacher prediction framing outperforms student simulation</td>
  <td>&ldquo;Take Out Your Calculators&rdquo; (2026)</td>
  <td class="sig">&rho;=0.50 per-rep; 0.57 averaged</td>
  <td class="sig">&rho;=0.66 (baseline calibration)</td>
</tr>
<tr>
  <td>H4</td>
  <td>Misconception hints improve estimation</td>
  <td>Eedi competition literature</td>
  <td class="null">Full hints &rho;=0.12 (worse than hidden &rho;=0.41)</td>
  <td>&mdash;</td>
</tr>
<tr>
  <td>H5</td>
  <td>Higher temperature improves difficulty discrimination</td>
  <td>Novel (RSM screening)</td>
  <td class="null">Flat (&rho;&asymp;0.35&ndash;0.58 across t=0.3&ndash;2.0 with contrastive prompt); original &ldquo;cliff&rdquo; was prompt&times;temp confound</td>
  <td>Model-dependent: Gemini &uarr; (0.81&rarr;0.88); DeepSeek &darr; (0.80&rarr;0.68)</td>
</tr>
<tr>
  <td>H6</td>
  <td>Contrastive prompting stabilizes estimates</td>
  <td>Novel (metaprompt sweep)</td>
  <td class="sig">&rho;=0.577&plusmn;0.075 (most stable)</td>
  <td>&mdash;</td>
</tr>
<tr>
  <td>H7</td>
  <td>Error analysis prompting yields highest mean correlation</td>
  <td>Novel (metaprompt sweep)</td>
  <td class="sig">&rho;=0.604&plusmn;0.062 (highest mean)</td>
  <td>&mdash;</td>
</tr>
<tr>
  <td>H8</td>
  <td>Error information in calibration boosts estimation</td>
  <td>Novel</td>
  <td>&mdash;</td>
  <td class="sig">&rho;=0.83 (errors strategy)</td>
</tr>
<tr>
  <td>H9</td>
  <td>Model choice matters more than prompt choice</td>
  <td>Cross-model comparison</td>
  <td class="sig">Gemini Flash >> Llama-70B >> Qwen/GPT-4o-mini</td>
  <td>DeepSeek &ge; Gemini at baseline; Gemini wins with structured prompts at high temp</td>
</tr>
<tr>
  <td>H10</td>
  <td>Averaging across multiple runs improves estimates</td>
  <td>Wisdom-of-crowds literature</td>
  <td class="sig">Averaged &rho; exceeds mean single-rep &rho; consistently</td>
  <td>&mdash;</td>
</tr>
<tr>
  <td>H11</td>
  <td>Optimized configs generalize to held-out items</td>
  <td>Confirmation experiment</td>
  <td class="null"><strong>NO.</strong> 105-item: &rho;=0.114 (probe 20: &rho;=0.46; new 85: null)</td>
  <td class="sig"><strong>YES.</strong> 140-item: &rho;=0.547 (probe 20: &rho;=0.77; held-out 120: &rho;=0.52)</td>
</tr>
</table>

<!-- ============================================================ -->
<h2>3. Results</h2>

<p>We present results in the order of the DOE pipeline: screening, optimisation, boundary testing, and generalisation. The key result&mdash;the item-type dissociation&mdash;emerges in Section 3.4.</p>

<h3>3.1 Screening: The Null Wall and One Viable Region</h3>

<p><strong>Direct estimation at scale (Eedi, n=1,869).</strong> We tested direct difficulty estimation using 5 models &times; 4 prompt variants at temperature 0.</p>

<table>
<tr><th>Model</th><th>Best r</th><th>n</th></tr>
<tr><td>GPT-4o-mini</td><td>+0.010</td><td>1,869</td></tr>
<tr><td>Llama-3.3-70B</td><td>&minus;0.032</td><td>1,530</td></tr>
<tr><td>Llama-4-Scout</td><td>&minus;0.013</td><td>1,868</td></tr>
<tr><td>Llama-4-Maverick</td><td>&minus;0.024</td><td>1,735</td></tr>
<tr><td>Qwen3-32B</td><td>+0.015</td><td>1,548</td></tr>
</table>
<p style="font-size:0.85rem; color:var(--muted)">All correlations indistinguishable from zero.</p>

<p><strong>Feature extraction + ML (n=1,869).</strong> GPT-4o extracted 7 features; gradient boosted machine trained on 80% split. Test r=0.063. An initial pilot on 50 items had shown r=0.77 on 10 test items&mdash;classic small-sample overfitting.</p>

<p><strong>Classroom simulation (n=1,869).</strong> Llama-3.3-70B simulated 20 students &times; 4 ability levels per item. r=0.013. The model produces near-identical response distributions regardless of item difficulty.</p>

<p><strong>Box-Behnken screening (n=20 probe items).</strong> Of 46 configurations, only one region produced significant signal: teacher-prediction framing at high temperature.</p>

<table>
<tr><th>Prompt style</th><th>t=0.3</th><th>t=0.9</th><th>t=1.5</th></tr>
<tr><td>Individual roleplay</td><td>0.307</td><td>&mdash;</td><td>0.306</td></tr>
<tr><td>Classroom batch</td><td>0.211</td><td>&mdash;</td><td>0.325</td></tr>
<tr class="highlight"><td>Teacher prediction</td><td>0.119</td><td>&mdash;</td><td class="sig">0.673*</td></tr>
</table>
<p style="font-size:0.85rem; color:var(--muted)">*Single run; later validated at &rho;&asymp;0.50 (10-rep mean). The apparent &ldquo;temperature cliff&rdquo; was later shown to be a prompt&times;temperature confound (see Section 3.5).</p>

<div id="rsmHeatmap" class="plot-container" style="min-height:300px"></div>
<div class="figure-caption">Figure 1. Box-Behnken screening results. Only teacher-prediction framing at high temperature produced signal. The t=1.5 outlier (&rho;=0.67) was later shown to be confounded with prompt quality.</div>

<h3>3.2 Optimisation: Prompt Design Is the Primary Variable</h3>

<p>Within the viable region, we tested 7 prompt variants &times; 3 temperatures &times; 3 replications.</p>

<table>
<tr><th>Prompt</th><th>t=1.5</th><th>t=1.8</th><th>t=2.0</th></tr>
<tr class="highlight"><td>Error analysis</td><td>0.451 &plusmn; 0.049</td><td>0.566 &plusmn; 0.108</td><td class="sig"><strong>0.604 &plusmn; 0.062</strong></td></tr>
<tr class="highlight"><td>Contrastive</td><td class="sig"><strong>0.577 &plusmn; 0.075</strong></td><td>0.518 &plusmn; 0.061</td><td>0.562 &plusmn; 0.084</td></tr>
<tr><td>Devil&rsquo;s advocate</td><td>0.543 &plusmn; 0.020</td><td>0.405 &plusmn; 0.035</td><td>0.516 &plusmn; 0.122</td></tr>
<tr><td>Imagine classroom</td><td>0.232 &plusmn; 0.041</td><td>0.404 &plusmn; 0.035</td><td>0.428 &plusmn; 0.115</td></tr>
<tr><td>Sparse</td><td class="null">0.235 &plusmn; 0.189</td><td class="null">0.111 &plusmn; 0.032</td><td class="null">0.232 &plusmn; 0.175</td></tr>
</table>
<p style="font-size:0.85rem; color:var(--muted)">All results on 20 probe items. The range within a column (prompt effect) far exceeds the range within a row (temperature effect).</p>

<div id="promptSweep" class="plot-container" style="min-height:350px"></div>
<div class="figure-caption">Figure 2. Prompt variant comparison at t=2.0 (20 probe items, 3 reps). Prompt design creates a 3&times; difference in correlation. Error bars are &plusmn;1 SD.</div>

<p><strong>10-rep validation.</strong> The top two configurations were indistinguishable after 10 replications:</p>

<table>
<tr><th>Config</th><th>3-rep estimate</th><th>10-rep per-rep mean</th><th>10-rep averaged &rho;</th></tr>
<tr><td>Error analysis t=2.0</td><td>0.604 &plusmn; 0.062</td><td>0.500 &plusmn; 0.111</td><td>0.573</td></tr>
<tr><td>Contrastive t=1.5</td><td>0.577 &plusmn; 0.075</td><td>0.513 &plusmn; 0.097</td><td>0.571</td></tr>
</table>

<div class="note">
<strong>3-rep estimates were optimistically biased by ~0.10.</strong> True per-rep performance is &rho;&asymp;0.50. Averaged predictions converge to &rho;&asymp;0.57, with diminishing returns beyond 7 reps.
</div>

<h3>3.3 Boundary Testing</h3>

<h4>3.3.1 Cross-Model Survey</h4>

<p>Using the contrastive prompt at t=1.5, we tested 15 models across 5 providers (3 reps each).</p>

<table>
<tr><th>Tier</th><th>Model</th><th>Provider</th><th>Per-rep &rho;</th><th>SD</th><th>Avg-pred &rho;</th></tr>
<tr class="highlight"><td rowspan="3">1</td><td>Gemini 3 Flash</td><td>Google</td><td class="sig">0.470</td><td>0.076</td><td class="sig">0.549</td></tr>
<tr class="highlight"><td>Llama-4-Scout 17B</td><td>Groq</td><td>0.355*</td><td>0.167*</td><td class="sig">0.668*</td></tr>
<tr class="highlight"><td>DeepSeek V3</td><td>DeepSeek</td><td>0.338</td><td>0.140</td><td>0.409</td></tr>
<tr><td rowspan="4">2</td><td>GPT-OSS-120B</td><td>Groq</td><td>0.336</td><td>0.100</td><td>0.450</td></tr>
<tr><td>Kimi-K2</td><td>Groq</td><td>0.336</td><td>0.116</td><td>0.319</td></tr>
<tr><td>Llama-3.3-70B</td><td>Groq</td><td>0.257</td><td>0.062</td><td>&mdash;</td></tr>
<tr><td>GPT-4o</td><td>OpenAI</td><td class="null">0.172</td><td>0.126</td><td>0.204</td></tr>
<tr><td rowspan="4">3</td><td>Qwen3-32B</td><td>Groq</td><td class="null">0.005</td><td>0.110</td><td>&mdash;</td></tr>
<tr><td>Llama-3.1-8B</td><td>Groq</td><td class="null">&minus;0.081</td><td>0.097</td><td>&minus;0.009</td></tr>
<tr><td>GPT-4o-mini</td><td>OpenAI</td><td class="null">&minus;0.092</td><td>0.266</td><td>&mdash;</td></tr>
<tr><td>Gemini 2.0 Flash</td><td>OpenRouter</td><td class="null">&minus;0.200</td><td>0.132</td><td>&minus;0.269</td></tr>
</table>
<p style="font-size:0.85rem; color:var(--muted)">*Scout values from 10-rep validation with error analysis at t=2.0. All on 20 probe items.</p>

<div id="crossModelPlot" class="plot-container" style="min-height:400px"></div>
<div class="figure-caption">Figure 3. Cross-model survey (20 probe items). No correlation between model benchmark performance and task performance. GPT-4o (frontier) is outperformed by Scout 17B (free).</div>

<h4>3.3.2 Deliberation Hurts</h4>

<table>
<tr><th>Model</th><th>Thinking</th><th>&rho;</th></tr>
<tr class="highlight"><td>Gemini 3 Flash (no thinking)</td><td>None</td><td class="sig">0.500</td></tr>
<tr><td>Gemini 3 Flash (thinking_budget=1024)</td><td>Optional</td><td>0.467</td></tr>
<tr><td>Gemini 2.5 Pro (mandatory thinking)</td><td>Required</td><td class="null">0.240</td></tr>
<tr><td>Gemini 3 Pro (mandatory thinking)</td><td>Required</td><td class="null">0.052</td></tr>
<tr><td>DeepSeek-Reasoner</td><td>Required</td><td class="null">NaN (constant output)</td></tr>
</table>

<div class="finding">
<strong>The &ldquo;System 1&rdquo; finding:</strong> Every form of deliberation degrades performance. The task requires fast pattern matching on implicit pedagogical knowledge, not step-by-step reasoning. Models forced to &ldquo;think&rdquo; perform worse.
</div>

<h4>3.3.3 Structured Elicitation: Testing Theoretical Predictions</h4>

<p>We tested predictions from four learning science traditions. Each predicts that richer specification of student cognition should improve estimation.</p>

<table>
<tr><th>Strategy</th><th>Theoretical basis</th><th>&rho;</th><th>vs. baseline (0.50)</th></tr>
<tr class="highlight"><td>Direct prediction</td><td>KC theory</td><td class="sig">0.500 &plusmn; 0.111</td><td>&mdash;</td></tr>
<tr><td>Two-stage cognitive chains</td><td>Conceptual change</td><td>0.508 &plusmn; 0.067</td><td>&asymp; 0</td></tr>
<tr><td>Buggy reasoning (no hint)</td><td>BUGGY</td><td>0.488</td><td>&minus;0.01</td></tr>
<tr><td>Two-stage backstory</td><td>Generative students</td><td class="null">0.408 &plusmn; 0.096</td><td>&minus;0.09</td></tr>
<tr><td>Buggy reasoning (with hint)</td><td>ESS E3 + confusion tuples</td><td class="null">0.193</td><td>&minus;0.31</td></tr>
<tr><td>Cognitive modeling (10 CoT)</td><td>SMART / classroom sim</td><td class="null">0.165 &plusmn; 0.046</td><td>&minus;0.34</td></tr>
</table>

<div id="scaffoldingPlot" class="plot-container" style="min-height:350px"></div>
<div class="figure-caption">Figure 4. Structured elicitation comparison (20 probe items). Dashed line = direct prediction baseline. Every structured approach matched or underperformed simple prediction. Providing misconception hints <em>actively hurt</em>.</div>

<p>One nuanced finding: cognitive chains used as <em>context</em> for a teacher-prediction prompt (&rho;=0.51) dramatically outperformed the same chains used as a <em>prediction mechanism</em> by counting simulated answers (&rho;=0.17). The model integrates diverse perspectives better than it enacts individual student cognition. But context-based elicitation provides no improvement over direct prediction&mdash;it merely stabilises variance (SD 0.067 vs. 0.111).</p>

<h4>3.3.4 Calibration Anchors</h4>

<table>
<tr><th></th><th>SmartPaper (20 probe)</th><th>Eedi (4-fold CV)</th></tr>
<tr><td>With anchors</td><td class="sig">&rho;=0.81</td><td>&rho;=0.445</td></tr>
<tr><td>Without anchors</td><td>&rho;=0.66</td><td>&rho;=0.525</td></tr>
<tr><td>Effect</td><td class="sig"><strong>+0.15</strong></td><td class="null"><strong>&minus;0.08</strong></td></tr>
</table>
<p>Anchors help on SmartPaper but hurt on Eedi&mdash;likely because providing IRT values confuses the ability-stratified distribution format.</p>

<h3>3.4 Generalisation: The Central Finding</h3>

<h4>Eedi: Signal Collapse</h4>

<table>
<tr><th>Item set</th><th>n</th><th>&rho;</th><th>p</th><th>95% CI</th></tr>
<tr><td>Original 20 probe</td><td>20</td><td class="sig">0.462</td><td>0.04</td><td>&mdash;</td></tr>
<tr><td>30 randomly sampled</td><td>30</td><td class="null">&minus;0.176</td><td>0.35</td><td>&mdash;</td></tr>
<tr><td>Combined 50</td><td>50</td><td class="null">0.039</td><td>0.79</td><td>&mdash;</td></tr>
<tr class="highlight"><td><strong>105-item confirmation</strong></td><td><strong>105</strong></td><td class="null"><strong>0.114</strong></td><td><strong>0.25</strong></td><td><strong>[&minus;0.07, 0.30]</strong></td></tr>
</table>

<p>To understand why, we analysed probe and non-probe items <em>within</em> the 105-item experiment (same prompt, model, and reps for all items). The 20 probe items still show <span class="sig">&rho;=0.537</span> (p=0.015) while the 85 non-probe items show <span class="null">&rho;=0.027</span> (p=0.81). The signal is item-specific.</p>

<div class="note">
<strong>Diagnosis: difficulty-range inflation.</strong> The probe set was stratified by difficulty quintile and contains two items with extreme IRT difficulty (b=&minus;5.18, &minus;3.37), far outside the non-probe range ([&minus;1.62, +1.85]). This gives the probe set <strong>2.3&times;</strong> the difficulty SD of the non-probe items (1.46 vs. 0.64). Removing just these two outliers drops the probe correlation from &rho;=0.537 to &rho;=0.391 (p=0.11, non-significant). The model can distinguish &ldquo;very easy&rdquo; items from items in the normal difficulty range, but cannot rank-order within that range.
</div>

<p>Three further checks confirm this interpretation: (1) all 105 items test the same four misconceptions as the probe set, ruling out misconception familiarity; (2) prediction spread is identical across probe and non-probe items (SD=0.087 vs. 0.085), so the correlation comes from ground-truth spread, not better predictions; (3) bootstrap resampling (10,000 random 20-item subsets from the 105) yields mean &rho;=0.110, with only 2.5% of subsets reaching &rho;&ge;0.50&mdash;the probe result is a tail event explained by its extreme difficulty range.</p>

<p>The 105-item result is definitive: &rho;=0.114, 95% CI [&minus;0.072, 0.297]. The probe-set correlations reported in Sections 3.1&ndash;3.3 are inflated by difficulty-range selection and should be interpreted as parameter-space exploration, not as evidence of genuine item-level discrimination.</p>

<h4>SmartPaper: Signal Generalises</h4>

<table>
<tr><th>Item set</th><th>n</th><th>&rho;</th><th>p</th></tr>
<tr><td>Probe 20</td><td>20</td><td class="sig">0.768</td><td>&lt;0.001</td></tr>
<tr><td>Held-out 120</td><td>120</td><td class="sig">0.518</td><td>&lt;0.0001</td></tr>
<tr class="highlight"><td><strong>All 140</strong></td><td><strong>140</strong></td><td class="sig"><strong>0.547</strong></td><td><strong>&lt;0.0001</strong></td></tr>
</table>

<p><strong>By subject:</strong></p>
<table>
<tr><th>Subject</th><th>&rho;</th><th>n</th><th>p</th></tr>
<tr><td>Science</td><td class="sig">0.734</td><td>32</td><td>&lt;0.0001</td></tr>
<tr><td>Social Science</td><td class="sig">0.702</td><td>34</td><td>&lt;0.0001</td></tr>
<tr><td>Mathematics</td><td class="sig">0.600</td><td>33</td><td>0.0002</td></tr>
<tr><td>English</td><td class="sig">0.432</td><td>41</td><td>0.005</td></tr>
</table>

<div id="generalizationPlot" class="plot-container" style="min-height:370px"></div>
<div class="figure-caption">Figure 5. The central finding: generalisation contrast. SmartPaper signal holds from probe to full set; Eedi signal collapses.</div>

<h4>The Dissociation</h4>

<table>
<tr><th></th><th>Eedi (MCQ)</th><th>SmartPaper (open-ended)</th></tr>
<tr><td>Probe items</td><td>&rho;=0.46 (n=20)</td><td class="sig">&rho;=0.77 (n=20)</td></tr>
<tr><td>Non-probe items</td><td class="null">&rho;=&minus;0.18 (n=30)</td><td class="sig">&rho;=0.52 (n=120)</td></tr>
<tr class="highlight"><td><strong>Full confirmation</strong></td><td class="null"><strong>&rho;=0.11 (n=105)</strong></td><td class="sig"><strong>&rho;=0.55 (n=140)</strong></td></tr>
<tr><td>Generalises?</td><td class="null"><strong>No</strong></td><td class="sig"><strong>Yes</strong></td></tr>
</table>

<div id="subjectPlot" class="plot-container" style="min-height:320px"></div>
<div class="figure-caption">Figure 6. SmartPaper results by subject. Signal is strongest in Science and Social Science (where content complexity variation is greatest) and weakest in English.</div>

<h3>3.5 Mechanistic Investigation: Disentangling Temperature and Prompt</h3>

<p>Controlled temperature sweeps with the contrastive prompt held constant (Gemini Flash, 3 reps per point):</p>

<div id="tempSweepPlot" class="plot-container" style="min-height:350px"></div>
<div class="figure-caption">Figure 7. Temperature curve for Gemini Flash with contrastive prompt on Eedi probe items (3 reps per point). The curve is roughly flat (&rho;&asymp;0.35&ndash;0.58). The original RSM &ldquo;cliff&rdquo; was a prompt&times;temperature confound.</div>

<p>On SmartPaper, temperature interacts with model:</p>

<table>
<tr><th>Model</th><th>Strategy</th><th>t=0.5</th><th>t=1.0</th><th>t=1.5</th><th>t=2.0</th></tr>
<tr><td>Gemini</td><td>errors</td><td>0.774</td><td>0.803</td><td>0.783</td><td class="sig"><strong>0.841</strong></td></tr>
<tr><td>Gemini</td><td>anchors</td><td>0.811</td><td>0.821</td><td>0.783</td><td class="sig"><strong>0.877</strong></td></tr>
<tr><td>DeepSeek</td><td>baseline</td><td class="sig"><strong>0.800</strong></td><td class="sig"><strong>0.809</strong></td><td>0.728</td><td class="null">0.675</td></tr>
<tr><td>DeepSeek</td><td>errors</td><td class="sig"><strong>0.799</strong></td><td>0.787</td><td>0.783</td><td>0.766</td></tr>
</table>
<p>Temperature helps Gemini (monotonic increase with structured prompts) but hurts DeepSeek (monotonic decrease). There is no universal temperature recommendation.</p>

<h4>Two-Stage Diversity at Scale</h4>

<table>
<tr><th>Method</th><th>&rho;</th><th>p</th><th>n</th></tr>
<tr class="highlight"><td>Direct prediction (Phase 4)</td><td class="sig"><strong>0.547</strong></td><td>&lt;0.0001</td><td>140</td></tr>
<tr><td>Two-stage cognitive chains</td><td class="null">0.059</td><td>0.50</td><td>134</td></tr>
</table>

<div class="note">
<strong>Two-stage destroyed the signal.</strong> The damage was worst where direct prediction was strongest (Science: 0.734&rarr;0.173; Social Science: 0.702&rarr;&minus;0.011). Simulated student attempts act as noise that dilutes the model&rsquo;s implicit knowledge.
</div>

<!-- ============================================================ -->
<h2>4. Discussion</h2>

<h3>4.1 Why the Literature Disagrees: Complexity-Driven vs. Selectivity-Driven Difficulty</h3>

<p>The dissociation between Eedi and SmartPaper resolves the conflicting findings in the literature. We propose a framework distinguishing two sources of item difficulty:</p>

<p><strong>Complexity-driven difficulty.</strong> The item requires more cognitive steps, deeper reasoning, or less familiar content. This is legible from item text&mdash;an LLM can detect that multi-step reasoning is harder than recall. SmartPaper items vary primarily on this dimension: &ldquo;explain the water cycle&rdquo; is harder than &ldquo;name the capital of India&rdquo; because it requires more cognitive operations, and this is evident from the text alone. Studies reporting high correlations (Razavi &amp; Powers, 2025: r=0.83; Kr&ouml;ger et al., 2025: r=0.82 via simulation) likely use item pools where difficulty is primarily complexity-driven.</p>

<p><strong>Selectivity-driven difficulty.</strong> The item&rsquo;s difficulty depends on whether specific distractors trigger specific misconceptions in specific students&mdash;and how prevalent those misconceptions are in the target population. Two items testing the same concept (e.g., &minus;1&times;&minus;4 vs. 3&times;&minus;2) can have very different error rates depending on how the numbers interact with common bugs. This information is not in the item text; it is an empirical property of the student&ndash;item interaction. Eedi items vary primarily on this dimension.</p>

<p>This framework explains three patterns:</p>
<ol>
  <li><strong>Variable correlations across studies.</strong> Razavi &amp; Powers (r=0.83 on diverse K&ndash;5 items) vs. Kr&ouml;ger et al. (r&asymp;0 for direct estimation) reflects different difficulty sources, not methodological superiority.</li>
  <li><strong>Why simulation fails on Eedi.</strong> Eedi items are narrowly targeted within single misconception clusters; the model produces near-identical simulated response distributions regardless of which specific item tests a given misconception.</li>
  <li><strong>Why the 20-item success didn&rsquo;t generalise.</strong> The probe set was stratified by difficulty quintile and contained two extreme-difficulty outliers (b=&minus;5.2, &minus;3.4) that inflated the difficulty SD to 2.3&times; the non-probe level. Removing these two items renders the probe correlation non-significant. The model can distinguish &ldquo;very easy&rdquo; from &ldquo;moderate&rdquo; but cannot rank-order within the normal difficulty range.</li>
</ol>

<p><strong>Important caveat.</strong> This framework is a post-hoc interpretation. The two datasets differ in multiple ways beyond difficulty source: item format, population, subject breadth, and difficulty metric. Matched item sets would be needed to isolate the causal factor.</p>

<h3>4.2 The &ldquo;System 1&rdquo; Finding: Deliberation Hurts</h3>

<p>Across every comparison, fast pattern-matching outperformed deliberative reasoning:</p>
<ul>
  <li>Direct prediction (&rho;=0.50) &gt; cognitive modeling with 10 reasoning chains (&rho;=0.17)</li>
  <li>Gemini Flash without thinking (&rho;=0.50) &gt; with thinking budget (&rho;=0.47) &gt; Pro with mandatory thinking (&rho;=0.05)</li>
  <li>Simple teacher prediction (&rho;=0.50) &gt; buggy reasoning with misconception specification (&rho;=0.19)</li>
  <li>Direct prediction on SmartPaper (&rho;=0.55) &gt; two-stage with 5 simulated attempts as context (&rho;=0.06)</li>
</ul>

<p>This is consistent with dual-process theory (Kahneman, 2011) applied to the model&rsquo;s knowledge retrieval. The model&rsquo;s implicit pedagogical knowledge is best accessed as a fast judgment, not a deliberative analysis. Scaffolding&mdash;specifying misconceptions, generating student personas, requiring step-by-step analysis&mdash;actively degrades performance.</p>

<h3>4.3 What Doesn&rsquo;t Work: A Registry of Null Results</h3>

<ol>
  <li><strong>Feature extraction + ML</strong> collapses from r=0.77 (n=10 test) to r=0.06 (n=1,869)&mdash;small-sample overfitting.</li>
  <li><strong>Classroom simulation</strong> produces r=0.01 on misconception-targeted MCQs.</li>
  <li><strong>Misconception hints actively hurt</strong> (&rho;=0.19 vs. 0.50 without).</li>
  <li><strong>Student personas add noise</strong> (&rho;=0.41 vs. 0.50 without).</li>
  <li><strong>Deliberation hurts</strong> across all models tested.</li>
  <li><strong>Option shuffling adds noise</strong> (&rho;=0.62 vs. 0.67 original order).</li>
  <li><strong>Two-stage diversity destroys signal at scale</strong> (&rho;=0.06 vs. 0.55 direct on SmartPaper).</li>
  <li><strong>Calibration anchors don&rsquo;t transfer</strong>&mdash;helped on SmartPaper (+0.15) but hurt on Eedi (&minus;0.08).</li>
  <li><strong>Model size doesn&rsquo;t predict success</strong>&mdash;GPT-4o (&rho;=0.17) outperformed by Scout 17B (&rho;=0.36).</li>
  <li><strong>Small evaluation sets inflate correlations</strong>&mdash;&rho;=0.50 on 20 items collapsed to &rho;=0.11 on 105.</li>
</ol>

<h3>4.4 Averaging Helps, but Has Limits</h3>

<table>
<tr><th>Model</th><th>Per-rep &rho;</th><th>3-rep avg &rho;</th><th>10-rep avg &rho;</th></tr>
<tr><td>Gemini Flash</td><td>0.500 &plusmn; 0.111</td><td>0.666</td><td>0.573</td></tr>
<tr><td>Scout 17B</td><td>0.355 &plusmn; 0.167</td><td>0.609</td><td>0.668</td></tr>
</table>

<p>Scout&rsquo;s trajectory is instructive: high per-rep variance (SD=0.167) but monotonic improvement with averaging (3&rarr;0.609, 5&rarr;0.639, 7&rarr;0.648, 10&rarr;0.668). However, averaging only boosts signal that already exists. On the full 105-item Eedi set, where the per-rep signal is null, averaging cannot recover what isn&rsquo;t there.</p>

<h3>4.5 Limitations</h3>

<ol>
  <li><strong>Probe set size.</strong> Screening and optimisation used 20 items per dataset. The RSM surface was estimated on a small basis.</li>
  <li><strong>Two datasets, many differences.</strong> Eedi and SmartPaper differ in format, population, subject breadth, and difficulty metric. Matched item sets would be needed to isolate causation.</li>
  <li><strong>Not pre-registered.</strong> Mitigated through the 105-item confirmation test (null) and transparent reporting of all conditions.</li>
  <li><strong>Model-specific results.</strong> Strongest Eedi probe-set results require Gemini 3 Flash or Llama-4-Scout specifically.</li>
  <li><strong>Surface feature confound on SmartPaper.</strong> Text length correlates &rho;=&minus;0.44 with difficulty. The LLM&rsquo;s &rho;=0.55 exceeds this, but surface features contribute partially.</li>
  <li><strong>Calibration offset.</strong> Models overestimate easiness by +0.40 for Indian government school students.</li>
</ol>

<!-- ============================================================ -->
<h2>5. Conclusion</h2>

<p>We mapped the parameter landscape for LLM item difficulty estimation across two datasets, 15+ models, and 18 hypotheses using a sequential design-of-experiments pipeline. The central finding is a dissociation: LLMs can rank-order difficulty for open-ended items where difficulty varies in cognitive demand (<span class="sig">&rho;=0.55, n=140</span>, generalises to held-out items) but cannot do so for misconception-targeted MCQs where difficulty depends on distractor&ndash;misconception interactions (<span class="null">&rho;=0.11, n=105</span>, non-significant). No combination of model, prompt, temperature, structured elicitation, or aggregation strategy overcame this boundary.</p>

<p>Within each dataset, the factors that matter are not what the literature would predict. Temperature is flat on Eedi once prompt quality is controlled, and interacts with model on SmartPaper. Prompt design is the dominant factor on Eedi, creating a 3&ndash;4&times; difference. Model size and benchmark performance do not predict task success. And every form of cognitive scaffolding either matched or degraded performance relative to simple direct prediction.</p>

<p>The practical recommendation: characterise the difficulty source in your item pool before investing in LLM-based estimation, and always test generalisation beyond curated probe sets.</p>

<!-- ============================================================ -->
<h2>6. Related Work</h2>

<h3>6.1 Direct Difficulty Estimation</h3>
<p>Attali (2024) demonstrated GPT-4 can estimate item difficulty for adaptive testing items. Razavi &amp; Powers (2025) achieved r=0.83 using GPT-4o on K&ndash;5 items. Benedetto et al. (2023) applied LLMs to language teaching items. These studies share a common design: one model, one prompt, one dataset, one correlation. Our contribution is methodological: the outcome depends on model &times; prompt &times; temperature &times; item-type interactions, and single-configuration studies cannot reveal this.</p>

<h3>6.2 Student Simulation</h3>
<p>Kr&ouml;ger et al. (2025) simulate students at four NAEP ability levels (r=0.75&ndash;0.82). The SMART framework (Lan et al., 2025) aligns simulated responses with IRT parameters. Our replication on Eedi (r=0.01) challenges simulation for misconception-targeted items. The nuanced finding that simulated reasoning used as <em>context</em> (&rho;=0.51) outperforms the same reasoning as <em>mechanism</em> (&rho;=0.17) suggests implicit pedagogical knowledge is better accessed through judgment than simulation.</p>

<h3>6.3 Reasoning and Scaffolding</h3>
<p>The AIED 2025 paper (arXiv:2503.08551) reports 10&ndash;28% MSE reduction from reasoning augmentation. The ESS framework (Tack &amp; Piech, 2024) proposes a taxonomy from E0 to E4. Our experiments directly test these predictions and find them wrong for difficulty estimation: every form of cognitive scaffolding matched or degraded performance.</p>

<h3>6.4 LLM Error Alignment</h3>
<p>Liu, Sonkar &amp; Baraniuk (2025) find LLM errors align with student errors (r=0.73&ndash;0.80). Our broader finding is consistent: LLMs have implicit knowledge <em>about</em> common misconceptions but cannot simulate the <em>population prevalence</em> of those misconceptions.</p>

<h3>6.5 Design of Experiments in AI Evaluation</h3>
<p>RSM (Box &amp; Behnken, 1960) and sequential DOE are standard in manufacturing and pharmaceuticals. Their application to LLM evaluation is novel. DOE differs from hyperparameter optimisation (Bergstra &amp; Bengio, 2012) in seeking to <em>understand</em> the response surface, not just find the optimum.</p>

<!-- ============================================================ -->
<h2>References</h2>

<div class="ref">Attali, Y. (2024). Can language models estimate item difficulty? <em>Educational Measurement: Issues and Practice</em>.</div>
<div class="ref">Benedetto, L., et al. (2023). On the application of large language models for language teaching. <em>AAAI Workshop on AI for Education</em>.</div>
<div class="ref">Bergstra, J., &amp; Bengio, Y. (2012). Random search for hyper-parameter optimization. <em>JMLR</em>, 13, 281&ndash;305.</div>
<div class="ref">Box, G. E. P., &amp; Behnken, D. W. (1960). Some new three level designs for the study of quantitative variables. <em>Technometrics</em>, 2(4), 455&ndash;475.</div>
<div class="ref">Brown, J. S., &amp; Burton, R. R. (1978). Diagnostic models for procedural bugs in basic mathematical skills. <em>Cognitive Science</em>, 2(2), 155&ndash;192.</div>
<div class="ref">Chi, M. T. H., Slotta, J. D., &amp; de Leeuw, N. (1994). From things to processes. <em>Learning and Instruction</em>, 4(1), 27&ndash;43.</div>
<div class="ref">Eedi. (2024). Mining misconceptions in mathematics [Dataset]. Kaggle.</div>
<div class="ref">Kahneman, D. (2011). <em>Thinking, Fast and Slow</em>. Farrar, Straus and Giroux.</div>
<div class="ref">Koedinger, K. R., Corbett, A. T., &amp; Perfetti, C. (2012). The Knowledge-Learning-Instruction framework. <em>Cognitive Science</em>, 36(5), 757&ndash;798.</div>
<div class="ref">Kr&ouml;ger, B., et al. (2025). Take out your calculators: LLM-simulated classrooms for item difficulty estimation. arXiv:2601.09953.</div>
<div class="ref">Lan, A., et al. (2025). SMART: Simulating students aligned with item response theory. arXiv:2507.05129.</div>
<div class="ref">Liu, Z., Sonkar, S., &amp; Baraniuk, R. (2025). Do LLMs make mistakes like students? arXiv:2502.15140.</div>
<div class="ref">Lu, Y., &amp; Wang, S. (2024). Generative students. arXiv:2405.11591.</div>
<div class="ref">Razavi, R., &amp; Powers, D. (2025). Estimating item difficulty using large language models. arXiv:2504.08804.</div>
<div class="ref">Tack, A., &amp; Piech, C. (2024). Harnessing LLMs for automated prediction of item difficulty. <em>BEA Workshop, ACL 2024</em>.</div>
<div class="ref">Tack, A., et al. (2025). Towards valid student simulation with LLMs. arXiv:2601.05473.</div>
<div class="ref">Yaneva, V., et al. (2024). Predicting item difficulty for medical examinations. <em>Journal of Educational Measurement</em>.</div>
<div class="ref">Reasoning and sampling-augmented MCQ difficulty prediction. (2025). arXiv:2503.08551.</div>

<hr style="margin:2rem 0">
<p style="font-size:0.8rem; color:var(--muted)">
  Draft generated February 2026. Data, code, and raw model outputs available at [repository URL].
  All experiments used publicly available APIs. Research conducted using Claude Code (Anthropic).
</p>

<!-- ============================================================ -->
<!-- FIGURES (Plotly.js) -->
<!-- ============================================================ -->
<script>
const plotFont = {family: 'Georgia'};
const gridColor = '#eee';

// ============================================================
// Figure 1: RSM Screening Heatmap
// ============================================================
{
  const prompts = ['Individual roleplay', 'Classroom batch', 'Teacher prediction'];
  const temps = ['t=0.3', 't=0.9', 't=1.5'];
  const z = [
    [0.307, null, 0.306],
    [0.211, null, 0.325],
    [0.119, null, 0.673],
  ];
  Plotly.newPlot('rsmHeatmap', [{
    z: z, x: temps, y: prompts, type: 'heatmap',
    colorscale: [[0,'#fee2e2'],[0.4,'#fef9c3'],[0.7,'#dcfce7'],[1,'#16a34a']],
    zmin: 0, zmax: 0.7,
    text: z.map(row => row.map(v => v === null ? '' : '=' + v.toFixed(3))),
    texttemplate: '%{text}', textfont: {size: 13},
    hoverinfo: 'z',
    colorbar: {title: '', titleside: 'right'},
  }], {
    height: 240, margin: {l: 150, r: 80, t: 20, b: 40},
    font: plotFont,
    annotations: [{
      x: 't=1.5', y: 'Teacher prediction',
      text: ' confound', showarrow: true, arrowhead: 2, ax: 60, ay: 0,
      font: {size: 10, color: '#dc2626'},
    }],
  });
}

// ============================================================
// Figure 2: Prompt Sweep (bar chart at t=2.0)
// ============================================================
{
  const prompts = ['Error<br>analysis', 'Contrastive', "Devil's<br>advocate", 'Imagine<br>classroom', 'Sparse'];
  const means = [0.604, 0.562, 0.516, 0.428, 0.232];
  const sds = [0.062, 0.084, 0.122, 0.115, 0.175];
  const colors = means.map(m => m >= 0.55 ? '#16a34a' : m >= 0.4 ? '#d97706' : '#dc2626');

  Plotly.newPlot('promptSweep', [{
    type: 'bar', x: prompts, y: means,
    marker: {color: colors},
    error_y: {type:'data', array: sds, visible: true, thickness: 2, width: 4, color: '#333'},
    text: means.map(m => '=' + m.toFixed(3)),
    textposition: 'outside', textfont: {size: 11},
  }], {
    height: 340, margin: {l:60, r:20, t:20, b:80},
    yaxis: {title:'Spearman  (20 probe items)', range:[-0.05, 0.8], gridcolor: gridColor},
    font: plotFont,
  });
}

// ============================================================
// Figure 3: Cross-Model Survey
// ============================================================
{
  const models = [
    'Gemini 2.0 Flash', 'GPT-4o-mini', 'Llama-3.1-8B', 'Qwen3-32B',
    'GPT-4o', 'Llama-3.3-70B', 'Kimi-K2', 'GPT-OSS-120B',
    'DeepSeek V3', 'Llama-4-Scout 17B', 'Gemini 3 Flash'
  ];
  const means = [-0.200, -0.092, -0.081, 0.005, 0.172, 0.257, 0.336, 0.336, 0.338, 0.355, 0.470];
  const sds = [0.132, 0.266, 0.097, 0.110, 0.126, 0.062, 0.116, 0.100, 0.140, 0.167, 0.076];
  const colors = means.map(m => m >= 0.3 ? '#16a34a' : m >= 0.1 ? '#d97706' : '#dc2626');

  Plotly.newPlot('crossModelPlot', [{
    type: 'bar', y: models, x: means, orientation: 'h',
    marker: {color: colors},
    error_x: {type:'data', array: sds, visible: true, thickness: 2, width: 4, color: '#333'},
    text: means.map(m => '=' + m.toFixed(3)),
    textposition: 'outside', textfont: {size: 10},
  }], {
    height: 400, margin: {l:150, r:60, t:20, b:50},
    xaxis: {title:'Per-rep Spearman  (20 probe items)', range:[-0.5, 0.7], gridcolor: gridColor, zeroline: true, zerolinewidth: 2},
    font: plotFont,
    shapes: [{type:'line', x0:0, x1:0, y0:-0.5, y1:10.5, line:{color:'#999',width:1,dash:'dot'}}],
  });
}

// ============================================================
// Figure 4: Scaffolding Comparison
// ============================================================
{
  const strategies = [
    'Direct<br>prediction', 'Two-stage<br>cognitive', 'Buggy<br>(no hint)',
    'Two-stage<br>backstory', 'Buggy<br>(with hint)', 'Cognitive<br>modeling'
  ];
  const means = [0.500, 0.508, 0.488, 0.408, 0.193, 0.165];
  const sds = [0.111, 0.067, 0, 0.096, 0, 0.046];
  const colors = means.map(m => m >= 0.49 ? '#16a34a' : m >= 0.35 ? '#d97706' : '#dc2626');

  Plotly.newPlot('scaffoldingPlot', [{
    type: 'bar', x: strategies, y: means,
    marker: {color: colors},
    error_y: {type:'data', array: sds, visible: true, thickness: 2, width: 4, color: '#333'},
    text: means.map(m => '=' + m.toFixed(3)),
    textposition: 'outside', textfont: {size: 11},
  }], {
    height: 340, margin: {l:60, r:20, t:20, b:90},
    yaxis: {title:'Spearman ', range:[-0.05, 0.7], gridcolor: gridColor},
    font: plotFont,
    shapes: [{
      type:'line', x0:-0.5, x1:5.5, y0:0.500, y1:0.500,
      line:{color:'#16a34a', width:2, dash:'dash'},
    }],
    annotations: [{
      x:5, y:0.53, text:'baseline', showarrow:false,
      font:{size:10, color:'#16a34a'},
    }],
  });
}

// ============================================================
// Figure 5: Generalization contrast
// ============================================================
{
  Plotly.newPlot('generalizationPlot', [
    {
      x: ['Probe (20)', 'Full set'],
      y: [0.462, 0.114],
      name: 'Eedi MCQs',
      type: 'bar',
      marker: {color: '#dc2626'},
      text: ['=0.462', '=0.114'],
      textposition: 'outside', textfont: {size: 12},
      error_y: {type:'data', array:[0, 0.094], visible:true, thickness:2, width:4},
    },
    {
      x: ['Probe (20)', 'Full set'],
      y: [0.768, 0.547],
      name: 'SmartPaper open-ended',
      type: 'bar',
      marker: {color: '#2563eb'},
      text: ['=0.768', '=0.547'],
      textposition: 'outside', textfont: {size: 12},
    },
  ], {
    height: 370, margin: {l:60, r:20, t:30, b:50},
    barmode: 'group',
    yaxis: {title:'Spearman ', range:[-0.1, 0.95], gridcolor: gridColor},
    font: plotFont,
    legend: {x: 0.55, y: 0.95},
    shapes: [{type:'line', x0:-0.5, x1:1.5, y0:0, y1:0, line:{color:'#999',width:1,dash:'dash'}}],
    annotations: [
      {x:'Full set', y:-0.05, text:'n=105', showarrow:false, font:{size:10, color:'#dc2626'}},
      {x:'Full set', y:0.49, text:'n=140', showarrow:false, font:{size:10, color:'#2563eb'}},
    ],
  });
}

// ============================================================
// Figure 6: SmartPaper by subject
// ============================================================
{
  const subjects = ['Science', 'Social Science', 'Mathematics', 'English'];
  const rhos = [0.734, 0.702, 0.600, 0.432];
  const ns = [32, 34, 33, 41];
  const colors = rhos.map(r => r >= 0.6 ? '#16a34a' : r >= 0.4 ? '#2563eb' : '#d97706');

  Plotly.newPlot('subjectPlot', [{
    type: 'bar', x: subjects, y: rhos,
    marker: {color: colors},
    text: rhos.map((r,i) => '=' + r.toFixed(3) + ' (n=' + ns[i] + ')'),
    textposition: 'outside', textfont: {size: 11},
  }], {
    height: 320, margin: {l:60, r:20, t:20, b:50},
    yaxis: {title:'Spearman ', range:[0, 0.85], gridcolor: gridColor},
    font: plotFont,
  });
}

// ============================================================
// Figure 7: Temperature sweep (flat)
// ============================================================
{
  const temps = [0.3, 0.6, 0.9, 1.2, 1.5, 2.0];
  const means = [0.449, 0.458, 0.358, 0.354, 0.462, 0.580];
  const sds = [0.118, 0.046, 0.070, 0.127, 0.031, 0.110];
  const upper = means.map((m,i) => m + sds[i]);
  const lower = means.map((m,i) => m - sds[i]);

  Plotly.newPlot('tempSweepPlot', [
    {
      x: temps.concat([...temps].reverse()),
      y: upper.concat([...lower].reverse()),
      fill: 'toself', fillcolor: 'rgba(99,102,241,0.15)',
      line: {color:'transparent'}, showlegend: false, hoverinfo: 'skip',
    },
    {
      x: temps, y: means, mode: 'lines+markers',
      line: {color:'#4f46e5', width: 3},
      marker: {size: 10, color: '#4f46e5'},
      text: means.map((m,i) => 't='+temps[i]+'<br>='+m.toFixed(3)+''+sds[i].toFixed(3)),
      hoverinfo: 'text', name: 'Contrastive prompt (Gemini Flash)',
    },
    {
      x: [0.3, 1.5], y: [0.119, 0.673],
      mode: 'markers', name: 'Original RSM (confounded)',
      marker: {size: 10, color: '#dc2626', symbol: 'x', line:{width:2}},
      text: ['Plain prompt t=0.3<br>=0.119', 'Strong prompt t=1.5<br>=0.673 (outlier)'],
      hoverinfo: 'text',
    },
  ], {
    height: 350, margin: {l:60, r:20, t:20, b:50},
    xaxis: {title:'Temperature', dtick: 0.3, gridcolor: gridColor},
    yaxis: {title:'Spearman ', range:[-0.1,0.8], gridcolor: gridColor},
    font: plotFont,
    legend: {x:0.02, y:0.98, bgcolor:'rgba(255,255,255,0.8)'},
    shapes: [
      {type:'line', x0:0.2, x1:2.1, y0:0, y1:0, line:{color:'#999',width:1,dash:'dash'}},
    ],
    annotations: [{
      x:1.5, y:0.7, text:'<b>Outlier</b><br>(prompt confound)',
      showarrow:true, arrowhead:2, ax:-50, ay:20,
      font:{size:10, color:'#dc2626'},
    },{
      x:1.2, y:0.22, text:'Flat: 0.350.58<br>(on 20 probe items only)',
      showarrow:false,
      font:{size:11, color:'#4f46e5'},
    }],
  });
}
</script>

</body>
</html>
