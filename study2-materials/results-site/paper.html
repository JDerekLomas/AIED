<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Optimizing LLM Item Difficulty Estimation: A Sequential Design-of-Experiments Study</title>
<script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>
<style>
  :root {
    --bg: #fafafa; --card: #fff; --text: #1a1a1a; --muted: #666;
    --accent: #2563eb; --accent2: #dc2626; --border: #e5e7eb;
    --green: #16a34a; --amber: #d97706;
  }
  * { box-sizing: border-box; margin: 0; padding: 0; }
  body {
    font-family: 'Georgia', 'Times New Roman', serif;
    background: var(--bg); color: var(--text);
    line-height: 1.7; max-width: 900px; margin: 0 auto;
    padding: 2rem 1.5rem;
  }
  h1 { font-size: 1.7rem; line-height: 1.3; margin-bottom: 0.3rem; }
  h2 { font-size: 1.3rem; margin: 2.5rem 0 0.8rem; border-bottom: 2px solid var(--accent); padding-bottom: 0.3rem; }
  h3 { font-size: 1.1rem; margin: 1.5rem 0 0.5rem; }
  h4 { font-size: 1rem; margin: 1.2rem 0 0.4rem; }
  p { margin-bottom: 1rem; }
  .authors { color: var(--muted); font-size: 0.95rem; margin-bottom: 0.3rem; }
  .venue { color: var(--muted); font-size: 0.9rem; font-style: italic; margin-bottom: 1.5rem; }
  .abstract {
    background: #f0f4ff; border-left: 4px solid var(--accent);
    padding: 1rem 1.2rem; margin: 1.5rem 0; font-size: 0.95rem;
  }
  .abstract strong { display: block; margin-bottom: 0.3rem; }
  table {
    width: 100%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem;
  }
  th, td { padding: 0.4rem 0.6rem; border: 1px solid var(--border); text-align: left; }
  th { background: #f3f4f6; font-weight: 600; }
  tr.highlight { background: #eff6ff; }
  .note {
    background: #fffbeb; border-left: 4px solid var(--amber);
    padding: 0.8rem 1rem; margin: 1rem 0; font-size: 0.9rem;
  }
  .finding {
    background: #f0fdf4; border-left: 4px solid var(--green);
    padding: 0.8rem 1rem; margin: 1rem 0; font-size: 0.9rem;
  }
  .plot-container { width: 100%; margin: 1rem 0; }
  .figure-caption { font-size: 0.85rem; color: var(--muted); margin-top: 0.3rem; font-style: italic; }
  code { background: #f3f4f6; padding: 0.15rem 0.4rem; border-radius: 3px; font-size: 0.88rem; }
  .ref { font-size: 0.88rem; margin-bottom: 0.5rem; }
  .keyword { display: inline-block; background: #e0e7ff; color: #3730a3; padding: 0.1rem 0.5rem; border-radius: 12px; font-size: 0.8rem; margin: 0.1rem; }
  sup { font-size: 0.7rem; }
  a { color: var(--accent); }
  .two-col { display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; }
  @media (max-width: 700px) { .two-col { grid-template-columns: 1fr; } }
  .stat { font-family: 'Courier New', monospace; font-weight: 600; }
  .null { color: var(--accent2); }
  .sig { color: var(--green); }
  ol, ul { margin: 0.5rem 0 1rem 1.5rem; }
  li { margin-bottom: 0.3rem; }
</style>
</head>
<body>

<h1>Optimizing LLM Item Difficulty Estimation: A Sequential Design-of-Experiments Approach</h1>
<div class="authors">Derek Lomas<sup>1</sup></div>
<div class="venue">Submission to AIED 2026 &middot; Draft February 2026</div>

<div style="margin-bottom:1rem">
  <span class="keyword">item difficulty estimation</span>
  <span class="keyword">LLM evaluation</span>
  <span class="keyword">design of experiments</span>
  <span class="keyword">psychometrics</span>
  <span class="keyword">sequential optimization</span>
</div>

<div class="abstract">
  <strong>Abstract</strong>
  Published correlations between LLM-predicted and empirical item difficulty range from &rho;&asymp;0 to r=0.83, but each study tests a single configuration on a single dataset, leaving open which factors drive these differences. We apply sequential design of experiments (DOE)&mdash;standard in industrial process optimization but, to our knowledge, novel in LLM evaluation&mdash;to systematically map the parameter landscape across three datasets, six models, and seven theory-grounded prompt framings.

  <p style="margin-top:0.5rem">In a three-phase pipeline&mdash;prompt screening (7 framings), model survey (6 models), cross-dataset confirmation&mdash;we find that prompt framing grounded in Knowledge Component theory (counting prerequisite failure points) achieves <span class="sig">&rho;=0.686</span> on 140 open-ended items (95% CI [0.577, 0.771]), outperforming direct teacher prediction (&rho;=0.555). On a held-out MCQ dataset (168 university CS items), the same framing achieves <span class="sig">&rho;=0.531</span> (95% CI [0.411, 0.640]), confirming cross-dataset transfer. Across 6 models, teacher-prediction framing consistently outperforms simulation and contrastive prompts, with performance uncorrelated with model size. On misconception-targeted MCQs, all methods produce only <span class="null">&rho;=0.114</span> (n=105, non-significant).</p>

  <p style="margin-top:0.5rem">Every form of cognitive scaffolding&mdash;student simulation, deliberative reasoning, misconception hints&mdash;matched or degraded performance relative to direct prediction. The practical recommendation: use theory-grounded counting prompts, not simulation; characterize the difficulty source before investing in LLM-based estimation; and always validate beyond curated probe sets.</p>
</div>

<!-- ============================================================ -->
<h2>1. Introduction</h2>

<p>Can LLMs estimate how difficult a test item is for students? Published results vary widely: from &rho;&asymp;0 (Kr&ouml;ger et al., 2025, direct estimation) to r=0.83 (Razavi &amp; Powers, 2025). Each study evaluates one model with one prompt on one dataset, so it remains unclear which factors account for this variation.</p>

<p>Difficulty estimation depends on multiple interacting factors: the model, the prompt framing, the sampling temperature, the item type, and the student population. Single-configuration studies confound these factors. A systematic method is needed to vary them independently and identify which ones matter.</p>

<p>We apply sequential design of experiments (DOE), standard in industrial process optimization (Box &amp; Draper, 2007) but novel in LLM evaluation. The pipeline has three phases: (1) screen 7 theory-grounded prompt framings to identify the best approach, (2) survey 6 models to test generalization across architectures, and (3) confirm the top configurations on a held-out dataset. Each phase&rsquo;s results determine the next phase&rsquo;s design.</p>

<p>The core findings:</p>
<ol>
  <li><strong>Prompt framing is the primary lever.</strong> On 140 open-ended items, the best framing (&rho;=0.686) outperforms the worst (&rho;=0.483) by 0.20&mdash;a larger effect than model choice or temperature.</li>
  <li><strong>Theory-driven counting beats intuitive judgment.</strong> Prompts that operationalize learning science theories as counting tasks (count prerequisite failure points, count working memory elements) outperform direct &ldquo;estimate difficulty&rdquo; prompts.</li>
  <li><strong>Simulation and scaffolding do not help.</strong> Across all models, student simulation produces near-zero signal. Deliberative reasoning and misconception hints do not improve performance.</li>
  <li><strong>The method does not transfer to misconception-targeted MCQs.</strong> On Eedi items, &rho;=0.114 (n=105, non-significant). The dissociation depends on the source of difficulty, not the estimation method.</li>
</ol>

<h3>1.1 Theoretical Grounding</h3>

<p>Rather than testing prompts ad hoc, we operationalize four learning science traditions as prompt framings, each offering a different account of what makes items difficult:</p>

<ol>
  <li><strong>Knowledge Component theory</strong> (Koedinger, Corbett &amp; Perfetti, 2012): Difficulty scales with the number of prerequisite knowledge components an item requires. Operationalized as <em>prerequisite_chain</em>: count distinct prerequisite skills and failure points.</li>
  <li><strong>Cognitive Load Theory</strong> (Sweller, 1988): Difficulty scales with intrinsic cognitive load&mdash;the number of interacting elements held in working memory. Operationalized as <em>cognitive_load</em>: count elements students must hold simultaneously.</li>
  <li><strong>Buggy Procedures</strong> (Brown &amp; Burton, 1978): Students apply systematic but flawed algorithms. Operationalized as <em>error_affordance</em>: count distinct plausible error paths.</li>
  <li><strong>Pedagogical Content Knowledge</strong> (Shulman, 1986): Expert teachers predict difficulty through pattern matching on absorbed knowledge about common student errors. Operationalized as <em>teacher</em>: direct difficulty prediction in a teacher role.</li>
</ol>

<p>All four traditions predict that structured scaffolding (specifying what to count, what to reason about) should improve estimation over unstructured judgment. Our experiments test this prediction.</p>

<!-- ============================================================ -->
<h2>2. Method</h2>

<h3>2.1 Datasets</h3>

<div class="two-col">
<div>
<h4>SmartPaper (India, Open-ended)</h4>
<ul>
  <li><strong>Items:</strong> 140 questions across 4 subjects (English, Maths, Science, Social Science)</li>
  <li><strong>Format:</strong> Open-ended with rubric scoring</li>
  <li><strong>Students:</strong> Indian government schools, Grades 6&ndash;8 (728K+ responses)</li>
  <li><strong>Difficulty metric:</strong> Classical p-correct (proportion scoring full marks)</li>
  <li><strong>Range:</strong> 0.04&ndash;0.83 (mean 0.29)</li>
  <li><strong>Population:</strong> Economically weaker sections, many Hindi-medium backgrounds</li>
</ul>
</div>
<div>
<h4>Eedi (UK, MCQ)</h4>
<ul>
  <li><strong>Items:</strong> 1,869 diagnostic maths questions (105 with IRT parameters)</li>
  <li><strong>Format:</strong> 4-option MCQ; each distractor targets a specific misconception</li>
  <li><strong>Students:</strong> UK, ages 11&ndash;16 (73,000+ responses)</li>
  <li><strong>Difficulty metric:</strong> IRT b<sub>2PL</sub> (105 items) and classical p-value (1,869 items)</li>
  <li><strong>Design:</strong> 20-item probe set stratified by difficulty quintile; 105-item confirmation set</li>
</ul>
</div>
</div>

<h4>DBE-KT22 (South Africa, MCQ) &mdash; Phase 3 confirmation dataset</h4>
<ul>
  <li><strong>Items:</strong> 168 undergraduate computer science MCQs across 27 knowledge components</li>
  <li><strong>Students:</strong> South African university (1,300+ students, complete assignment)</li>
  <li><strong>Difficulty metric:</strong> Classical p-correct</li>
  <li><strong>Purpose:</strong> Held-out dataset for pre-specified hypothesis testing</li>
</ul>

<h3>2.2 Sequential DOE Pipeline</h3>

<table>
<tr><th>Phase</th><th>Purpose</th><th>Dataset</th><th>Design</th><th>Calls</th></tr>
<tr><td>1. Screening</td><td>Identify best prompt framing</td><td>SmartPaper (140 items)</td><td>7 framings &times; 2 temps &times; 3 reps</td><td>5,880</td></tr>
<tr><td>2. Model survey</td><td>Test generalization across models</td><td>SmartPaper (140 items)</td><td>6 models &times; 3 prompts &times; 3 reps</td><td>7,560</td></tr>
<tr><td>3. Confirmation</td><td>Pre-specified test on held-out data</td><td>DBE-KT22 (168 items)</td><td>5 prompts &times; 2 temps &times; 3 reps</td><td>5,040</td></tr>
</table>

<p>Each phase&rsquo;s results inform the next. Phase 1 identifies which framings to carry forward. Phase 2 tests whether findings generalize across models. Phase 3 tests whether they generalize across datasets with pre-specified hypotheses written before seeing results.</p>

<h3>2.3 Prompt Framings (Phase 1)</h3>

<p>All 7 framings share a common structure: the LLM is placed in a teacher role, given the item text and population context, and asked to predict what proportion of students would answer correctly. They differ in what intermediate reasoning the prompt requests:</p>

<table>
<tr><th>Framing</th><th>Theory</th><th>What the prompt asks</th></tr>
<tr><td><strong>teacher</strong></td><td>PCK (Shulman)</td><td>Direct difficulty judgment&mdash;no intermediate steps</td></tr>
<tr><td><strong>error_analysis</strong></td><td>&mdash;</td><td>Identify specific errors students would make, then estimate</td></tr>
<tr><td><strong>devil_advocate</strong></td><td>&mdash;</td><td>Challenge your initial estimate&mdash;teachers tend to overestimate students</td></tr>
<tr><td><strong>prerequisite_chain</strong></td><td>KC theory</td><td>List prerequisite skills; count failure points; use count to estimate</td></tr>
<tr><td><strong>error_affordance</strong></td><td>BUGGY</td><td>Count distinct plausible error paths; use count to estimate</td></tr>
<tr><td><strong>cognitive_load</strong></td><td>CLT (Sweller)</td><td>Count interacting elements in working memory; use count to estimate</td></tr>
<tr><td><strong>familiarity_gradient</strong></td><td>&mdash;</td><td>Rate distance from textbook drills; use rating to estimate</td></tr>
</table>

<h3>2.4 Models (Phase 2)</h3>

<table>
<tr><th>Model</th><th>Parameters</th><th>Provider</th></tr>
<tr><td>Gemini 3 Flash</td><td>Unknown</td><td>Google</td></tr>
<tr><td>GPT-4o</td><td>Unknown (large)</td><td>OpenAI</td></tr>
<tr><td>Llama-3.3-70B</td><td>70B</td><td>Groq</td></tr>
<tr><td>Gemma-3-27B</td><td>27B</td><td>Google</td></tr>
<tr><td>Llama-4-Scout</td><td>17B active (109B MoE)</td><td>Groq</td></tr>
<tr><td>Llama-3.1-8B</td><td>8B</td><td>Groq</td></tr>
</table>

<h3>2.5 Statistical Approach</h3>

<p>All correlations are Spearman &rho; (rank-order), which is invariant to the systematic calibration offset we observe (models overestimate easiness by +0.10 to +0.43). We report 95% bootstrap confidence intervals (10,000 resamples) for primary results. Temperature was tested at t=1.0 and t=2.0. Each condition uses 3 replications; reported &rho; is the averaged-prediction correlation (mean prediction across reps correlated with ground truth).</p>

<!-- ============================================================ -->
<h2>3. Results</h2>

<h3>3.1 Phase 1: Prompt Screening (SmartPaper, Gemini 3 Flash, n=140)</h3>

<p>Seven prompt framings, each at two temperatures, 3 replications per condition.</p>

<table>
<tr><th>Framing</th><th>Theory</th><th>&rho; (t=1.0)</th><th>&rho; (t=2.0)</th><th>MAE</th><th>Bias</th></tr>
<tr class="highlight"><td><strong>prerequisite_chain</strong></td><td>KC theory</td><td class="sig">0.653</td><td class="sig"><strong>0.686</strong></td><td>0.148</td><td>+0.106</td></tr>
<tr class="highlight"><td><strong>cognitive_load</strong></td><td>CLT</td><td class="sig">0.617</td><td class="sig"><strong>0.673</strong></td><td>0.190</td><td>+0.158</td></tr>
<tr><td>devil_advocate</td><td>&mdash;</td><td class="sig">0.596</td><td class="sig">0.595</td><td>0.098</td><td>&minus;0.048</td></tr>
<tr><td>error_analysis</td><td>&mdash;</td><td>0.548</td><td class="sig">0.596</td><td>0.121</td><td>+0.048</td></tr>
<tr><td>teacher</td><td>PCK</td><td>0.555</td><td>0.546</td><td>0.438</td><td>+0.433</td></tr>
<tr><td>error_affordance</td><td>BUGGY</td><td>0.493</td><td>TBD</td><td>0.179</td><td>+0.131</td></tr>
<tr><td>familiarity_gradient</td><td>&mdash;</td><td>0.483</td><td>TBD</td><td>0.407</td><td>+0.385</td></tr>
</table>

<div id="phase1Bar" class="plot-container" style="min-height:380px"></div>
<div class="figure-caption">Figure 1. Phase 1 prompt screening results (SmartPaper, 140 items, Gemini 3 Flash). Theory-driven counting prompts (prerequisite_chain, cognitive_load) outperform both direct prediction and heuristic framings. Best temperature shown for each.</div>

<div class="finding">
<strong>Finding 1: Theory-driven counting beats intuitive judgment.</strong> The two prompts that operationalize learning science theories as explicit counting tasks&mdash;prerequisite_chain (&rho;=0.686) and cognitive_load (&rho;=0.673)&mdash;outperform direct teacher prediction (&rho;=0.555) by 0.12&ndash;0.13. Counting gives the model a concrete intermediate representation.
</div>

<div class="finding">
<strong>Finding 2: Calibration quality varies independently of ranking accuracy.</strong> devil_advocate achieves the best calibration (bias &minus;0.048, MAE 0.098) despite only middling ranking (&rho;=0.596). teacher has the worst calibration (bias +0.433) despite decent ranking (&rho;=0.555). The &ldquo;challenge your assumptions&rdquo; framing corrects the systematic overestimation of student ability.
</div>

<div class="note">
<strong>Temperature is a minor factor.</strong> For most framings, &Delta;&rho; between t=1.0 and t=2.0 is &lt;0.05. The exceptions are prerequisite_chain (+0.033) and cognitive_load (+0.056), both benefiting slightly from higher temperature. Temperature is not the dominant factor.
</div>

<h3>3.2 Phase 2: Model Survey (SmartPaper, 6 models, n=140)</h3>

<p>Three prompt types (teacher, contrastive, simulation) tested across 6 models at t=2.0.</p>

<table>
<tr><th>Model</th><th>Params</th><th>&rho; (teacher)</th><th>&rho; (contrastive)</th><th>&rho; (simulation)</th></tr>
<tr class="highlight"><td><strong>Gemini 3 Flash</strong></td><td>?</td><td class="sig"><strong>0.550</strong></td><td>0.077</td><td>0.271</td></tr>
<tr><td>Gemma-3-27B</td><td>27B</td><td class="sig">0.501</td><td class="null">&minus;0.058</td><td>0.239</td></tr>
<tr><td>Llama-3.3-70B</td><td>70B</td><td>0.452</td><td>0.100</td><td class="null">&minus;0.026</td></tr>
<tr><td>Llama-4-Scout</td><td>17B/109B</td><td>0.410</td><td>0.058</td><td>0.041</td></tr>
<tr><td>GPT-4o</td><td>? (large)</td><td>0.360</td><td>0.159</td><td>0.030</td></tr>
<tr><td>Llama-3.1-8B</td><td>8B</td><td>0.244</td><td class="null">&minus;0.155</td><td>0.025</td></tr>
</table>

<div id="phase2Heatmap" class="plot-container" style="min-height:350px"></div>
<div class="figure-caption">Figure 2. Phase 2 model &times; prompt results. Teacher prediction dominates across all models. Contrastive and simulation prompts produce near-zero or negative signal for most models.</div>

<div class="finding">
<strong>Finding 3: Teacher prediction dominates; simulation fails.</strong> Across all 6 models, teacher prediction outperforms both contrastive (mean &Delta;=+0.36) and simulation (mean &Delta;=+0.34) prompts. Contrastive prompting&mdash;which worked on Eedi&rsquo;s 20-item probe set (&rho;=0.50)&mdash;produces near-zero signal on SmartPaper&rsquo;s 140 items. Simulation produces near-zero signal everywhere.
</div>

<div class="finding">
<strong>Finding 4: Model size does not predict success.</strong> GPT-4o (&rho;=0.360, teacher) is outperformed by Gemma-3-27B (&rho;=0.501) and comparable to Scout 17B (&rho;=0.410). The 8B Llama (&rho;=0.244) is the only model where size clearly limits performance.
</div>

<h3>3.3 Prior Results: Eedi (UK MCQs)</h3>

<p>Before the SmartPaper phases, we conducted extensive experiments on 1,869 Eedi maths MCQs using the same DOE approach. The results are summarized here for comparison.</p>

<h4>3.3.1 Eedi Screening (n=20 probe items)</h4>

<p>A Box-Behnken response surface design (46 configurations) varying prompt style, temperature, batch size, misconception hints, and model. Only teacher-prediction framing at high temperature produced signal (&rho;&asymp;0.50 per-rep after 10-rep validation).</p>

<h4>3.3.2 Eedi Prompt Optimization (n=20 probe items)</h4>

<table>
<tr><th>Prompt</th><th>&rho; (10-rep mean)</th><th>SD</th></tr>
<tr><td>Error analysis (t=2.0)</td><td class="sig">0.500</td><td>0.111</td></tr>
<tr><td>Contrastive (t=1.5)</td><td class="sig">0.513</td><td>0.097</td></tr>
</table>

<h4>3.3.3 Eedi Cross-Model Survey (n=20 probe items, contrastive prompt)</h4>

<table>
<tr><th>Model</th><th>Per-rep &rho;</th><th>SD</th></tr>
<tr><td>Gemini 3 Flash</td><td class="sig">0.470</td><td>0.076</td></tr>
<tr><td>Llama-4-Scout 17B</td><td>0.355</td><td>0.167</td></tr>
<tr><td>DeepSeek V3</td><td>0.338</td><td>0.140</td></tr>
<tr><td>GPT-4o</td><td class="null">0.172</td><td>0.126</td></tr>
<tr><td>Qwen3-32B</td><td class="null">0.005</td><td>0.110</td></tr>
<tr><td>Gemini 2.0 Flash</td><td class="null">&minus;0.200</td><td>0.132</td></tr>
</table>

<h4>3.3.4 Eedi Confirmation: Low Signal on Misconception-Targeted Items</h4>

<table>
<tr><th>Item set</th><th>n</th><th>&rho;</th><th>p</th><th>95% CI</th></tr>
<tr><td>Probe 20</td><td>20</td><td class="sig">0.462</td><td>0.04</td><td>&mdash;</td></tr>
<tr><td>New 30 (random)</td><td>30</td><td class="null">&minus;0.176</td><td>0.35</td><td>&mdash;</td></tr>
<tr class="highlight"><td><strong>105-item confirmation</strong></td><td><strong>105</strong></td><td class="null"><strong>0.114</strong></td><td><strong>0.25</strong></td><td><strong>[&minus;0.07, 0.30]</strong></td></tr>
</table>

<div class="note">
<strong>The 20-item probe inflated the signal.</strong> The probe set was stratified by difficulty quintile and contained two extreme-difficulty outliers (IRT b=&minus;5.18, &minus;3.37) that gave it 2.3&times; the difficulty SD of the non-probe items. Bootstrap resampling (10,000 random 20-item subsets from 105) yields mean &rho;=0.110; only 2.5% reach &rho;&ge;0.50. The probe result was a tail event, not a representative estimate.
</div>

<h4>3.3.5 Structured Elicitation on Eedi</h4>

<p>We tested predictions from four learning science traditions. Each predicts richer specification of student cognition should improve estimation.</p>

<table>
<tr><th>Strategy</th><th>Theory</th><th>&rho;</th><th>vs. baseline</th></tr>
<tr><td>Direct prediction</td><td>KC theory</td><td class="sig">0.500</td><td>&mdash;</td></tr>
<tr><td>Two-stage cognitive chains</td><td>Conceptual change</td><td>0.508</td><td>+0.01</td></tr>
<tr><td>Buggy reasoning (no hint)</td><td>BUGGY</td><td>0.488</td><td>&minus;0.01</td></tr>
<tr><td>Buggy reasoning (with hint)</td><td>BUGGY + ESS</td><td class="null">0.193</td><td>&minus;0.31</td></tr>
<tr><td>Cognitive modeling (10 CoT)</td><td>SMART/sim</td><td class="null">0.165</td><td>&minus;0.34</td></tr>
</table>

<div class="finding">
<strong>Finding 5: Deliberation hurts.</strong> On Eedi, every form of structured elicitation matched or degraded direct prediction. Providing misconception hints <em>actively hurt</em> (&rho;=0.19 vs. 0.50). Reasoning models (Gemini 2.5 Pro, DeepSeek Reasoner) performed worse than non-reasoning models. The task requires fast pattern matching, not step-by-step analysis.
</div>

<h3>3.4 The Dissociation</h3>

<div id="dissociationPlot" class="plot-container" style="min-height:370px"></div>
<div class="figure-caption">Figure 3. The central finding: generalisation contrast. SmartPaper signal holds from probe to full set and improves with theory-driven prompts. Eedi signal does not hold at scale.</div>

<table>
<tr><th></th><th>Eedi (MCQ, misconception-targeted)</th><th>SmartPaper (open-ended, multi-subject)</th></tr>
<tr><td>Best probe result</td><td>&rho;=0.50 (n=20)</td><td>&rho;=0.77 (n=20)</td></tr>
<tr><td>Full-set confirmation</td><td class="null">&rho;=0.114 (n=105, ns)</td><td class="sig">&rho;=0.686 (n=140)</td></tr>
<tr><td>Generalises?</td><td class="null"><strong>No</strong></td><td class="sig"><strong>Yes</strong></td></tr>
<tr><td>Best method</td><td>Teacher prediction (&rho;&asymp;0.50 on probe only)</td><td>prerequisite_chain (&rho;=0.686)</td></tr>
</table>

<h4>SmartPaper by Subject</h4>

<table>
<tr><th>Subject</th><th>&rho; (teacher, t=2.0)</th><th>n</th></tr>
<tr><td>Science</td><td class="sig">0.734</td><td>32</td></tr>
<tr><td>Social Science</td><td class="sig">0.702</td><td>34</td></tr>
<tr><td>Mathematics</td><td class="sig">0.600</td><td>33</td></tr>
<tr><td>English</td><td class="sig">0.432</td><td>41</td></tr>
</table>

<div id="subjectPlot" class="plot-container" style="min-height:320px"></div>
<div class="figure-caption">Figure 4. SmartPaper results by subject (teacher prompt). Signal is strongest where content complexity varies most (Science, Social Science) and weakest for English.</div>

<h3>3.5 Phase 3: Cross-Dataset Confirmation (DBE-KT22, n=168)</h3>

<p>Phase 3 tests whether Phase 1 findings transfer to a held-out dataset: 168 undergraduate computer science MCQs from a South African university (DBE-KT22; 1,300+ students). Prompts were adapted for the new population and format while preserving framing type. Notably, the correct answer is <strong>not</strong> shown to the LLM, matching the information available in Phase 1 (where items are open-ended).</p>

<p>Four hypotheses were pre-specified before seeing Phase 3 results:</p>

<ol>
  <li><strong>H1:</strong> prerequisite_chain produces &rho; &gt; 0.30 on DBE-KT22</li>
  <li><strong>H2:</strong> SmartPaper &rho; &gt; DBE-KT22 &rho; for every prompt</li>
  <li><strong>H3:</strong> Prompt ranking is preserved across datasets</li>
  <li><strong>H4:</strong> prerequisite_chain &gt; teacher on DBE-KT22</li>
</ol>

<table>
<tr><th>Framing</th><th>&rho; (t=1.0)</th><th>&rho; (t=2.0)</th><th>95% CI (best)</th><th>MAE</th><th>Bias</th></tr>
<tr class="highlight"><td><strong>prerequisite_chain</strong></td><td class="sig">0.519</td><td class="sig"><strong>0.531</strong></td><td>[0.411, 0.640]</td><td>0.159</td><td>&minus;0.103</td></tr>
<tr><td>teacher</td><td class="sig"><strong>0.521</strong></td><td class="sig">0.518</td><td>[0.394, 0.631]</td><td>0.188</td><td>&minus;0.151</td></tr>
<tr><td>error_analysis</td><td class="sig"><strong>0.516</strong></td><td class="sig">0.506</td><td>[0.391, 0.623]</td><td>0.188</td><td>&minus;0.151</td></tr>
<tr><td>devil_advocate</td><td class="sig">0.403</td><td class="sig"><strong>0.420</strong></td><td>[0.278, 0.543]</td><td>0.268</td><td>&minus;0.251</td></tr>
<tr><td>cognitive_load</td><td class="sig">0.455</td><td class="sig"><strong>0.469</strong></td><td>[0.341, 0.586]</td><td>0.152</td><td>&minus;0.074</td></tr>
</table>

<div id="phase3Bar" class="plot-container" style="min-height:380px"></div>
<div class="figure-caption">Figure 5. Phase 3 cross-dataset confirmation: SmartPaper (Phase 1) vs. DBE-KT22 (Phase 3). All five framings transfer with reduced but significant signal (&rho; &gt; 0.30). The top-3 ranking is preserved across datasets, but the spread compresses markedly.</div>

<h4>Hypothesis Tests</h4>

<table>
<tr><th>Hypothesis</th><th>Result</th><th>Status</th></tr>
<tr><td><strong>H1:</strong> prerequisite_chain &rho; &gt; 0.30</td><td>&rho;=0.531 [0.411, 0.640]</td><td class="sig"><strong>PASS</strong></td></tr>
<tr><td><strong>H2:</strong> SmartPaper &rho; &gt; DBE-KT22 &rho;</td><td>All 5 framings: SP &gt; DBE</td><td class="sig"><strong>PASS</strong> (5/5)</td></tr>
<tr><td><strong>H3:</strong> Prompt ranking preserved</td><td>Top-3 ranking preserved (prerequisite_chain, teacher, error_analysis); cognitive_load and devil_advocate swap positions</td><td class="sig"><strong>PASS</strong> (largely preserved)</td></tr>
<tr><td><strong>H4:</strong> prerequisite_chain &gt; teacher</td><td>0.531 vs. 0.521 (&Delta;=+0.010)</td><td><strong>PASS</strong> (but advantage reduced from 0.131 to 0.010)</td></tr>
</table>

<div class="finding">
<strong>Finding 6: Cross-dataset transfer confirmed, but theory-driven advantage attenuates.</strong> All five framings produce significant signal on DBE-KT22: prerequisite_chain (&rho;=0.531), teacher (&rho;=0.521), error_analysis (&rho;=0.516), cognitive_load (&rho;=0.469), and devil_advocate (&rho;=0.420). The top-3 ranking is preserved across datasets, but the spread compresses: the 0.131 advantage on SmartPaper reduces to 0.015 on DBE-KT22. Notably, cognitive_load achieves the best calibration (MAE=0.152, bias=&minus;0.074) despite mid-range ranking accuracy, suggesting that ranking and calibration may be partially dissociable. The method transfers; the relative advantage of structured prompting does not.
</div>

<div class="note">
<strong>Calibration direction reversed.</strong> On SmartPaper, models overestimate student ability (bias +0.11 to +0.43). On DBE-KT22, they underestimate it (bias &minus;0.10 to &minus;0.15). This is consistent with the datasets: SmartPaper students (Indian government schools) score lower than models expect; DBE-KT22 students (university CS) score higher (mean p=0.79).
</div>

<!-- ============================================================ -->
<h2>4. Discussion</h2>

<h3>4.1 Why Theory-Driven Counting Works</h3>

<p>The top two prompts (prerequisite_chain, cognitive_load) share a structure: they ask the model to perform an explicit counting operation grounded in a learning science construct. prerequisite_chain asks &ldquo;list the prerequisite skills and count failure points.&rdquo; cognitive_load asks &ldquo;count the elements a student must hold in working memory simultaneously.&rdquo; Both convert difficulty estimation from a holistic judgment into a structured enumeration.</p>

<p>This works for a specific reason: the counting task creates an intermediate representation that correlates with difficulty. Items requiring more prerequisites or more working memory elements are genuinely harder. The model already &ldquo;knows&rdquo; this implicitly, but the counting prompt forces it to surface the knowledge as a structured feature rather than a vague impression.</p>

<p>This is distinct from cognitive scaffolding or student simulation. The model is not simulating student cognition&mdash;it is analyzing item structure. The signal comes from item properties visible in the text, not from modeling student-item interactions.</p>

<h3>4.2 Why Simulation Fails</h3>

<p>Across all 6 models in Phase 2, simulation produced near-zero signal (mean &rho;=0.097 across models). Contrastive prompting, which asks the model to compare items, also produced negligible signal (mean &rho;=0.030). Only direct teacher prediction yielded consistent results.</p>

<p>The failure of simulation confirms the finding from Eedi: LLMs cannot model the population-level prevalence of errors. They can identify <em>what</em> errors are possible but not <em>how many students</em> would make each error. This requires empirical calibration data that is not in the training set.</p>

<h3>4.3 Complexity-Driven vs. Selectivity-Driven Difficulty</h3>

<p>The dissociation between SmartPaper and Eedi maps onto a distinction in what makes items difficult:</p>

<p><strong>Complexity-driven difficulty</strong> (SmartPaper): The item requires more cognitive steps, more prerequisites, less familiar content. This is visible in the item text. An LLM can detect that &ldquo;explain the water cycle&rdquo; is harder than &ldquo;name the capital of India&rdquo; because it requires more cognitive operations.</p>

<p><strong>Selectivity-driven difficulty</strong> (Eedi): The item&rsquo;s difficulty depends on whether specific distractors trigger specific misconceptions in specific student populations. Two items testing the same concept (&minus;1&times;&minus;4 vs. 3&times;&minus;2) can have very different error rates depending on how numbers interact with common bugs. This is an empirical property of student-item interactions, not visible in the text.</p>

<p>Phase 3 provides a partial test: DBE-KT22 items are MCQs (like Eedi) but their difficulty varies in cognitive complexity across 27 knowledge components (unlike Eedi&rsquo;s misconception-targeted design). The method achieves &rho;=0.53 on these MCQs, confirming that format is not the confound&mdash;the source of difficulty variance is.</p>

<h3>4.4 Practical Recommendations</h3>

<ol>
  <li><strong>Use theory-grounded counting prompts.</strong> prerequisite_chain or cognitive_load, not generic &ldquo;estimate difficulty.&rdquo;</li>
  <li><strong>Do not use simulation.</strong> Student roleplay and classroom simulation produce near-zero signal across all models tested.</li>
  <li><strong>Characterize your difficulty source.</strong> If items vary in cognitive complexity (multi-step reasoning, prerequisite chains), LLM estimation is viable (&rho;&asymp;0.6&ndash;0.7). If difficulty depends on distractor-misconception interactions, it is not.</li>
  <li><strong>Model choice matters less than prompt choice.</strong> Gemini 3 Flash leads, but the prompt framing effect (&Delta;&rho;&asymp;0.20) exceeds the model effect (&Delta;&rho;&asymp;0.15 across viable models).</li>
  <li><strong>Validate beyond small probe sets.</strong> Our Eedi experience (&rho;=0.50 on 20 items &rarr; &rho;=0.11 on 105) is a cautionary tale for the field.</li>
</ol>

<h3>4.5 Limitations</h3>

<ol>
  <li><strong>Three datasets, one model in Phase 3.</strong> Cross-dataset confirmation uses only Gemini 3 Flash. Phase 2 shows model choice matters less than prompt choice, but single-model confirmation is a limitation.</li>
  <li><strong>Not pre-registered.</strong> Phases 1&ndash;2 are exploratory by DOE design. Phase 3 hypotheses were pre-specified before seeing results but not formally registered.</li>
  <li><strong>Calibration direction varies.</strong> Models overestimate student ability on SmartPaper (bias +0.11 to +0.43) but underestimate on DBE-KT22 (bias &minus;0.10 to &minus;0.15). Ranking accuracy (Spearman &rho;) is unaffected, but absolute predictions require dataset-specific calibration.</li>
  <li><strong>Information asymmetry across datasets.</strong> SmartPaper items are open-ended (no answer options shown). DBE-KT22 items are MCQ (answer options shown, but correct answer withheld). Eedi items are MCQ with correct answer shown in some experiments. These differences limit direct comparison of absolute &rho; values.</li>
  <li><strong>Surface feature confound.</strong> On SmartPaper, text length correlates &rho;=&minus;0.44 with difficulty. The LLM&rsquo;s &rho;=0.686 exceeds this, but surface features contribute partially.</li>
  <li><strong>Theory-driven advantage did not transfer.</strong> The 0.131 advantage of prerequisite_chain over teacher on SmartPaper reduced to 0.010 on DBE-KT22. This may reflect dataset-specific properties (DBE-KT22&rsquo;s restricted domain of database systems) rather than a general limitation of structured prompting.</li>
</ol>

<!-- ============================================================ -->
<h2>5. Conclusion</h2>

<p>We applied sequential design of experiments to map the parameter landscape for LLM item difficulty estimation. Three findings stand out. First, prompt framing grounded in learning science theory (counting prerequisite failure points, counting working memory elements) outperforms both intuitive judgment and cognitive simulation by &rho;&asymp;0.13&ndash;0.20. Second, this signal generalizes across the full 140-item SmartPaper set (&rho;=0.686) but not to misconception-targeted MCQs (&rho;=0.114 on 105 Eedi items). Third, every form of deliberation and scaffolding either matched or hurt performance&mdash;including student simulation, reasoning models, and misconception hints.</p>

<p>The practical upshot: LLMs can estimate difficulty when it comes from cognitive complexity visible in item text, but not when it comes from empirical student-error patterns. Use counting prompts, not simulation. And always validate on a full item set&mdash;probe-set correlations are unreliable.</p>

<!-- ============================================================ -->
<h2>References</h2>

<div class="ref">Attali, Y. (2024). Can language models estimate item difficulty? <em>Educational Measurement: Issues and Practice</em>.</div>
<div class="ref">Benedetto, L., et al. (2023). On the application of large language models for language teaching. <em>AAAI Workshop on AI for Education</em>.</div>
<div class="ref">Box, G. E. P., &amp; Behnken, D. W. (1960). Some new three level designs for the study of quantitative variables. <em>Technometrics</em>, 2(4), 455&ndash;475.</div>
<div class="ref">Box, G. E. P., &amp; Draper, N. R. (2007). <em>Response Surfaces, Mixtures, and Ridge Analyses</em> (2nd ed.). Wiley.</div>
<div class="ref">Brown, J. S., &amp; Burton, R. R. (1978). Diagnostic models for procedural bugs in basic mathematical skills. <em>Cognitive Science</em>, 2(2), 155&ndash;192.</div>
<div class="ref">Koedinger, K. R., Corbett, A. T., &amp; Perfetti, C. (2012). The Knowledge-Learning-Instruction framework. <em>Cognitive Science</em>, 36(5), 757&ndash;798.</div>
<div class="ref">Kr&ouml;ger, B., et al. (2025). Take out your calculators: LLM-simulated classrooms for item difficulty estimation. arXiv:2601.09953.</div>
<div class="ref">Razavi, R., &amp; Powers, D. (2025). Estimating item difficulty using large language models. arXiv:2504.08804.</div>
<div class="ref">Shulman, L. S. (1986). Those who understand: Knowledge growth in teaching. <em>Educational Researcher</em>, 15(2), 4&ndash;14.</div>
<div class="ref">Sweller, J. (1988). Cognitive load during problem solving. <em>Cognitive Science</em>, 12(2), 257&ndash;285.</div>
<div class="ref">Tack, A., &amp; Piech, C. (2024). Harnessing LLMs for automated prediction of item difficulty. <em>BEA Workshop, ACL 2024</em>.</div>

<hr style="margin:2rem 0">
<p style="font-size:0.8rem; color:var(--muted)">
  Draft generated February 2026. Data, code, and raw model outputs available at [repository URL].
  All experiments used publicly available APIs. Research conducted using Claude Code (Anthropic).
</p>

<!-- ============================================================ -->
<!-- FIGURES (Plotly.js) -->
<!-- ============================================================ -->
<script>
const plotFont = {family: 'Georgia'};
const gridColor = '#eee';

// ============================================================
// Figure 1: Phase 1 Prompt Screening
// ============================================================
{
  const framings = [
    'prerequisite<br>chain', 'cognitive<br>load', 'devil<br>advocate',
    'error<br>analysis', 'teacher', 'error<br>affordance', 'familiarity<br>gradient'
  ];
  const rhos = [0.686, 0.673, 0.596, 0.596, 0.555, 0.493, 0.483];
  const colors = rhos.map(r => r >= 0.65 ? '#16a34a' : r >= 0.55 ? '#2563eb' : '#d97706');

  Plotly.newPlot('phase1Bar', [{
    type: 'bar', x: framings, y: rhos,
    marker: {color: colors},
    text: rhos.map(r => '&rho;=' + r.toFixed(3)),
    textposition: 'outside', textfont: {size: 11},
  }], {
    height: 380, margin: {l:60, r:20, t:20, b:90},
    yaxis: {title:'Spearman &rho; (140 items)', range:[0, 0.8], gridcolor: gridColor},
    font: plotFont,
    shapes: [{
      type:'line', x0:-0.5, x1:6.5, y0:0.555, y1:0.555,
      line:{color:'#999', width:2, dash:'dash'},
    }],
    annotations: [{
      x:6, y:0.57, text:'teacher baseline', showarrow:false,
      font:{size:10, color:'#999'},
    }],
  });
}

// ============================================================
// Figure 2: Phase 2 Model Ã— Prompt Heatmap
// ============================================================
{
  const models = ['Gemini 3 Flash', 'Gemma-3-27B', 'Llama-3.3-70B', 'Llama-4-Scout', 'GPT-4o', 'Llama-3.1-8B'];
  const prompts = ['Teacher', 'Simulation', 'Contrastive'];
  const z = [
    [0.550, 0.271, 0.077],
    [0.501, 0.239, -0.058],
    [0.452, -0.026, 0.100],
    [0.410, 0.041, 0.058],
    [0.360, 0.030, 0.159],
    [0.244, 0.025, -0.155],
  ];
  Plotly.newPlot('phase2Heatmap', [{
    z: z, x: prompts, y: models, type: 'heatmap',
    colorscale: [[0,'#fee2e2'],[0.3,'#fef9c3'],[0.6,'#dcfce7'],[1,'#16a34a']],
    zmin: -0.2, zmax: 0.6,
    text: z.map(row => row.map(v => '&rho;=' + v.toFixed(3))),
    texttemplate: '%{text}', textfont: {size: 12},
    hoverinfo: 'z',
    colorbar: {title: '&rho;', titleside: 'right'},
  }], {
    height: 350, margin: {l: 150, r: 80, t: 20, b: 50},
    font: plotFont,
  });
}

// ============================================================
// Figure 3: Dissociation (SmartPaper vs Eedi)
// ============================================================
{
  Plotly.newPlot('dissociationPlot', [
    {
      x: ['Probe (20)', 'Full set'],
      y: [0.462, 0.114],
      name: 'Eedi MCQs (misconception)',
      type: 'bar',
      marker: {color: '#dc2626'},
      text: ['&rho;=0.462', '&rho;=0.114 (ns)'],
      textposition: 'outside', textfont: {size: 12},
    },
    {
      x: ['Probe (20)', 'Full set'],
      y: [0.768, 0.686],
      name: 'SmartPaper (open-ended)',
      type: 'bar',
      marker: {color: '#2563eb'},
      text: ['&rho;=0.768', '&rho;=0.686'],
      textposition: 'outside', textfont: {size: 12},
    },
  ], {
    height: 370, margin: {l:60, r:20, t:30, b:50},
    barmode: 'group',
    yaxis: {title:'Spearman &rho;', range:[-0.1, 0.95], gridcolor: gridColor},
    font: plotFont,
    legend: {x: 0.45, y: 0.95},
    shapes: [{type:'line', x0:-0.5, x1:1.5, y0:0, y1:0, line:{color:'#999',width:1,dash:'dash'}}],
    annotations: [
      {x:'Full set', y:-0.05, text:'n=105', showarrow:false, font:{size:10, color:'#dc2626'}},
      {x:'Full set', y:0.63, text:'n=140', showarrow:false, font:{size:10, color:'#2563eb'}},
    ],
  });
}

// ============================================================
// Figure 4: SmartPaper by subject
// ============================================================
{
  const subjects = ['Science', 'Social Science', 'Mathematics', 'English'];
  const rhos = [0.734, 0.702, 0.600, 0.432];
  const ns = [32, 34, 33, 41];
  const colors = rhos.map(r => r >= 0.6 ? '#16a34a' : r >= 0.4 ? '#2563eb' : '#d97706');

  Plotly.newPlot('subjectPlot', [{
    type: 'bar', x: subjects, y: rhos,
    marker: {color: colors},
    text: rhos.map((r,i) => '&rho;=' + r.toFixed(3) + ' (n=' + ns[i] + ')'),
    textposition: 'outside', textfont: {size: 11},
  }], {
    height: 320, margin: {l:60, r:20, t:20, b:50},
    yaxis: {title:'Spearman &rho;', range:[0, 0.85], gridcolor: gridColor},
    font: plotFont,
  });
}

// ============================================================
// Figure 5: Phase 3 Cross-Dataset Confirmation
// ============================================================
{
  const framings = ['prerequisite_chain', 'teacher', 'error_analysis', 'cognitive_load', 'devil_advocate'];
  const sp = [0.686, 0.555, 0.596, 0.596, 0.493];
  const dbe = [0.531, 0.521, 0.516, 0.469, 0.420];

  Plotly.newPlot('phase3Bar', [
    {type:'bar', name:'SmartPaper (Phase 1)', x:framings, y:sp,
     marker:{color:'#2563eb'},
     text:sp.map(r=>'&rho;='+r.toFixed(3)), textposition:'outside', textfont:{size:11}},
    {type:'bar', name:'DBE-KT22 (Phase 3)', x:framings, y:dbe,
     marker:{color:'#16a34a'},
     text:dbe.map(r=>'&rho;='+r.toFixed(3)), textposition:'outside', textfont:{size:11}},
  ], {
    height:380, margin:{l:60,r:20,t:30,b:120}, barmode:'group',
    yaxis:{title:'Spearman &rho;', range:[0,0.8], gridcolor:gridColor},
    legend:{x:0.55, y:0.95},
    font:plotFont,
    shapes:[{type:'line',x0:-0.5,x1:4.5,y0:0.3,y1:0.3,
      line:{color:'#999',width:1.5,dash:'dot'}}],
    annotations:[{x:3.2,y:0.32,text:'H1 threshold (&rho;=0.30)',
      showarrow:false,font:{size:9,color:'#999'}}],
  });
}
</script>

</body>
</html>
