<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Replication Dashboard — LLM Difficulty Estimation</title>
<script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
<style>
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #0a0a0a; color: #e0e0e0; }
  .container { max-width: 1400px; margin: 0 auto; padding: 24px; }
  h1 { font-size: 1.8rem; font-weight: 600; margin-bottom: 4px; color: #fff; }
  .subtitle { color: #888; font-size: 0.95rem; margin-bottom: 32px; }
  .grid { display: grid; grid-template-columns: 1fr 1fr; gap: 24px; margin-bottom: 32px; }
  .card { background: #141414; border: 1px solid #222; border-radius: 12px; padding: 20px; }
  .card h2 { font-size: 1.1rem; font-weight: 500; margin-bottom: 16px; color: #ccc; }
  .full-width { grid-column: 1 / -1; }
  .stat-row { display: flex; gap: 16px; margin-bottom: 24px; flex-wrap: wrap; }
  .stat { background: #1a1a1a; border: 1px solid #2a2a2a; border-radius: 8px; padding: 16px 20px; flex: 1; min-width: 140px; }
  .stat .label { font-size: 0.75rem; color: #666; text-transform: uppercase; letter-spacing: 0.05em; }
  .stat .value { font-size: 1.8rem; font-weight: 600; margin-top: 4px; }
  .stat .detail { font-size: 0.8rem; color: #888; margin-top: 2px; }
  .null-result { border-color: #6a2d2d; }
  .null-result .value { color: #f87171; }
  .positive { border-color: #2d6a2d; }
  .positive .value { color: #4ade80; }
  .neutral .value { color: #facc15; }
  .section-title { font-size: 1.3rem; font-weight: 600; margin: 40px 0 16px; color: #fff; border-bottom: 1px solid #222; padding-bottom: 8px; }
  .insight { background: #1a1a0a; border-left: 3px solid #facc15; padding: 12px 16px; margin: 16px 0; border-radius: 0 8px 8px 0; font-size: 0.9rem; color: #ccc; line-height: 1.5; }
  .insight.negative { background: #1a0a0a; border-left-color: #f87171; }
  .insight.positive { background: #0a1a0a; border-left-color: #4ade80; }
  table { width: 100%; border-collapse: collapse; font-size: 0.85rem; }
  th { text-align: left; padding: 10px 12px; border-bottom: 2px solid #333; color: #888; font-weight: 500; cursor: pointer; user-select: none; }
  th:hover { color: #ccc; }
  th .sort-arrow { font-size: 0.7rem; margin-left: 4px; }
  td { padding: 10px 12px; border-bottom: 1px solid #1a1a1a; }
  tr:hover { background: #1a1a1a; }
  .plot-container { width: 100%; min-height: 400px; }
  .badge { display: inline-block; padding: 2px 8px; border-radius: 4px; font-size: 0.75rem; font-weight: 600; }
  .badge-null { background: #3a1a1a; color: #f87171; }
  .badge-sig { background: #1a3a1a; color: #4ade80; }
  .badge-pending { background: #3a3a1a; color: #facc15; }
  .badge-planned { background: #1a1a3a; color: #60a5fa; }
  .methodology-tag { display: inline-block; padding: 2px 8px; border-radius: 4px; font-size: 0.7rem; background: #222; color: #888; margin-right: 4px; }
</style>
</head>
<body>
<div class="container">

  <!-- Navigation bar -->
  <div style="display:flex;gap:12px;margin-bottom:20px">
    <a href="replication_dashboard.html" style="padding:6px 16px;border-radius:6px;background:#333;color:#fff;text-decoration:none;font-size:0.85rem">Replication Dashboard</a>
    <a href="rsm_analysis.html" style="padding:6px 16px;border-radius:6px;background:#1a1a1a;color:#888;text-decoration:none;font-size:0.85rem;border:1px solid #333">RSM / Optimization Dashboard</a>
  </div>

  <h1>LLM Difficulty Estimation: Replication Results</h1>
  <p class="subtitle">Corrected ground truth (Kaggle/NeurIPS alignment fix) | 1,869 Eedi UK math MCQs | 2026-02-02</p>

  <!-- Top-level finding -->
  <div class="stat-row">
    <div class="stat null-result">
      <div class="label">Universal Finding</div>
      <div class="value">r &asymp; 0</div>
      <div class="detail">All single-pass methods null after data fix</div>
    </div>
    <div class="stat positive">
      <div class="label">Exception</div>
      <div class="value">&rho; = 0.666</div>
      <div class="detail">RSM: Gemini Flash + error_analysis + t=2.0 + averaging (n=20 items, 3 reps)</div>
    </div>
    <div class="stat neutral">
      <div class="label">Models Tested</div>
      <div class="value">7</div>
      <div class="detail">GPT-4o-mini, Llama 3.3, Llama 4 Scout/Maverick, Qwen3, Gemini Flash, DeepSeek V3</div>
    </div>
    <div class="stat neutral">
      <div class="label">Methods Tested</div>
      <div class="value">6</div>
      <div class="detail">Direct, Feature, Uncertainty, Classroom Sim, Error Analysis A/B</div>
    </div>
    <div class="stat" style="border-color:#2d4a6a">
      <div class="label">Est. API Cost</div>
      <div class="value" style="color:#60a5fa">~$3</div>
      <div class="detail">Single-pass experiments (GPT-4o-mini dominant)</div>
    </div>
  </div>

  <!-- Section 1: The null wall -->
  <div class="section-title">1. The Null Wall: Single-Pass Methods</div>

  <div class="grid">
    <div class="card full-width">
      <h2>All Single-Pass Results (Corrected Ground Truth)</h2>
      <div id="nullWall" class="plot-container"></div>
    </div>
  </div>

  <div class="insight negative">
    <strong>Key finding:</strong> After correcting the Kaggle/NeurIPS answer ordering bug, every single-pass difficulty estimation method produces r &asymp; 0. This includes 5 prompt strategies, 7 models across 4 providers, feature extraction with GBM, uncertainty measures, and classroom simulation. Published benchmarks (r=0.54-0.87) do not replicate on this dataset with corrected ground truth.
  </div>

  <!-- Section 2: Complete results table -->
  <div class="section-title">2. Complete Results Table</div>

  <div class="card full-width">
    <table id="resultsTable">
      <thead>
        <tr>
          <th data-col="method">Method <span class="sort-arrow"></span></th>
          <th data-col="model">Model <span class="sort-arrow"></span></th>
          <th data-col="prompt">Prompt <span class="sort-arrow"></span></th>
          <th data-col="n">n valid <span class="sort-arrow"></span></th>
          <th data-col="temp">Temp <span class="sort-arrow"></span></th>
          <th data-col="nReps">N reps <span class="sort-arrow"></span></th>
          <th data-col="r">Pearson r <span class="sort-arrow"></span></th>
          <th data-col="rho">Spearman &rho; <span class="sort-arrow"></span></th>
          <th data-col="target">Target <span class="sort-arrow"></span></th>
          <th data-col="ci">Bootstrap 95% CI <span class="sort-arrow"></span></th>
          <th data-col="status">Status <span class="sort-arrow"></span></th>
        </tr>
      </thead>
      <tbody></tbody>
    </table>
  </div>

  <!-- Section 3: A/B Test -->
  <div class="section-title">3. Error Analysis A/B Test</div>

  <div class="insight">
    <strong>Question:</strong> The RSM experiment achieved &rho;=0.666 using the error_analysis prompt with Gemini Flash at t=2.0. This prompt has two ingredients: (A) a reasoning scaffold (teacher marking exams, analyzing misconceptions), and (B) a structured distribution output format (per-option percentages at 4 ability levels). Which ingredient matters?
  </div>

  <div class="grid">
    <div class="card">
      <h2>A/B Test Design</h2>
      <div style="padding: 12px; font-size: 0.9rem; line-height: 1.6;">
        <p><strong>Condition A (direct):</strong> Full reasoning scaffold &rarr; single ESTIMATE: XX number</p>
        <p style="margin-top:8px"><strong>Condition B (sim):</strong> Full reasoning scaffold &rarr; per-level per-option distribution format</p>
        <p style="margin-top:12px;color:#888">Both at temperature 2.0. Tested across 3 models.</p>
        <p style="margin-top:12px;color:#888">If A shows signal: reasoning scaffold is the key ingredient.<br>
        If only B shows signal: the distribution format forces better calibration.<br>
        If neither shows signal: the RSM's multi-sample averaging is the key ingredient.</p>
      </div>
    </div>
    <div class="card">
      <h2>A/B Results</h2>
      <div id="abTest" class="plot-container" style="min-height:300px"></div>
    </div>
  </div>

  <div class="insight negative">
    <strong>A/B finding:</strong> Neither condition shows meaningful signal on Llama 3.3 70b (n=1,800+). DeepSeek V3 and Gemini Flash runs still accumulating data (slow APIs). Early DeepSeek results (n~120) also null. This suggests single-pass estimation fails regardless of prompt structure — the RSM's multi-sample averaging with Gemini Flash may be the essential ingredient.
  </div>

  <!-- Section 4: Model matters -->
  <div class="section-title">4. Model Specificity</div>

  <div class="grid">
    <div class="card full-width">
      <h2>Cross-Model Comparison (Direct Estimation, expert prompt)</h2>
      <div id="modelCompare" class="plot-container" style="min-height:350px"></div>
    </div>
  </div>

  <div class="insight">
    <strong>From RSM experiments:</strong> Gemini Flash (&rho;=0.58) >> Llama 70B (0.27) >> Qwen 32B (0.01) &asymp; GPT-4o-mini (-0.09). But in single-pass mode without RSM averaging, even Gemini shows no signal. The capability is model-specific AND requires the multi-sample averaging methodology.
  </div>

  <!-- Section 5: Cross-Model Ranking -->
  <div class="section-title">5. Cross-Model Ranking (RSM Pipeline)</div>

  <div class="grid">
    <div class="card full-width">
      <h2>Cross-Model Ranking (20 probe items, 3 reps, error_analysis @ t=2.0)</h2>
      <div id="crossModelRanking" class="plot-container" style="min-height:500px"></div>
    </div>
  </div>

  <div class="insight positive">
    <strong>Averaging is magic:</strong> Llama-4-Scout jumps 0.371 &rarr; 0.624 with 3-rep averaging (avgPred column). Even modest single-rep correlations become strong when noise is cancelled through multi-sample aggregation. Gemini 3 Flash leads at &rho;=0.604 (mean) / 0.666 (averaged prediction).
  </div>

  <!-- Section 6: 3-Rep Scaling -->
  <div class="section-title">6. 3-Rep Scaling Experiment</div>

  <div class="card full-width">
    <div style="padding: 16px; font-size: 0.9rem; line-height: 1.7;">
      <span class="badge badge-planned">PLANNED</span>
      <p style="margin-top:12px">Experiment in progress: Running 3-rep averaging on full 1,869 items with top-tier models (Llama-4-Scout, DeepSeek V3, Gemini Flash). Results will test whether the averaging effect observed on 20 probe items scales to the full dataset.</p>
    </div>
  </div>

  <!-- Section 7: What the RSM does differently -->
  <div class="section-title">7. Why RSM Works (Hypothesis)</div>

  <div class="grid">
    <div class="card full-width">
      <div style="padding: 12px; font-size: 0.9rem; line-height: 1.7;">
        <p>The RSM (Response Surface Method) experiment achieves &rho;=0.666 where all single-pass methods fail. The key differences:</p>
        <ol style="margin: 12px 0 12px 24px;">
          <li><strong>Multi-sample averaging:</strong> 3 replications at high temperature, averaged. This cancels noise while preserving signal.</li>
          <li><strong>Structured output format:</strong> Per-option percentages at 4 ability levels forces the model to decompose the estimation task.</li>
          <li><strong>Gemini Flash specifically:</strong> Only this model shows signal in the RSM framework. Other models are null even with RSM.</li>
          <li><strong>Correlation target:</strong> RSM correlates against b_2pl (IRT difficulty), not raw p_value. These are monotonically related but b_2pl may be more discriminating.</li>
        </ol>
        <p style="color:#facc15"><strong>Implication for the paper:</strong> LLM difficulty estimation is not a general capability. It requires a specific pipeline (model + prompt + temperature + aggregation) and even then only works with particular models. This is a much more nuanced finding than "LLMs can estimate difficulty."</p>
      </div>
    </div>
  </div>

  <!-- Section 8: Timeline -->
  <div class="section-title">8. Research Timeline</div>

  <div class="card full-width">
    <div style="padding: 12px; font-size: 0.85rem; line-height: 1.6; color: #aaa;">
      <p><strong>Jan 30:</strong> Discovered Kaggle vs NeurIPS answer ordering bug. 76% of items had wrong ground truth for error rate calculations.</p>
      <p style="margin-top:8px"><strong>Feb 2 AM:</strong> Re-ran all fixable experiments with corrected data. Universal null result across 4 methodologies.</p>
      <p style="margin-top:8px"><strong>Feb 2 PM:</strong> Added Groq (Llama, Qwen), Gemini, DeepSeek providers. Tested 7 models total. All null.</p>
      <p style="margin-top:8px"><strong>Feb 2 PM:</strong> Designed A/B test to decompose error_analysis prompt into reasoning vs distribution format. Llama null for both. DeepSeek/Gemini in progress.</p>
      <p style="margin-top:8px"><strong>Confirmed:</strong> RSM result (&rho;=0.666) remains valid — it correlates against b_2pl (ordering-independent) and uses correct_answer_kaggle only for answer text (not scoring).</p>
    </div>
  </div>

</div>

<script>
// === DATA ===

const allResults = [
  // Direct estimation - expert prompt
  { method: "Direct", model: "GPT-4o-mini", prompt: "expert", n: 1869, temp: 0, nReps: 1, r: 0.010, p: 0.673, rho: 0.008, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Direct", model: "GPT-4o-mini", prompt: "basic", n: 1869, temp: 0, nReps: 1, r: -0.031, p: 0.181, rho: -0.028, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Direct", model: "GPT-4o-mini", prompt: "irt", n: 1869, temp: 0, nReps: 1, r: 0.005, p: 0.832, rho: 0.001, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Direct", model: "GPT-4o-mini", prompt: "comparative", n: 1869, temp: 0, nReps: 1, r: -0.012, p: 0.601, rho: -0.009, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Direct", model: "Qwen3 32B", prompt: "expert", n: 1548, temp: 0, nReps: 1, r: 0.015, p: 0.544, rho: 0.012, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Direct", model: "Llama 3.3 70B", prompt: "expert", n: 1530, temp: 0, nReps: 1, r: -0.032, p: 0.205, rho: -0.029, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Direct", model: "Llama 4 Scout", prompt: "expert", n: 1868, temp: 0, nReps: 1, r: -0.013, p: 0.561, rho: -0.010, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Direct", model: "Llama 4 Maverick", prompt: "expert", n: 1735, temp: 0, nReps: 1, r: -0.024, p: 0.326, rho: -0.021, target: "p_value", ci: "\u2014", status: "null" },
  // Feature extraction
  { method: "Feature (GBM)", model: "GPT-4o", prompt: "7 features", n: 1869, temp: 0, nReps: 1, r: 0.063, p: 0.223, rho: 0.058, target: "p_value", ci: "\u2014", status: "null" },
  // Uncertainty
  { method: "Uncertainty", model: "GPT-4o-mini", prompt: "answer prob", n: 1869, temp: 0, nReps: 1, r: -0.007, p: 0.747, rho: -0.005, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Uncertainty", model: "GPT-4o-mini", prompt: "entropy", n: 1869, temp: 0, nReps: 1, r: 0.016, p: 0.486, rho: 0.014, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Uncertainty", model: "GPT-4o-mini", prompt: "permutation", n: 1869, temp: 0, nReps: 1, r: -0.010, p: 0.662, rho: -0.008, target: "p_value", ci: "\u2014", status: "null" },
  // Classroom sim
  { method: "Classroom Sim", model: "Llama 3.3 70B", prompt: "20 students", n: 1869, temp: 0, nReps: 1, r: 0.013, p: 0.560, rho: 0.011, target: "p_value", ci: "\u2014", status: "null" },
  // Error analysis A/B
  { method: "Error A/B", model: "Llama 3.3 70B", prompt: "direct (A)", n: 1858, temp: 2.0, nReps: 1, r: -0.068, p: 0.004, rho: -0.061, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Error A/B", model: "Llama 3.3 70B", prompt: "sim (B)", n: 1817, temp: 2.0, nReps: 1, r: -0.008, p: 0.740, rho: -0.009, target: "p_value", ci: "\u2014", status: "null" },
  { method: "Error A/B", model: "DeepSeek V3", prompt: "direct (A)", n: 120, temp: 2.0, nReps: 1, r: -0.013, p: 0.888, rho: -0.008, target: "p_value", ci: "\u2014", status: "pending" },
  { method: "Error A/B", model: "DeepSeek V3", prompt: "sim (B)", n: 33, temp: 2.0, nReps: 1, r: -0.167, p: 0.354, rho: -0.140, target: "p_value", ci: "\u2014", status: "pending" },
  { method: "Error A/B", model: "Gemini Flash", prompt: "direct (A)", n: 18, temp: 2.0, nReps: 1, r: -0.248, p: 0.321, rho: -0.185, target: "p_value", ci: "\u2014", status: "pending" },
  { method: "Error A/B", model: "Gemini Flash", prompt: "sim (B)", n: 6, temp: 2.0, nReps: 1, r: -0.131, p: null, rho: -0.314, target: "p_value", ci: "\u2014", status: "pending" },
  // RSM (reference)
  { method: "RSM (3-rep avg)", model: "Gemini Flash", prompt: "error_analysis t=2.0", n: 20, temp: 2.0, nReps: 3, r: null, p: 0.001, rho: 0.666, target: "b_2pl", ci: "[0.35, 0.88]", status: "sig" },
  // 3-rep fullset planned
  { method: "3-Rep Fullset", model: "Llama-4-Scout", prompt: "error_analysis", n: 1869, temp: 2.0, nReps: 3, r: null, p: null, rho: null, target: "b_2pl", ci: "\u2014", status: "planned" },
  { method: "3-Rep Fullset", model: "DeepSeek V3", prompt: "contrastive", n: 1869, temp: 1.5, nReps: 3, r: null, p: null, rho: null, target: "b_2pl", ci: "\u2014", status: "planned" },
  { method: "3-Rep Fullset", model: "Gemini Flash", prompt: "error_analysis", n: 1869, temp: 2.0, nReps: 3, r: null, p: null, rho: null, target: "b_2pl", ci: "\u2014", status: "planned" },
];

// === POPULATE TABLE ===
const tbody = document.querySelector('#resultsTable tbody');
function renderTable(data) {
  tbody.innerHTML = '';
  data.forEach(r => {
    const tr = document.createElement('tr');
    const badgeClass = r.status === 'null' ? 'badge-null' : r.status === 'sig' ? 'badge-sig' : r.status === 'planned' ? 'badge-planned' : 'badge-pending';
    const badgeText = r.status === 'null' ? 'NULL' : r.status === 'sig' ? 'SIG' : r.status === 'planned' ? 'PLANNED' : 'IN PROGRESS';
    tr.innerHTML = `
      <td>${r.method}</td>
      <td>${r.model}</td>
      <td>${r.prompt}</td>
      <td>${r.n}</td>
      <td>${r.temp}</td>
      <td>${r.nReps}</td>
      <td>${r.r !== null ? r.r.toFixed(3) : '\u2014'}</td>
      <td>${r.rho !== null ? r.rho.toFixed(3) : '\u2014'}</td>
      <td>${r.target}</td>
      <td>${r.ci}</td>
      <td><span class="badge ${badgeClass}">${badgeText}</span></td>
    `;
    tbody.appendChild(tr);
  });
}
renderTable(allResults);

// === SORTABLE TABLE ===
let sortCol = null, sortAsc = true;
document.querySelectorAll('#resultsTable th').forEach(th => {
  th.addEventListener('click', () => {
    const col = th.dataset.col;
    if (sortCol === col) { sortAsc = !sortAsc; } else { sortCol = col; sortAsc = true; }
    const sorted = [...allResults].sort((a, b) => {
      let va = a[col], vb = b[col];
      if (va === null || va === undefined || va === '\u2014') va = sortAsc ? Infinity : -Infinity;
      if (vb === null || vb === undefined || vb === '\u2014') vb = sortAsc ? Infinity : -Infinity;
      if (typeof va === 'string') return sortAsc ? va.localeCompare(vb) : vb.localeCompare(va);
      return sortAsc ? va - vb : vb - va;
    });
    document.querySelectorAll('#resultsTable th .sort-arrow').forEach(s => s.textContent = '');
    th.querySelector('.sort-arrow').textContent = sortAsc ? ' \u25B2' : ' \u25BC';
    renderTable(sorted);
  });
});

// === NULL WALL CHART ===
const nullData = allResults.filter(r => r.status !== 'pending' && r.status !== 'planned' && r.rho !== null);
const labels = nullData.map(r => `${r.model}<br>${r.prompt}`);
const rhos = nullData.map(r => r.rho);
const colors = nullData.map(r => r.status === 'sig' ? '#4ade80' : '#f87171');

Plotly.newPlot('nullWall', [{
  type: 'bar',
  x: labels,
  y: rhos,
  marker: { color: colors },
  text: rhos.map(r => r.toFixed(3)),
  textposition: 'outside',
  textfont: { size: 10, color: '#888' }
}], {
  paper_bgcolor: '#141414',
  plot_bgcolor: '#141414',
  font: { color: '#888' },
  yaxis: { title: 'Spearman rho', range: [-0.2, 0.75], gridcolor: '#222', zeroline: true, zerolinecolor: '#444' },
  xaxis: { tickfont: { size: 9 } },
  margin: { t: 20, b: 120 },
  shapes: [
    { type: 'line', x0: -0.5, x1: labels.length - 0.5, y0: 0, y1: 0, line: { color: '#444', width: 2, dash: 'dash' } },
    { type: 'rect', x0: -0.5, x1: labels.length - 0.5, y0: -0.05, y1: 0.05, fillcolor: 'rgba(248,113,113,0.1)', line: { width: 0 } }
  ],
  annotations: [{
    x: labels.length - 1, y: 0.666, text: 'RSM result', showarrow: true, arrowhead: 2,
    font: { color: '#4ade80', size: 11 }, arrowcolor: '#4ade80'
  }]
}, { responsive: true });

// === A/B TEST CHART ===
const abData = allResults.filter(r => r.method === 'Error A/B');
const abModels = [...new Set(abData.map(r => r.model))];
const directData = abModels.map(m => {
  const d = abData.find(r => r.model === m && r.prompt.includes('direct'));
  return d ? d.rho : 0;
});
const simData = abModels.map(m => {
  const d = abData.find(r => r.model === m && r.prompt.includes('sim'));
  return d ? d.rho : 0;
});

Plotly.newPlot('abTest', [
  { type: 'bar', name: 'A: direct (reasoning\u2192number)', x: abModels, y: directData, marker: { color: '#60a5fa' } },
  { type: 'bar', name: 'B: sim (reasoning\u2192distributions)', x: abModels, y: simData, marker: { color: '#f472b6' } }
], {
  paper_bgcolor: '#141414', plot_bgcolor: '#141414', font: { color: '#888' },
  yaxis: { title: 'Spearman rho', gridcolor: '#222', zeroline: true, zerolinecolor: '#444', range: [-0.4, 0.1] },
  barmode: 'group', margin: { t: 20 },
  legend: { x: 0, y: 1.15, orientation: 'h', font: { size: 10 } },
  shapes: [{ type: 'rect', x0: -0.5, x1: 2.5, y0: -0.05, y1: 0.05, fillcolor: 'rgba(255,255,255,0.05)', line: { width: 0 } }]
}, { responsive: true });

// === MODEL COMPARISON ===
const directModels = allResults.filter(r => r.method === 'Direct' && r.prompt === 'expert');
Plotly.newPlot('modelCompare', [{
  type: 'bar',
  x: directModels.map(r => r.model),
  y: directModels.map(r => r.rho),
  marker: { color: directModels.map(r => r.rho > 0.05 ? '#4ade80' : r.rho < -0.05 ? '#f87171' : '#666') },
  text: directModels.map(r => `r=${r.r.toFixed(3)}`),
  textposition: 'outside',
  textfont: { size: 10, color: '#888' }
}], {
  paper_bgcolor: '#141414', plot_bgcolor: '#141414', font: { color: '#888' },
  yaxis: { title: 'Spearman rho', gridcolor: '#222', zeroline: true, zerolinecolor: '#444', range: [-0.1, 0.1] },
  margin: { t: 20, b: 80 },
  shapes: [{ type: 'rect', x0: -0.5, x1: 4.5, y0: -0.05, y1: 0.05, fillcolor: 'rgba(248,113,113,0.1)', line: { width: 0 } }]
}, { responsive: true });

// === CROSS-MODEL RANKING CHART ===
const crossModelRanking = [
  {name:'Gemini 3 Flash', mean:0.604, std:0.062, avgPred:0.666, prompt:'error_analysis', temp:2.0, nReps:3, nItems:20, provider:'Google'},
  {name:'Llama-4-Scout 17B', mean:0.505, std:0.048, avgPred:0.609, prompt:'error_analysis', temp:2.0, nReps:3, nItems:20, provider:'Groq (free)'},
  {name:'Llama-4-Scout (contrastive)', mean:0.371, std:0.115, avgPred:0.624, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'Groq (free)'},
  {name:'DeepSeek-Chat V3', mean:0.338, std:0.140, avgPred:0.409, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'DeepSeek'},
  {name:'GPT-OSS-120B', mean:0.336, std:0.100, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'Groq'},
  {name:'Kimi-K2', mean:0.336, std:0.116, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'Groq'},
  {name:'Claude Sonnet 4', mean:0.263, std:0.166, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'Anthropic'},
  {name:'Gemini 2.5 Pro', mean:0.240, std:0.242, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'Google'},
  {name:'Llama-3.3-70B', mean:0.257, std:0.062, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'Groq'},
  {name:'GPT-4o', mean:0.172, std:0.126, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'OpenAI'},
  {name:'Gemini 2.5 Flash', mean:0.088, std:0.131, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'Google'},
  {name:'Claude Haiku 3.5', mean:0.077, std:0.186, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'Anthropic'},
  {name:'Qwen3-32B', mean:0.005, std:0.110, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'Groq'},
  {name:'Gemini 2.0 Flash', mean:-0.019, std:0.050, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'Google'},
  {name:'GPT-4o-mini', mean:-0.092, std:0.266, avgPred:null, prompt:'contrastive', temp:1.5, nReps:3, nItems:20, provider:'OpenAI'},
];

const cmSorted = [...crossModelRanking].sort((a, b) => b.mean - a.mean);
const cmNames = cmSorted.map(d => d.name);
const cmMeans = cmSorted.map(d => d.mean);
const cmStds = cmSorted.map(d => d.std);
const cmColors = cmSorted.map(d => d.mean > 0.3 ? '#4ade80' : d.mean > 0.1 ? '#facc15' : d.mean > 0 ? '#888' : '#f87171');

Plotly.newPlot('crossModelRanking', [{
  type: 'bar',
  y: cmNames,
  x: cmMeans,
  orientation: 'h',
  error_x: { type: 'data', array: cmStds, visible: true, color: '#666', thickness: 1.5 },
  marker: { color: cmColors },
  text: cmMeans.map(v => v.toFixed(3)),
  textposition: 'outside',
  textfont: { size: 10, color: '#888' }
}], {
  paper_bgcolor: '#141414', plot_bgcolor: '#141414', font: { color: '#888' },
  xaxis: { title: 'Mean Spearman rho (across 3 reps)', gridcolor: '#222', zeroline: true, zerolinecolor: '#444', range: [-0.4, 0.85] },
  yaxis: { autorange: 'reversed', tickfont: { size: 11 } },
  margin: { l: 200, t: 20, r: 60 },
  shapes: [
    { type: 'line', y0: -0.5, y1: cmNames.length - 0.5, x0: 0, x1: 0, line: { color: '#444', width: 2, dash: 'dash' } }
  ]
}, { responsive: true });

</script>
</body>
</html>
