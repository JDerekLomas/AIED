[
  {
    "model": "llama33_70b",
    "model_name": "Llama-3.3-70B",
    "params": 70000000000.0,
    "prompt": "contrastive",
    "n_items": 5,
    "n_reps": 3,
    "avg_pred_rho": 0.821,
    "avg_pred_p": 0.088587,
    "ci_95": [
      NaN,
      NaN
    ],
    "per_rep": [],
    "mean_per_rep_rho": null,
    "sd_per_rep_rho": null,
    "by_subject": {}
  },
  {
    "model": "gemini",
    "model_name": "Gemini 3 Flash",
    "params": null,
    "prompt": "prerequisite_chain",
    "n_items": 114,
    "n_reps": 3,
    "avg_pred_rho": 0.633,
    "avg_pred_p": 0.0,
    "ci_95": [
      0.506,
      0.735
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.627,
        "n": 79
      },
      {
        "rep": 1,
        "rho": 0.783,
        "n": 17
      },
      {
        "rep": 2,
        "rho": 0.606,
        "n": 107
      }
    ],
    "mean_per_rep_rho": 0.672,
    "sd_per_rep_rho": 0.079,
    "by_subject": {
      "English": {
        "rho": 0.596,
        "p": 0.0001,
        "n": 40
      },
      "Mathematics": {
        "rho": 0.753,
        "p": 0.0,
        "n": 28
      },
      "Science": {
        "rho": 0.705,
        "p": 0.0004,
        "n": 21
      },
      "Social Science": {
        "rho": 0.746,
        "p": 0.0,
        "n": 25
      }
    }
  },
  {
    "model": "gemini",
    "model_name": "Gemini 3 Flash",
    "params": null,
    "prompt": "cognitive_load",
    "n_items": 114,
    "n_reps": 3,
    "avg_pred_rho": 0.619,
    "avg_pred_p": 0.0,
    "ci_95": [
      0.485,
      0.735
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.523,
        "n": 82
      },
      {
        "rep": 1,
        "rho": 0.611,
        "n": 98
      },
      {
        "rep": 2,
        "rho": 0.405,
        "n": 22
      }
    ],
    "mean_per_rep_rho": 0.513,
    "sd_per_rep_rho": 0.084,
    "by_subject": {
      "English": {
        "rho": 0.509,
        "p": 0.0009,
        "n": 39
      },
      "Mathematics": {
        "rho": 0.652,
        "p": 0.0013,
        "n": 21
      },
      "Science": {
        "rho": 0.71,
        "p": 0.0003,
        "n": 21
      },
      "Social Science": {
        "rho": 0.715,
        "p": 0.0,
        "n": 33
      }
    }
  },
  {
    "model": "gemma3_27b",
    "model_name": "Gemma-3-27B",
    "params": 27000000000.0,
    "prompt": "contrastive",
    "n_items": 6,
    "n_reps": 3,
    "avg_pred_rho": 0.577,
    "avg_pred_p": 0.230732,
    "ci_95": [
      NaN,
      NaN
    ],
    "per_rep": [],
    "mean_per_rep_rho": null,
    "sd_per_rep_rho": null,
    "by_subject": {}
  },
  {
    "model": "gemini",
    "model_name": "Gemini 3 Flash",
    "params": null,
    "prompt": "teacher",
    "n_items": 140,
    "n_reps": 3,
    "avg_pred_rho": 0.55,
    "avg_pred_p": 0.0,
    "ci_95": [
      0.411,
      0.664
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.548,
        "n": 140
      },
      {
        "rep": 1,
        "rho": 0.534,
        "n": 140
      },
      {
        "rep": 2,
        "rho": 0.531,
        "n": 140
      }
    ],
    "mean_per_rep_rho": 0.538,
    "sd_per_rep_rho": 0.007,
    "by_subject": {
      "English": {
        "rho": 0.43,
        "p": 0.005,
        "n": 41
      },
      "Mathematics": {
        "rho": 0.625,
        "p": 0.0001,
        "n": 33
      },
      "Science": {
        "rho": 0.717,
        "p": 0.0,
        "n": 32
      },
      "Social Science": {
        "rho": 0.705,
        "p": 0.0,
        "n": 34
      }
    }
  },
  {
    "model": "gemma3_27b",
    "model_name": "Gemma-3-27B",
    "params": 27000000000.0,
    "prompt": "teacher",
    "n_items": 140,
    "n_reps": 3,
    "avg_pred_rho": 0.501,
    "avg_pred_p": 0.0,
    "ci_95": [
      0.332,
      0.648
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.544,
        "n": 128
      },
      {
        "rep": 1,
        "rho": 0.452,
        "n": 122
      },
      {
        "rep": 2,
        "rho": 0.472,
        "n": 118
      }
    ],
    "mean_per_rep_rho": 0.489,
    "sd_per_rep_rho": 0.04,
    "by_subject": {
      "English": {
        "rho": 0.186,
        "p": 0.2433,
        "n": 41
      },
      "Mathematics": {
        "rho": 0.666,
        "p": 0.0,
        "n": 33
      },
      "Science": {
        "rho": 0.586,
        "p": 0.0004,
        "n": 32
      },
      "Social Science": {
        "rho": 0.617,
        "p": 0.0001,
        "n": 34
      }
    }
  },
  {
    "model": "gemma3_27b",
    "model_name": "Gemma-3-27B",
    "params": 27000000000.0,
    "prompt": "cognitive_load",
    "n_items": 101,
    "n_reps": 2,
    "avg_pred_rho": 0.5,
    "avg_pred_p": 0.0,
    "ci_95": [
      0.31,
      0.662
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.651,
        "n": 56
      },
      {
        "rep": 1,
        "rho": 0.457,
        "n": 91
      }
    ],
    "mean_per_rep_rho": 0.554,
    "sd_per_rep_rho": 0.097,
    "by_subject": {
      "English": {
        "rho": 0.304,
        "p": 0.1226,
        "n": 27
      },
      "Mathematics": {
        "rho": 0.703,
        "p": 0.0,
        "n": 28
      },
      "Science": {
        "rho": 0.404,
        "p": 0.077,
        "n": 20
      },
      "Social Science": {
        "rho": 0.632,
        "p": 0.0005,
        "n": 26
      }
    }
  },
  {
    "model": "gpt4o",
    "model_name": "GPT-4o",
    "params": null,
    "prompt": "teacher",
    "n_items": 140,
    "n_reps": 3,
    "avg_pred_rho": 0.494,
    "avg_pred_p": 0.0,
    "ci_95": [
      0.344,
      0.624
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.443,
        "n": 132
      },
      {
        "rep": 1,
        "rho": 0.449,
        "n": 128
      },
      {
        "rep": 2,
        "rho": 0.471,
        "n": 130
      }
    ],
    "mean_per_rep_rho": 0.454,
    "sd_per_rep_rho": 0.012,
    "by_subject": {
      "English": {
        "rho": 0.282,
        "p": 0.0738,
        "n": 41
      },
      "Mathematics": {
        "rho": 0.527,
        "p": 0.0016,
        "n": 33
      },
      "Science": {
        "rho": 0.609,
        "p": 0.0002,
        "n": 32
      },
      "Social Science": {
        "rho": 0.679,
        "p": 0.0,
        "n": 34
      }
    }
  },
  {
    "model": "llama33_70b",
    "model_name": "Llama-3.3-70B",
    "params": 70000000000.0,
    "prompt": "teacher",
    "n_items": 140,
    "n_reps": 3,
    "avg_pred_rho": 0.48,
    "avg_pred_p": 0.0,
    "ci_95": [
      0.33,
      0.606
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.459,
        "n": 140
      },
      {
        "rep": 1,
        "rho": 0.454,
        "n": 140
      },
      {
        "rep": 2,
        "rho": 0.443,
        "n": 140
      }
    ],
    "mean_per_rep_rho": 0.452,
    "sd_per_rep_rho": 0.007,
    "by_subject": {
      "English": {
        "rho": 0.184,
        "p": 0.2487,
        "n": 41
      },
      "Mathematics": {
        "rho": 0.601,
        "p": 0.0002,
        "n": 33
      },
      "Science": {
        "rho": 0.51,
        "p": 0.0029,
        "n": 32
      },
      "Social Science": {
        "rho": 0.556,
        "p": 0.0006,
        "n": 34
      }
    }
  },
  {
    "model": "scout",
    "model_name": "Llama-4-Scout",
    "params": 109000000000.0,
    "prompt": "teacher",
    "n_items": 117,
    "n_reps": 3,
    "avg_pred_rho": 0.474,
    "avg_pred_p": 0.0,
    "ci_95": [
      0.302,
      0.614
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.38,
        "n": 105
      },
      {
        "rep": 1,
        "rho": 0.433,
        "n": 104
      },
      {
        "rep": 2,
        "rho": 0.404,
        "n": 103
      }
    ],
    "mean_per_rep_rho": 0.405,
    "sd_per_rep_rho": 0.022,
    "by_subject": {
      "English": {
        "rho": 0.293,
        "p": 0.0632,
        "n": 41
      },
      "Mathematics": {
        "rho": 0.624,
        "p": 0.0171,
        "n": 14
      },
      "Science": {
        "rho": 0.469,
        "p": 0.0118,
        "n": 28
      },
      "Social Science": {
        "rho": 0.531,
        "p": 0.0012,
        "n": 34
      }
    }
  },
  {
    "model": "gemma3_27b",
    "model_name": "Gemma-3-27B",
    "params": 27000000000.0,
    "prompt": "prerequisite_chain",
    "n_items": 8,
    "n_reps": 3,
    "avg_pred_rho": 0.464,
    "avg_pred_p": 0.247138,
    "ci_95": [
      NaN,
      NaN
    ],
    "per_rep": [],
    "mean_per_rep_rho": null,
    "sd_per_rep_rho": null,
    "by_subject": {
      "English": {
        "rho": 0.354,
        "p": 0.5594,
        "n": 5
      }
    }
  },
  {
    "model": "gemma3_27b",
    "model_name": "Gemma-3-27B",
    "params": 27000000000.0,
    "prompt": "simulation",
    "n_items": 38,
    "n_reps": 3,
    "avg_pred_rho": 0.416,
    "avg_pred_p": 0.009349,
    "ci_95": [
      0.088,
      0.673
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.611,
        "n": 28
      },
      {
        "rep": 1,
        "rho": 0.42,
        "n": 32
      },
      {
        "rep": 2,
        "rho": 0.441,
        "n": 32
      }
    ],
    "mean_per_rep_rho": 0.491,
    "sd_per_rep_rho": 0.086,
    "by_subject": {
      "English": {
        "rho": 0.611,
        "p": 0.012,
        "n": 16
      },
      "Mathematics": {
        "rho": 0.092,
        "p": 0.7885,
        "n": 11
      },
      "Science": {
        "rho": 0.487,
        "p": 0.2682,
        "n": 7
      }
    }
  },
  {
    "model": "llama33_70b",
    "model_name": "Llama-3.3-70B",
    "params": 70000000000.0,
    "prompt": "cognitive_load",
    "n_items": 140,
    "n_reps": 3,
    "avg_pred_rho": 0.403,
    "avg_pred_p": 1e-06,
    "ci_95": [
      0.242,
      0.552
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.394,
        "n": 140
      },
      {
        "rep": 1,
        "rho": 0.532,
        "n": 62
      },
      {
        "rep": 2,
        "rho": 0.296,
        "n": 139
      }
    ],
    "mean_per_rep_rho": 0.407,
    "sd_per_rep_rho": 0.097,
    "by_subject": {
      "English": {
        "rho": 0.179,
        "p": 0.2617,
        "n": 41
      },
      "Mathematics": {
        "rho": 0.582,
        "p": 0.0004,
        "n": 33
      },
      "Science": {
        "rho": 0.326,
        "p": 0.0684,
        "n": 32
      },
      "Social Science": {
        "rho": 0.529,
        "p": 0.0013,
        "n": 34
      }
    }
  },
  {
    "model": "scout",
    "model_name": "Llama-4-Scout",
    "params": 109000000000.0,
    "prompt": "cognitive_load",
    "n_items": 138,
    "n_reps": 3,
    "avg_pred_rho": 0.4,
    "avg_pred_p": 1e-06,
    "ci_95": [
      0.237,
      0.547
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.324,
        "n": 128
      },
      {
        "rep": 1,
        "rho": 0.632,
        "n": 64
      },
      {
        "rep": 2,
        "rho": 0.358,
        "n": 130
      }
    ],
    "mean_per_rep_rho": 0.438,
    "sd_per_rep_rho": 0.138,
    "by_subject": {
      "English": {
        "rho": 0.224,
        "p": 0.1598,
        "n": 41
      },
      "Mathematics": {
        "rho": 0.508,
        "p": 0.003,
        "n": 32
      },
      "Science": {
        "rho": 0.459,
        "p": 0.0083,
        "n": 32
      },
      "Social Science": {
        "rho": 0.561,
        "p": 0.0007,
        "n": 33
      }
    }
  },
  {
    "model": "llama33_70b",
    "model_name": "Llama-3.3-70B",
    "params": 70000000000.0,
    "prompt": "prerequisite_chain",
    "n_items": 140,
    "n_reps": 3,
    "avg_pred_rho": 0.396,
    "avg_pred_p": 1e-06,
    "ci_95": [
      0.237,
      0.537
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.198,
        "n": 140
      },
      {
        "rep": 1,
        "rho": 0.301,
        "n": 67
      },
      {
        "rep": 2,
        "rho": 0.314,
        "n": 139
      }
    ],
    "mean_per_rep_rho": 0.271,
    "sd_per_rep_rho": 0.052,
    "by_subject": {
      "English": {
        "rho": 0.311,
        "p": 0.0475,
        "n": 41
      },
      "Mathematics": {
        "rho": 0.467,
        "p": 0.0062,
        "n": 33
      },
      "Science": {
        "rho": 0.446,
        "p": 0.0104,
        "n": 32
      },
      "Social Science": {
        "rho": 0.456,
        "p": 0.0067,
        "n": 34
      }
    }
  },
  {
    "model": "llama31_8b",
    "model_name": "Llama-3.1-8B",
    "params": 8000000000.0,
    "prompt": "contrastive",
    "n_items": 76,
    "n_reps": 3,
    "avg_pred_rho": 0.35,
    "avg_pred_p": 0.001923,
    "ci_95": [
      0.123,
      0.555
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.384,
        "n": 36
      },
      {
        "rep": 1,
        "rho": 0.327,
        "n": 32
      },
      {
        "rep": 2,
        "rho": 0.06,
        "n": 29
      }
    ],
    "mean_per_rep_rho": 0.257,
    "sd_per_rep_rho": 0.141,
    "by_subject": {
      "English": {
        "rho": 0.363,
        "p": 0.1161,
        "n": 20
      },
      "Mathematics": {
        "rho": 0.389,
        "p": 0.1698,
        "n": 14
      },
      "Science": {
        "rho": 0.1,
        "p": 0.6335,
        "n": 25
      },
      "Social Science": {
        "rho": 0.509,
        "p": 0.0368,
        "n": 17
      }
    }
  },
  {
    "model": "scout",
    "model_name": "Llama-4-Scout",
    "params": 109000000000.0,
    "prompt": "prerequisite_chain",
    "n_items": 136,
    "n_reps": 3,
    "avg_pred_rho": 0.345,
    "avg_pred_p": 3.9e-05,
    "ci_95": [
      0.181,
      0.491
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.349,
        "n": 127
      },
      {
        "rep": 1,
        "rho": 0.373,
        "n": 62
      },
      {
        "rep": 2,
        "rho": 0.388,
        "n": 119
      }
    ],
    "mean_per_rep_rho": 0.37,
    "sd_per_rep_rho": 0.016,
    "by_subject": {
      "English": {
        "rho": 0.204,
        "p": 0.2009,
        "n": 41
      },
      "Mathematics": {
        "rho": 0.305,
        "p": 0.0954,
        "n": 31
      },
      "Science": {
        "rho": 0.442,
        "p": 0.0146,
        "n": 30
      },
      "Social Science": {
        "rho": 0.473,
        "p": 0.0047,
        "n": 34
      }
    }
  },
  {
    "model": "qwen3_32b",
    "model_name": "Qwen3-32B",
    "params": 32000000000.0,
    "prompt": "teacher",
    "n_items": 55,
    "n_reps": 3,
    "avg_pred_rho": 0.304,
    "avg_pred_p": 0.024096,
    "ci_95": [
      0.05,
      0.53
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.343,
        "n": 25
      },
      {
        "rep": 1,
        "rho": 0.339,
        "n": 29
      },
      {
        "rep": 2,
        "rho": 0.247,
        "n": 18
      }
    ],
    "mean_per_rep_rho": 0.309,
    "sd_per_rep_rho": 0.044,
    "by_subject": {
      "English": {
        "rho": 0.477,
        "p": 0.1168,
        "n": 12
      },
      "Mathematics": {
        "rho": 0.624,
        "p": 0.0171,
        "n": 14
      },
      "Science": {
        "rho": -0.145,
        "p": 0.5792,
        "n": 17
      },
      "Social Science": {
        "rho": 0.334,
        "p": 0.2888,
        "n": 12
      }
    }
  },
  {
    "model": "llama31_8b",
    "model_name": "Llama-3.1-8B",
    "params": 8000000000.0,
    "prompt": "teacher",
    "n_items": 140,
    "n_reps": 3,
    "avg_pred_rho": 0.3,
    "avg_pred_p": 0.000313,
    "ci_95": [
      0.128,
      0.46
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.28,
        "n": 140
      },
      {
        "rep": 1,
        "rho": 0.249,
        "n": 139
      },
      {
        "rep": 2,
        "rho": 0.214,
        "n": 140
      }
    ],
    "mean_per_rep_rho": 0.248,
    "sd_per_rep_rho": 0.027,
    "by_subject": {
      "English": {
        "rho": 0.033,
        "p": 0.8392,
        "n": 41
      },
      "Mathematics": {
        "rho": 0.194,
        "p": 0.2799,
        "n": 33
      },
      "Science": {
        "rho": 0.24,
        "p": 0.1863,
        "n": 32
      },
      "Social Science": {
        "rho": 0.703,
        "p": 0.0,
        "n": 34
      }
    }
  },
  {
    "model": "llama31_8b",
    "model_name": "Llama-3.1-8B",
    "params": 8000000000.0,
    "prompt": "cognitive_load",
    "n_items": 103,
    "n_reps": 3,
    "avg_pred_rho": 0.214,
    "avg_pred_p": 0.029634,
    "ci_95": [
      0.018,
      0.394
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.059,
        "n": 70
      },
      {
        "rep": 1,
        "rho": 0.589,
        "n": 32
      },
      {
        "rep": 2,
        "rho": 0.285,
        "n": 51
      }
    ],
    "mean_per_rep_rho": 0.311,
    "sd_per_rep_rho": 0.217,
    "by_subject": {
      "English": {
        "rho": 0.144,
        "p": 0.4232,
        "n": 33
      },
      "Mathematics": {
        "rho": 0.232,
        "p": 0.324,
        "n": 20
      },
      "Science": {
        "rho": 0.225,
        "p": 0.3016,
        "n": 23
      },
      "Social Science": {
        "rho": 0.33,
        "p": 0.093,
        "n": 27
      }
    }
  },
  {
    "model": "gemini",
    "model_name": "Gemini 3 Flash",
    "params": null,
    "prompt": "simulation",
    "n_items": 44,
    "n_reps": 3,
    "avg_pred_rho": 0.163,
    "avg_pred_p": 0.291291,
    "ci_95": [
      -0.144,
      0.473
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.388,
        "n": 18
      },
      {
        "rep": 1,
        "rho": 0.204,
        "n": 25
      },
      {
        "rep": 2,
        "rho": -0.323,
        "n": 19
      }
    ],
    "mean_per_rep_rho": 0.09,
    "sd_per_rep_rho": 0.301,
    "by_subject": {
      "English": {
        "rho": 0.498,
        "p": 0.1189,
        "n": 11
      },
      "Mathematics": {
        "rho": 0.259,
        "p": 0.3713,
        "n": 14
      },
      "Science": {
        "rho": 0.06,
        "p": 0.8466,
        "n": 13
      },
      "Social Science": {
        "rho": -0.334,
        "p": 0.5177,
        "n": 6
      }
    }
  },
  {
    "model": "llama31_8b",
    "model_name": "Llama-3.1-8B",
    "params": 8000000000.0,
    "prompt": "simulation",
    "n_items": 139,
    "n_reps": 3,
    "avg_pred_rho": 0.128,
    "avg_pred_p": 0.132879,
    "ci_95": [
      -0.038,
      0.285
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.122,
        "n": 119
      },
      {
        "rep": 1,
        "rho": -0.02,
        "n": 118
      },
      {
        "rep": 2,
        "rho": 0.054,
        "n": 121
      }
    ],
    "mean_per_rep_rho": 0.052,
    "sd_per_rep_rho": 0.058,
    "by_subject": {
      "English": {
        "rho": 0.046,
        "p": 0.7759,
        "n": 40
      },
      "Mathematics": {
        "rho": 0.164,
        "p": 0.3627,
        "n": 33
      },
      "Science": {
        "rho": 0.1,
        "p": 0.5864,
        "n": 32
      },
      "Social Science": {
        "rho": 0.043,
        "p": 0.8108,
        "n": 34
      }
    }
  },
  {
    "model": "llama33_70b",
    "model_name": "Llama-3.3-70B",
    "params": 70000000000.0,
    "prompt": "simulation",
    "n_items": 129,
    "n_reps": 3,
    "avg_pred_rho": 0.123,
    "avg_pred_p": 0.163297,
    "ci_95": [
      -0.049,
      0.3
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.086,
        "n": 89
      },
      {
        "rep": 1,
        "rho": 0.037,
        "n": 84
      },
      {
        "rep": 2,
        "rho": 0.176,
        "n": 76
      }
    ],
    "mean_per_rep_rho": 0.1,
    "sd_per_rep_rho": 0.058,
    "by_subject": {
      "English": {
        "rho": 0.18,
        "p": 0.2784,
        "n": 38
      },
      "Mathematics": {
        "rho": -0.006,
        "p": 0.9759,
        "n": 31
      },
      "Science": {
        "rho": 0.048,
        "p": 0.802,
        "n": 30
      },
      "Social Science": {
        "rho": 0.152,
        "p": 0.4217,
        "n": 30
      }
    }
  },
  {
    "model": "llama31_8b",
    "model_name": "Llama-3.1-8B",
    "params": 8000000000.0,
    "prompt": "prerequisite_chain",
    "n_items": 110,
    "n_reps": 3,
    "avg_pred_rho": 0.044,
    "avg_pred_p": 0.649425,
    "ci_95": [
      -0.146,
      0.234
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.199,
        "n": 61
      },
      {
        "rep": 1,
        "rho": -0.13,
        "n": 35
      },
      {
        "rep": 2,
        "rho": 0.051,
        "n": 64
      }
    ],
    "mean_per_rep_rho": 0.04,
    "sd_per_rep_rho": 0.134,
    "by_subject": {
      "English": {
        "rho": 0.126,
        "p": 0.4779,
        "n": 34
      },
      "Mathematics": {
        "rho": -0.284,
        "p": 0.169,
        "n": 25
      },
      "Science": {
        "rho": 0.075,
        "p": 0.7288,
        "n": 24
      },
      "Social Science": {
        "rho": 0.297,
        "p": 0.1323,
        "n": 27
      }
    }
  },
  {
    "model": "qwen3_32b",
    "model_name": "Qwen3-32B",
    "params": 32000000000.0,
    "prompt": "simulation",
    "n_items": 12,
    "n_reps": 1,
    "avg_pred_rho": 0.032,
    "avg_pred_p": 0.922252,
    "ci_95": [
      -0.592,
      0.646
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.032,
        "n": 12
      }
    ],
    "mean_per_rep_rho": 0.032,
    "sd_per_rep_rho": 0.0,
    "by_subject": {
      "English": {
        "rho": 0.067,
        "p": 0.8636,
        "n": 9
      }
    }
  },
  {
    "model": "qwen3_32b",
    "model_name": "Qwen3-32B",
    "params": 32000000000.0,
    "prompt": "contrastive",
    "n_items": 3,
    "n_reps": 1,
    "avg_pred_rho": 0.0,
    "avg_pred_p": 1.0,
    "ci_95": [
      NaN,
      NaN
    ],
    "per_rep": [],
    "mean_per_rep_rho": null,
    "sd_per_rep_rho": null,
    "by_subject": {}
  },
  {
    "model": "scout",
    "model_name": "Llama-4-Scout",
    "params": 109000000000.0,
    "prompt": "contrastive",
    "n_items": 16,
    "n_reps": 3,
    "avg_pred_rho": -0.076,
    "avg_pred_p": 0.779345,
    "ci_95": [
      NaN,
      NaN
    ],
    "per_rep": [],
    "mean_per_rep_rho": null,
    "sd_per_rep_rho": null,
    "by_subject": {
      "Mathematics": {
        "rho": 0.082,
        "p": 0.8472,
        "n": 8
      }
    }
  },
  {
    "model": "scout",
    "model_name": "Llama-4-Scout",
    "params": 109000000000.0,
    "prompt": "simulation",
    "n_items": 135,
    "n_reps": 3,
    "avg_pred_rho": -0.225,
    "avg_pred_p": 0.008581,
    "ci_95": [
      -0.396,
      -0.044
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": -0.198,
        "n": 108
      },
      {
        "rep": 1,
        "rho": -0.189,
        "n": 112
      },
      {
        "rep": 2,
        "rho": -0.191,
        "n": 111
      }
    ],
    "mean_per_rep_rho": -0.193,
    "sd_per_rep_rho": 0.004,
    "by_subject": {
      "English": {
        "rho": -0.211,
        "p": 0.185,
        "n": 41
      },
      "Mathematics": {
        "rho": -0.392,
        "p": 0.0294,
        "n": 31
      },
      "Science": {
        "rho": -0.298,
        "p": 0.1162,
        "n": 29
      },
      "Social Science": {
        "rho": -0.318,
        "p": 0.0665,
        "n": 34
      }
    }
  },
  {
    "model": "gemini",
    "model_name": "Gemini 3 Flash",
    "params": null,
    "prompt": "contrastive",
    "n_items": 12,
    "n_reps": 3,
    "avg_pred_rho": -0.225,
    "avg_pred_p": 0.482673,
    "ci_95": [
      NaN,
      NaN
    ],
    "per_rep": [],
    "mean_per_rep_rho": null,
    "sd_per_rep_rho": null,
    "by_subject": {
      "Mathematics": {
        "rho": -0.2,
        "p": 0.7471,
        "n": 5
      },
      "Science": {
        "rho": NaN,
        "p": NaN,
        "n": 6
      }
    }
  },
  {
    "model": "gpt4o",
    "model_name": "GPT-4o",
    "params": null,
    "prompt": "contrastive",
    "n_items": 1,
    "n_reps": 1,
    "avg_pred_rho": NaN,
    "avg_pred_p": NaN,
    "ci_95": [
      NaN,
      NaN
    ],
    "per_rep": [],
    "mean_per_rep_rho": null,
    "sd_per_rep_rho": null,
    "by_subject": {}
  },
  {
    "model": "gpt4o",
    "model_name": "GPT-4o",
    "params": null,
    "prompt": "simulation",
    "n_items": 37,
    "n_reps": 3,
    "avg_pred_rho": 0.026,
    "avg_pred_p": 0.877493,
    "ci_95": [
      NaN,
      NaN
    ],
    "per_rep": [
      {
        "rep": 0,
        "rho": 0.07,
        "n": 17
      },
      {
        "rep": 1,
        "rho": -0.195,
        "n": 13
      },
      {
        "rep": 2,
        "rho": NaN,
        "n": 11
      }
    ],
    "mean_per_rep_rho": NaN,
    "sd_per_rep_rho": NaN,
    "by_subject": {
      "English": {
        "rho": 0.172,
        "p": 0.5565,
        "n": 14
      },
      "Mathematics": {
        "rho": -0.139,
        "p": 0.7205,
        "n": 9
      },
      "Science": {
        "rho": 0.131,
        "p": 0.8047,
        "n": 6
      },
      "Social Science": {
        "rho": -0.412,
        "p": 0.31,
        "n": 8
      }
    }
  }
]