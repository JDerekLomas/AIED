% AIED 2026 Submission â€” Springer LNCS/LNAI format
% 14 pages including references
\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{lineno}
\usepackage{subcaption}
\linenumbers

% Compact tables
\usepackage{array}
\newcolumntype{L}{>{\raggedright\arraybackslash}p}

\begin{document}

\title{It's Hard to Know How Hard It Is:\\Mapping the Design Space of LLM Item Difficulty Estimation}

% Double-blind: remove authors for submission
\author{Anonymous}
\institute{Anonymous Institution}

\maketitle

\begin{abstract}
Can LLMs estimate how hard a test question is? Published results disagree wildly---correlations range from near-zero to $r=0.87$---but each study typically tests one prompt on one model on one dataset. We map the design space systematically: 15 prompts grounded in learning science, 6 models, 3 datasets, and multiple temperature settings (\textasciitilde120 configurations, \textasciitilde\$100 in API calls). Prompts that direct the model to enumerate structural item features---prerequisites, cognitive load, error paths---achieve $\rho=0.65$--$0.69$ on open-ended items. However, this advantage attenuates on MCQs: on two independent datasets (DBE-KT22, BEA~2024), prompts achieve $\rho\approx0.45$--$0.58$, with simple teacher judgment nearly matching structured prompts. On the BEA~2024 benchmark, our zero-shot approach with linear calibration achieves RMSE$=$0.280---matching the best supervised system (0.281). Prompt design is the primary lever; model size, deliberation, and temperature each contribute minimally ($\eta^2 < 0.05$). Methodologically, we find that correlations from small item sets ($n<50$) are unreliable, and that unbalanced hyperparameter sweeps produce Simpson's paradox artifacts---we recommend two-stage sequential DOE for future work. LLM difficulty estimates can triage items into difficulty bands at \textasciitilde\$0.01 per item, but the signal is strongest for items whose difficulty varies across knowledge domains.

\keywords{item difficulty estimation \and LLM evaluation \and design of experiments \and psychometrics \and prompt engineering}
\end{abstract}

%% ============================================================
\section{Introduction}

A well-designed test gives each student questions at the right level of challenge---hard enough to reveal what they know, easy enough to avoid frustration. Getting this right depends on knowing how difficult each question is before it reaches a student. In practice, this means field-testing items with hundreds or thousands of real students and computing how many answer correctly. In classical test theory, this proportion is called $p$-correct; item response theory refines it with a latent difficulty parameter~$b$ that accounts for examinee ability~\cite{embretson2000irt}. Either way, calibrating each item requires collecting responses from 200 to 1,000+ students depending on the IRT model~\cite{deayala2009}, making difficulty estimation one of the most resource-intensive steps in assessment development.

That cost is becoming a bottleneck. LLMs can now generate assessment items at scale~\cite{moore2024}, but generated items arrive without calibrated difficulty estimates. Without such estimates, items cannot be assembled into well-targeted tests, used in adaptive systems, or compared to existing item banks. The question is whether LLMs can also estimate difficulty---replacing or supplementing the expensive field-testing step.

The task is harder than it appears. LLMs are trained on expert-written text and tend to find most items trivial; Li, Chen et al.~\cite{lichen2025} find that high model performance paradoxically impedes difficulty estimation---models that solve items easily cannot perceive what makes them hard for humans. This ``curse of knowledge'' creates a systematic bias: models overestimate student ability and compress difficulty estimates toward the easy end of the scale. Expert teacher judgment, by comparison, achieves median correlations of $r=0.66$~\cite{hoge1989} to mean $r=0.63$~\cite{sudkamp2012} with actual student performance---a useful reference point, though these studies measure judgment of individual students rather than aggregate item difficulty.

Recent work reports correlations ranging from $r\approx0$ for direct estimation~\cite{acquaye2025} to $r=0.87$ when LLM-extracted features are combined with gradient boosting~\cite{razavi2025}. A systematic review of 37 papers on text-based difficulty prediction found rapid growth but no consensus on methods, and noted that fine-tuned small language models (BERT, RoBERTa) outperform LLMs on structured prediction tasks~\cite{peters2025,lijiao2025}. The first shared task on automated difficulty prediction (BEA 2024)~\cite{yaneva2024} found that best results only marginally beat baselines on 667 medical items. Li, Chen et al.~\cite{lichen2025} evaluated 20+ models across multiple assessment domains and found that models converge on a shared ``machine consensus'' that systematically diverges from human difficulty perception.

A key limitation is that each study typically tests one model with one prompt on one dataset. When results disagree, it is unclear whether the discrepancy reflects model choice, prompt design, item properties, or student population. We address this by mapping the design space rather than testing any single configuration.

Our optimization target is \textbf{rank-order agreement} between LLM-predicted difficulty and empirical $p$-correct, measured by Spearman's~$\rho$. We choose rank correlation because practical applications---item sequencing, adaptive test assembly, item bank stratification---depend on difficulty \emph{ordering} rather than exact calibration. Each experimental condition prompts an LLM to estimate the proportion of students who would answer each item correctly; we then correlate these estimates against observed $p$-correct from real student responses.

\paragraph{Contributions.}
(1)~The first systematic comparison of 15 theory-grounded prompts for LLM difficulty estimation, tested across 6~models and 3~datasets (\textasciitilde120 configurations).
(2)~An \emph{item-centric vs.\ population-centric} distinction explaining why some student-focused prompts (buggy\_rules) succeed while others (synthetic\_students) fail.
(3)~Methodological guidance: Simpson's paradox artifacts in unbalanced hyperparameter sweeps, $\ge$100 items required for reliable validation, and two-stage sequential DOE as the recommended design.

%% ============================================================
\section{Related Work}

We organize prior work by approach and extract testable claims from each.

\paragraph{Direct LLM Estimation.}
Razavi \& Powers~\cite{razavi2025} report $r=0.83$ (math) and $r=0.81$ (reading) on K--5 items using GPT-4o. However, Acquaye et al.~\cite{acquaye2025} find $r\approx0$ for direct estimation on NAEP items, and Li, Chen et al.~\cite{lichen2025} find average $\rho=0.28$ across 20+ models on four assessment domains. \emph{Testable claim: direct estimation produces meaningful correlations ($\rho>0.30$) on curriculum-aligned items.}

\paragraph{Feature Extraction + ML.}
Razavi \& Powers~\cite{razavi2025} achieve their strongest results ($r=0.87$) by extracting features from LLMs and training gradient-boosted models. \emph{Testable claim: LLM-extracted features combined with ML outperform direct estimation.}

\paragraph{Simulated Classrooms.}
Acquaye et al.~\cite{acquaye2025} simulate classrooms of LLM students at multiple ability levels, achieving $r=0.75$--$0.82$ on NAEP items. Counterintuitively, weaker math models predicted difficulty better than stronger ones. However, Li, Chen et al.~\cite{lichen2025} find significant misalignment between AI and human difficulty perception. \emph{Testable claim: simulation outperforms direct estimation.}

\paragraph{Model Uncertainty.}
Zotos et al.~\cite{zotos2025} use LLM uncertainty features (first-token probability, choice-order sensitivity) to predict difficulty. \emph{Testable claim: items the model finds uncertain are items students find difficult.}

\paragraph{Reasoning Augmentation.}
Feng et al.~\cite{feng2025} report up to 28\% MSE reduction when generating reasoning before predicting difficulty. However, Li, Jiao et al.~\cite{lijiao2025} find minimal improvements from chain-of-thought prompting for fine-tuned models. \emph{Testable claim: structured deliberation improves difficulty estimation.}

\paragraph{Cognitive Item Models.}
A separate tradition predicts difficulty from structural item features. Embretson's~\cite{embretson1998} cognitive design system counts processing steps and working memory demands. Gorin~\cite{gorin2005} manipulates item features to generate items at target difficulties. The KLI framework~\cite{koedinger2012} posits that the number and nature of knowledge components drive learning difficulty. Recent work has used LLMs for KC tagging~\cite{moore2024}, but not to operationalize cognitive item models as difficulty estimation prompts. \emph{Testable claim: prompts grounded in cognitive item modeling outperform atheoretical prompts.}

%% ============================================================
\section{Hypotheses}

We derive hypotheses from three sources: replication of prior claims, predictions from learning science theory, and exploratory questions about the design space. Table~\ref{tab:hypotheses} lists all 13 hypotheses with their sources and predictions.

\begin{table}[t]
\caption{Hypotheses tested in this study. Verdicts: \checkmark\ = supported, $\times$ = rejected, $\sim$ = partial/mixed.}\label{tab:hypotheses}
\centering\footnotesize
\begin{tabular}{@{}cllll@{}}
\toprule
\# & Hypothesis & Source/Theory & Prediction & Verdict \\
\midrule
\multicolumn{5}{@{}l}{\emph{From Prior Work (Replication)}} \\
H1 & Direct estimation yields $\rho>0.30$ & Razavi \& Powers & $\rho > 0.30$ & \checkmark\ .56 \\
H2 & Simulation beats direct & Acquaye et al. & $\rho_\text{sim} > \rho_\text{direct}$ & $\times$\ .19$<$.56 \\
H3 & Larger models do better & Scaling expectation & Larger $\to$ higher $\rho$ & $\sim$ mixed \\
\midrule
\multicolumn{5}{@{}l}{\emph{From Learning Science Theory}} \\
H4 & Item analysis beats direct & Embretson (1998) & $\rho_\text{analysis} > \rho_\text{direct}$ & \checkmark\ .69$>$.56 \\
H5 & Prerequisite counting works & KLI (Koedinger) & More prereqs $\to$ harder & \checkmark\ .69 \\
H6 & Cognitive load counting works & CLT (Sweller) & More elements $\to$ harder & \checkmark\ .67 \\
H7 & Buggy rules analysis works & Brown \& Burton & More error paths $\to$ harder & \checkmark\ .66 \\
\midrule
\multicolumn{5}{@{}l}{\emph{Exploratory (Design Space)}} \\
H8 & Temperature enhances predictions & Stochastic diversity & Higher temp $\to$ higher $\rho$ & $\times$\ $\eta^2$=.001 \\
H9 & Multi-sample averaging helps & Wisdom of crowds & $\rho_{3\text{-rep}} > \rho_{1\text{-rep}}$ & $\sim$ +.03 \\
H10 & Prompt is primary lever & Design space & Prompt $>$ model + temp & \checkmark\ on open; $\sim$ on MCQ \\
H11 & Analysis needs capable models & Capacity limits & Analysis $\times$ capability & \checkmark\ interaction \\
H12 & Signal transfers across datasets & Generalizability & Signal on D1 $\to$ D2 & \checkmark\ attenuates \\
\bottomrule
\end{tabular}
\end{table}

%% ============================================================
\section{Method}

\subsection{Datasets}

\paragraph{SmartPaper (India, Open-ended)---Primary.}
140 open-ended questions from Indian state assessments across four subjects (English, Mathematics, Science, Social Science), Grades~6--8, with 728,000+ responses. Ground truth: classical $p$-correct (proportion scoring full marks). Difficulty range: 0.04--0.83, mean 0.29.

\paragraph{DBE-KT22 (South Africa, MCQ)---Confirmation.}
168 undergraduate computer science MCQs spanning 27 knowledge components, administered to 1,300+ students. Ground truth: classical $p$-correct. Differs from SmartPaper in format, domain, and population.

\paragraph{BEA 2024 Shared Task (USA, USMLE MCQ)---Validation.}
595 text-only items from the BEA~2024 automated difficulty prediction shared task~\cite{yaneva2024}, comprising USMLE Steps~1--3 questions (667 total items minus 72 requiring images). Ground truth: IRT difficulty parameters from operational administrations. This benchmark allows direct comparison with other difficulty estimation approaches.

\subsection{Design Space}

\begin{table}[t]
\caption{Experimental stages.}\label{tab:stages}
\centering\footnotesize
\begin{tabular}{@{}llll@{}}
\toprule
Stage & Purpose & Dataset & Design \\
\midrule
Screening & Map prompt space & SmartPaper (140) & 15 prompts $\times$ 2--5 temps $\times$ 3 reps \\
Model survey & Model generalization & SmartPaper (140) & 6 models $\times$ 3 prompts $\times$ 3 reps \\
Confirmation & Cross-dataset transfer & DBE-KT22 (168) & 3 prompts $\times$ 2 temps $\times$ 3 reps \\
Validation & Published benchmark & BEA 2024 (595) & 3 prompts $\times$ 1 temp $\times$ 3 reps \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Prompts}

All 16 prompts share a common output: the LLM predicts what proportion of students would answer each item correctly. They differ along two binary dimensions: whether the prompt directs the model to \emph{analyze structural item features} (prerequisites, steps, cognitive demands) before estimating, and whether it directs the model to \emph{model the student population} (reason about proficiency levels, struggles). Table~\ref{tab:prompts} lists all 16 prompts; 15 were tested in screening (synthetic\_students was added in the model survey). All prompts are available in the project repository.

\begin{table}[t]
\caption{Prompt taxonomy with factor coding. Item = analyzes item structure; Pop.\ = models student population. All 15 prompts except synthetic\_students were tested in screening.}\label{tab:prompts}
\centering\footnotesize
\begin{tabular}{@{}lccl@{}}
\toprule
Prompt & Item & Pop. & Theory/Prior Work \\
\midrule
teacher & & & PCK (Shulman, 1986) \\
verbalized\_sampling & & & Wisdom of crowds \\
familiarity\_gradient & \checkmark & & Transfer distance \\
contrastive & \checkmark & & Comparative judgment \\
prerequisite\_chain & \checkmark & & KC theory (Koedinger et al.) \\
cognitive\_load & \checkmark & & CLT (Sweller, 1988) \\
cognitive\_profile & \checkmark & & CLT + profiling \\
devil\_advocate & & \checkmark & Debiasing heuristics \\
teacher\_decomposed & & \checkmark & IRT stratification \\
classroom\_sim & & \checkmark & Acquaye et al. (2025) \\
imagine\_classroom & & \checkmark & Imagery-based reasoning \\
synthetic\_students$^*$ & & \checkmark & Student simulation \\
error\_analysis & \checkmark & \checkmark & Error analysis tradition \\
error\_affordance & \checkmark & \checkmark & Brown \& Burton (1978) \\
buggy\_rules & \checkmark & \checkmark & Brown \& Burton (1978) \\
misconception\_holistic & \checkmark & \checkmark & Chi et al.\ (1994) \\
\bottomrule
\multicolumn{4}{@{}l@{}}{\footnotesize $^*$Tested in model survey only ($\rho=0.19$, worst performer).}
\end{tabular}
\end{table}

\subsection{Models}

Six models spanning 8B to frontier-scale: Gemini~3~Flash (screening model), GPT-4o, Llama-3.3-70B, Gemma-3-27B, Llama-4-Scout (17B active/109B MoE), and Llama-3.1-8B. Qwen3-32B was tested but excluded due to low parse rates ($<$50\% valid responses).

\subsection{Statistical Approach}

Our primary metric is Spearman's~$\rho$ between LLM-predicted and observed $p$-correct. We report 95\% bootstrap confidence intervals (10,000 resamples) for primary results and MAE as a calibration measure. Each condition uses 3~replications; reported~$\rho$ is the averaged-prediction correlation (mean prediction across reps correlated with ground truth).

%% ============================================================
\section{Results}

We present results in five parts: the headline finding on item analysis prompts, the mechanism behind it, factors that matter less than expected, generalization across three datasets, and practical implications.

\subsection{Item Analysis Prompts Achieve $\rho=0.65$--$0.69$}

Fifteen prompts were screened on SmartPaper (140 open-ended items) using Gemini~3~Flash at 2--5 temperature settings with 3~replications per condition. The top three prompts---all directing the model to analyze structural item features before estimating difficulty---achieved $\rho=0.65$--$0.69$ (Table~\ref{tab:screening}, Figure~\ref{fig:screening}): \textbf{prerequisite\_chain} ($\rho=0.686$) enumerates knowledge prerequisites before estimating; \textbf{cognitive\_load} ($\rho=0.673$) counts interacting memory elements; and \textbf{buggy\_rules} ($\rho=0.655$) identifies procedural error paths. For context, meta-analyses report that experienced teachers can predict individual student performance with $r=0.63$--$0.66$~\cite{hoge1989,sudkamp2012}. The constructs differ substantially---teachers judge individual students while we predict aggregate item difficulty---so direct comparison is inappropriate, but the magnitude suggests LLM estimates capture meaningful difficulty information.

\begin{table}[t]
\caption{Prompt screening results (SmartPaper, $n=140$): top 10 of 15 screened prompts, ranked by best~$\rho$. Five additional prompts (verbalized\_sampling, familiarity\_gradient, imagine\_classroom, irt\_sim, kc\_mastery) achieved $\rho<0.55$. CIs shown for prompts advanced to model survey.}\label{tab:screening}
\centering\footnotesize
\begin{tabular}{@{}lccrccr@{}}
\toprule
Prompt & Item & Pop. & Best $\rho$ & Temp & 95\% CI & MAE \\
\midrule
\textbf{prerequisite\_chain} & \checkmark & & \textbf{0.686} & 0.5 & [.576,.775] & .155 \\
\textbf{cognitive\_load} & \checkmark & & \textbf{0.673} & 2.0 & [.550,.766] & .190 \\
\textbf{buggy\_rules} & \checkmark & \checkmark & \textbf{0.655} & 1.0 & [.532,.752] & .117 \\
misconception\_holistic & \checkmark & \checkmark & 0.636 & 2.0 & --- & .204 \\
error\_analysis & \checkmark & \checkmark & 0.596 & 2.0 & --- & .121 \\
devil\_advocate & & \checkmark & 0.596 & 1.0 & --- & .098 \\
cognitive\_profile & \checkmark & & 0.586 & 1.0 & [.456,.693] & .214 \\
contrastive & \checkmark & & 0.584 & 1.0 & --- & .123 \\
classroom\_sim & & \checkmark & 0.562 & 2.0 & --- & .240 \\
teacher & & & 0.555 & 1.0 & [.422,.664] & .439 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.58\textwidth}
\includegraphics[width=\textwidth]{figures/fig1_screening.pdf}
\caption{Prompt screening. Color = factor membership (green = item only, blue = item + pop, red = pop only, grey = neither). Dashed = teacher baseline.}
\label{fig:screening}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.40\textwidth}
\includegraphics[width=\textwidth]{figures/fig2_temperature.pdf}
\caption{Temperature effects. Most prompts show minimal sensitivity ($\Delta\rho<0.05$).}
\label{fig:temp}
\end{subfigure}
\caption{(a) Prompt screening results on SmartPaper ($n=140$). (b) Temperature sensitivity for selected prompts.}\label{fig:screening-temp}
\end{figure}

\subsection{The Mechanism: Item Analysis, Not Population Modeling}

Grouping prompts by their 2$\times$2 factor membership (Table~\ref{tab:prompts}) reveals a clear pattern: item analysis is the primary driver. Prompts that model student populations without analyzing item structure perform \emph{worse} than baseline.

Grouping by 2$\times$2 factor membership: item analysis only achieves mean $\rho=0.61$ (4 prompts), item + population $\rho=0.59$ (5 prompts), neither (baseline) $\rho=0.52$ (3 prompts), and population only $\rho=0.47$ (4 prompts). This parallels Embretson's~\cite{embretson1998} cognitive design system, where item difficulty is predicted from counts of processing steps and working memory demands. The LLM operationalizes similar constructs when directed to enumerate prerequisites or interacting elements.

The critical distinction is \emph{item-centric} vs.\ \emph{population-centric} framing. Hybrid prompts like buggy\_rules ($\rho=0.66$) succeed because ``identify procedural errors students might make'' is fundamentally item analysis---it asks what error paths the item affords, not how a simulated population would respond. Population modeling \emph{alone} is unreliable: explicit student simulation (synthetic\_students: $\rho=0.19$) performs worst, consistent with Li, Chen et al.'s~\cite{lichen2025} finding that proficiency simulation does not reliably improve difficulty estimation. Framing analysis through student behavior works; simulating student populations does not.

\subsection{What Doesn't Matter}

Three factors had minimal effects (each unique $\eta^2 < 0.05$):

\paragraph{Temperature.} Most prompts show $\Delta\rho<0.05$ across temperatures 0.5--2.0 (Figure~\ref{fig:temp}). The exception: prerequisite\_chain peaks at $t=0.5$ while cognitive\_load peaks at $t=2.0$.

\paragraph{Model size.} GPT-4o ($\rho=0.49$) underperforms Gemini~3~Flash ($\rho=0.55$) and matches Gemma-3-27B. Llama-3.1-8B struggles ($\rho=0.30$), but scaling alone does not predict success (Table~\ref{tab:models}).

\paragraph{Deliberation.} Undirected chain-of-thought reasoning degrades performance. The structured prompts that work use \emph{directed} analysis---they specify exactly what to enumerate (prerequisites, interacting elements, error paths). Undirected ``think step by step'' produces unfocused reasoning that drifts toward solving the item.

\paragraph{Surface features.} Text length correlates $\rho=-0.44$ with difficulty on SmartPaper (shorter questions are harder). Controlling for text length, partial $\rho=0.59$--$0.66$ for item analysis prompts---the correlation changes minimally ($\Delta\rho \le 0.06$), confirming that LLM predictions are not merely proxies for question length.

\begin{table}[t]
\caption{Model survey ($\rho$, SmartPaper, $t=1.0$). ``---''~indicates condition not run or $<$50 valid responses.}\label{tab:models}
\centering\footnotesize
\begin{tabular}{@{}lcrrr@{}}
\toprule
Model & Params & teacher & cog\_load & prereq \\
\midrule
Gemini 3 Flash & --- & 0.55 & 0.62 & \textbf{0.66} \\
Gemma-3-27B & 27B & 0.50 & 0.50 & --- \\
GPT-4o & --- & 0.49 & --- & --- \\
Llama-3.3-70B & 70B & 0.48 & 0.40 & 0.40 \\
Llama-4-Scout & 17B/109B & 0.47 & 0.40 & 0.35 \\
Llama-3.1-8B & 8B & 0.30 & 0.21 & 0.04 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.54\textwidth}
\includegraphics[width=\textwidth]{figures/fig3_model_survey.pdf}
\caption{Model survey. Frontier models achieve $\rho>0.50$; Llama-8B fails on structured prompts.}
\label{fig:models}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.44\textwidth}
\includegraphics[width=\textwidth]{figures/fig5_factors.pdf}
\caption{Post-hoc grouping by factor membership. Item analysis is the primary driver; population modeling alone underperforms baseline.}
\label{fig:factors}
\end{subfigure}
\caption{(a) Model $\times$ prompt heatmap on SmartPaper. (b) Mean $\rho$ by 2$\times$2 factor membership with 95\% CIs.}\label{fig:model-factors}
\end{figure}

One interaction matters: item analysis prompts help capable models but hurt weak ones. On Gemini, prerequisite\_chain gains $+0.08$ over teacher; on Llama-8B, it loses $-0.26$.

\subsection{Generalization Across Datasets}

We validated on two additional datasets: DBE-KT22 (168 undergraduate CS MCQs) and the BEA~2024 shared task (595 USMLE medical MCQs with IRT difficulty). Table~\ref{tab:generalization} shows results.

\begin{table}[t]
\caption{Cross-dataset generalization (Gemini 3 Flash, best temperature per dataset).}\label{tab:generalization}
\centering\footnotesize
\begin{tabular}{@{}lrrrrr@{}}
\toprule
Dataset & $n$ & teacher & prereq & buggy & Best 95\% CI \\
\midrule
SmartPaper (open-ended) & 140 & 0.56 & \textbf{0.69} & 0.66 & [.58,.78] \\
DBE-KT22 (CS MCQ) & 168 & 0.52 & 0.53 & \textbf{0.58} & [.46,.68] \\
BEA 2024 (USMLE MCQ) & 595 & 0.45 & 0.44 & \textbf{0.45} & [.39,.52] \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/fig4_cross_dataset.pdf}
\caption{Cross-dataset generalization. The item-analysis advantage (prerequisite\_chain, green) is strongest on open-ended items and attenuates on MCQs, where simple teacher judgment (grey) nearly matches structured prompts.}\label{fig:generalization}
\end{figure}

The signal transfers: all prompts achieve significant correlations on all three datasets. However, the item analysis advantage attenuates on MCQs. On SmartPaper, prerequisite\_chain beats teacher by $+0.13$; on DBE-KT22, buggy\_rules achieves the highest $\rho=0.58$ but the gap over teacher is only $+0.06$; on BEA, all three prompts are equivalent ($\rho\approx0.45$).

This suggests a \textbf{structural predictability hypothesis}: LLMs estimate difficulty well when it varies across knowledge domains (open-ended items spanning topics), but the advantage of counting prompts diminishes when difficulty is driven by distractor design within a domain. Simple teacher judgment is equally effective for MCQs.

\paragraph{BEA 2024 Benchmark Comparison.}
The BEA~2024 shared task~\cite{yaneva2024} evaluated difficulty prediction on 667 USMLE items using RMSE on IRT difficulty as the primary metric. The winning system (EduTec) achieved RMSE$=$0.299, barely improving over a mean-prediction baseline of 0.311; the best post-competition result (UnibucLLM, SVR+BERT features~\cite{rogoz2024}) achieved RMSE$=$0.281 with Kendall~$\tau=0.28$. Our zero-shot predictions show systematic bias (models overestimate student ability), but when calibrated on the BEA training set via linear scaling, our three-prompt ensemble achieves RMSE$=$0.280 and $\tau=0.31$---matching or slightly exceeding the best published result despite using prompts designed for K--8 education rather than medical licensing exams. This suggests that zero-shot rank-order signal, combined with simple calibration, can match supervised feature-extraction approaches.

\subsection{Practical Implications}

At approximately \$0.01 per item (3~replications $\times$ Gemini~3~Flash at \$0.50/\$3.00 per 1M tokens input/output), LLM difficulty estimation is orders of magnitude cheaper than field testing. Rankings transfer across datasets; absolute calibration does not---models overestimate student ability on SmartPaper. Notably, item analysis prompts achieve lower MAE (0.12--0.19) than teacher judgment (MAE$=$0.44), but the rank-order is preserved regardless of calibration bias.

For practitioners: use a capable model (Gemini, GPT-4o), prompt it to analyze structural item features, and average 3+ replications. For standard MCQs, simple teacher judgment suffices.

%% ============================================================
\section{Discussion}

\paragraph{Why Directed Analysis Works.}
The success of item analysis prompts suggests LLMs possess implicit pedagogical content knowledge---understanding of what makes content difficult---but this knowledge must be \emph{elicited} through directed enumeration. Undirected chain-of-thought drifts toward solving the item; directed analysis (``list prerequisites, then estimate'') channels implicit knowledge productively. This parallels findings in expertise research: experts often cannot articulate knowledge until prompted with specific frameworks~\cite{chi1994}.

\paragraph{Boundary Conditions.}
The attenuation on MCQs reveals when LLM estimation fails: difficulty driven by \emph{structural} features visible in the item text (prerequisites, cognitive load, error paths) is predictable; difficulty driven by \emph{population} features absent from the text (prior exposure, distractor plausibility for specific misconceptions) is not. Practical heuristic: use structured prompts for open-ended items spanning knowledge domains; use simple teacher judgment for MCQs within a domain.

\paragraph{Methodological Recommendations.}
Our design-space exploration surfaced three lessons. (1)~\emph{Beware Simpson's paradox in unbalanced designs.} We initially swept temperature only for promising prompts; a marginal ANOVA showed temperature as dominant ($\eta^2 = 0.74$), but this was an artifact---low temperatures were confounded with high-performing prompts. When restricted to balanced conditions, temperature explained $<1\%$ of variance. (2)~\emph{Use two-stage sequential DOE.} Screen all prompts cheaply (1~rep, single temp) to eliminate the bottom half, then run a balanced factorial on survivors. (3)~\emph{Validate on $\ge$100 items.} Our 20-item probe yielded $\rho = 0.46$; the full 140-item set achieved $\rho = 0.55$. Bootstrap analysis showed only 2.5\% of 20-item subsets would produce $\rho \ge 0.50$. Many published results use $<$50 items---treat such samples as exploratory only.

\paragraph{Limitations.}
Four limitations warrant mention. First, all 15 prompts were screened on a single model (Gemini~3~Flash); the model survey shows effects generalize but with diminished magnitude. Second, calibration does not transfer---models overestimate ability on SmartPaper but rankings are preserved, so population-specific adjustment would be needed for absolute predictions. Third, we do not compare to fine-tuned models; fine-tuned BERT/RoBERTa can outperform zero-shot LLMs when training data is available~\cite{lijiao2025}, but our approach targets the zero-shot case where items arrive without calibration data. Fourth, and most fundamentally: LLM estimation tells you how hard an item \emph{looks}, not how hard it \emph{is} for real students with real gaps and struggles. The model has never been a confused 12-year-old in a Hindi-medium school parsing an English word problem---it can count prerequisites but cannot feel the weight of missing one. Field testing yields individual-level diagnostics, validity evidence, and distractor analysis that LLM estimates cannot provide. More importantly, assessment exists to help students learn; when you skip field testing, no one learns anything.

%% ============================================================
\section{Conclusion}

We mapped the design space for LLM item difficulty estimation across 15~prompts, 6~models, and 3~datasets (\textasciitilde120 configurations, \textasciitilde\$100 in API calls). Item analysis prompts that enumerate structural features (prerequisites, cognitive load, error paths) achieve $\rho=0.65$--$0.69$ on open-ended items, but this advantage attenuates on MCQs ($\rho\approx0.45$--$0.58$), where simple teacher judgment nearly matches. Prompt design is the primary lever; model size, temperature, and deliberation each contribute $\le 0.05$~$\rho$. Rankings transfer across datasets but calibration does not---at \textasciitilde\$0.01 per item, LLM estimates can triage items into difficulty bands, but absolute predictions require population-specific adjustment. Methodologically, correlations from $<$50 items are unreliable, and unbalanced hyperparameter sweeps produce Simpson's paradox artifacts; we recommend two-stage sequential DOE.

\paragraph{Ethics Statement.}
This study uses anonymized, aggregated student response data. SmartPaper data consists of item-level statistics without individual student identifiers. DBE-KT22 and BEA~2024 are publicly available research datasets. No personally identifiable information was accessed or processed.

\paragraph{Data and Code Availability.}
Prompts and analysis code are available in the project repository. SmartPaper data are available upon request from the originating organization. DBE-KT22 is publicly available~\cite{dbe-kt22-zenodo}. BEA~2024 data are available through the shared task organizers.

%% ============================================================
\bibliographystyle{splncs04}
\begin{thebibliography}{99}

\bibitem{acquaye2025}
Acquaye, C., Huang, Y.T., Carpuat, M., Rudinger, R.: Take out your calculators: Estimating the real difficulty of question items with LLM student simulations. arXiv:2601.09953 (2025)

\bibitem{benedetto2023}
Benedetto, L., et al.: A survey on recent approaches to question difficulty estimation from text. ACM Computing Surveys \textbf{56}(5), 1--37 (2023)

\bibitem{brown1978}
Brown, J.S., Burton, R.R.: Diagnostic models for procedural bugs in basic mathematical skills. Cognitive Science \textbf{2}(2), 155--192 (1978)

\bibitem{chi1994}
Chi, M.T.H., Slotta, J.D., de~Leeuw, N.: From things to processes: A theory of conceptual change. Learning and Instruction \textbf{4}(1), 27--43 (1994)

\bibitem{deayala2009}
de~Ayala, R.J.: The Theory and Practice of Item Response Theory. Guilford Press (2009)

\bibitem{dbe-kt22-zenodo}
Du~Plessis, C., van~Vuuren, J.: DBE-KT22: A Knowledge Tracing Dataset from South African Secondary Schools. Zenodo (2022). \url{https://doi.org/10.5281/zenodo.7096666}

\bibitem{embretson1998}
Embretson, S.E.: A cognitive design system approach to generating valid tests. Psychological Methods \textbf{3}(3), 380--396 (1998)

\bibitem{embretson2000irt}
Embretson, S.E., Reise, S.P.: Item Response Theory for Psychologists. Lawrence Erlbaum (2000)

\bibitem{feng2025}
Feng, W., Tran, P., Sireci, S.G., Lan, A.: Reasoning and sampling-augmented MCQ difficulty prediction via LLMs. arXiv:2503.08551 (2025)

\bibitem{gorin2005}
Gorin, J.S.: Manipulating processing difficulty of reading comprehension questions. J.\ Educational Measurement \textbf{42}(4), 351--373 (2005)

\bibitem{hoge1989}
Hoge, R.D., Coladarci, T.: Teacher-based judgments of academic achievement: A review. Review of Educational Research \textbf{59}(3), 297--313 (1989)

\bibitem{koedinger2012}
Koedinger, K.R., Corbett, A.T., Perfetti, C.: The Knowledge-Learning-Instruction framework. Cognitive Science \textbf{36}(5), 757--798 (2012)

\bibitem{lichen2025}
Li, M., Chen, H., Xiao, Y., et al.: Can LLMs estimate student struggles? arXiv:2512.18880 (2025)

\bibitem{lijiao2025}
Li, M., Jiao, H., Zhou, T., et al.: Item difficulty modeling using fine-tuned small and large language models. Educ.\ and Psych.\ Measurement \textbf{85}(6), 1065--1090 (2025)

\bibitem{moore2024}
Moore, S., Schmucker, R., Mitchell, T., Stamper, J.: Automated generation and tagging of knowledge components from multiple-choice questions. In: L@S 2024, pp.\ 122--133 (2024)

\bibitem{peters2025}
Peters, S., et al.: Text-based approaches to item difficulty modeling: A systematic review. arXiv:2509.23486 (2025)

\bibitem{razavi2025}
Razavi, P., Powers, S.J.: Estimating item difficulty using large language models and tree-based ML. arXiv:2504.08804 (2025)

\bibitem{rogoz2024}
Rogoz, A., Ionescu, R.T.: UnibucLLM: Harnessing LLMs for automated prediction of item difficulty and response time. In: BEA 2024, pp.\ 502--508 (2024)

\bibitem{sudkamp2012}
S\"udkamp, A., Kaiser, J., M\"oller, J.: Accuracy of teachers' judgments: A meta-analysis. J.\ Educational Psychology \textbf{104}(3), 743--763 (2012)

\bibitem{sweller1988}
Sweller, J.: Cognitive load during problem solving. Cognitive Science \textbf{12}(2), 257--285 (1988)

\bibitem{thurstone1927}
Thurstone, L.L.: A law of comparative judgment. Psychological Review \textbf{34}(4), 273--286 (1927)

\bibitem{yaneva2024}
Yaneva, V., et al.: Findings from the first shared task on automated prediction of difficulty. In: BEA 2024, pp.\ 470--482 (2024)

\bibitem{zotos2025}
Zotos, L., van~Rijn, H., Nissim, M.: Are you doubtful? Exploring model uncertainty for difficulty estimation. In: EDM 2025, pp.\ 77--89 (2025)

\end{thebibliography}

%% ============================================================
\appendix
\section{Example Prompts}\label{app:prompts}

We show the baseline and best-performing prompts. Both share common elements: grade level, subject, question text, rubric, and maximum score. They differ in what analysis they request before the final estimate. Full prompts for all 16 conditions are available in the project repository.

\paragraph{Teacher (baseline).}
\begin{quote}\small
\texttt{You are an experienced teacher in [subject] for Grade [N] students in India.}

\texttt{For this open-ended question, estimate what proportion of students would score full marks.}

\texttt{Question: [question\_text]}\\
\texttt{Rubric: [rubric]}\\
\texttt{Maximum score: [max\_score]}

\texttt{Think about:}\\
\texttt{- What specific errors or misunderstandings would cause students to lose marks?}\\
\texttt{- How clearly does the question communicate what's expected?}\\
\texttt{- What prerequisite knowledge is needed?}\\
\texttt{- How likely are students at this grade level to have that knowledge?}

\texttt{Respond with ONLY a number between 0 and 1 representing the proportion of students who would get full marks. For example: 0.45}
\end{quote}

\paragraph{Prerequisite Chain (best performer, $\rho=0.69$).}
\begin{quote}\small
\texttt{[Population context: economically weaker sections, Hindi-medium backgrounds]}

\texttt{For this question, identify the prerequisite knowledge and skills a student needs. Count how many independent things must ALL go right for a correct answer. Each prerequisite is a potential failure point.}

\texttt{Examples of prerequisites: reading comprehension, specific vocabulary, a math operation, a concept definition, multi-step reasoning, writing ability.}

\texttt{[Question, rubric, max score]}

\texttt{List the prerequisites, then estimate what proportion would get full marks.}

\texttt{PREREQUISITES: [list them]}\\
\texttt{COUNT: [N]}

\texttt{Respond with ONLY a number between 0 and 1 on the last line.}
\end{quote}

\end{document}
