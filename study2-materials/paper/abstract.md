# Misconception Alignment in LLM-Simulated Students: Validation and Prompting Strategies

## Abstract

Large language models (LLMs) are increasingly used to simulate students for tutor training and intelligent tutoring system development. However, a critical question remains underexplored: when LLM-simulated students make errors, do these errors align with documented human misconceptions? We investigate misconception alignment across five LLMs spanning a wide capability range (7-95% on GSM8K) and three prompting strategies. Using 50 mathematics items designed to elicit specific misconceptions, we measure the rate at which incorrect LLM responses select the misconception-aligned distractor versus other wrong answers. Our findings reveal three key insights: (1) LLM errors align with human misconceptions significantly above chance (33%), with rates ranging from 47-73% depending on model and prompt; (2) a "diagnose-then-simulate" prompting strategy, which asks models to first identify common student errors before exhibiting one, improves alignment over standard persona-based prompting; (3) misconception alignment shows an inverted-U relationship with model capabilityâ€”very weak models produce random errors while very strong models rarely err, with mid-capability models (50-60% GSM8K) achieving the highest alignment. These results have implications for the validity of LLM-based tutor training systems and suggest concrete strategies for improving the authenticity of simulated student responses.

**Keywords:** student simulation, large language models, misconceptions, intelligent tutoring systems, tutor training
